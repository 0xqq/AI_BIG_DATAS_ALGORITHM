回顾下GAN网络:

GAN网络中我们了解到，大概思路其实就是一个我们用来生成图片的网络G，一个用来判断图片是否准确的网络D。
为了让D判断更准确，我们事先给他训练好一个假的D，然后将训练好的参数w和b初始化我们真的D中，这样是为了让D开始就准确判断真假。
同时G网络也输入到D中，让D无法识别G是假的图片即可。这里的根本点就是损失函数的区别，因为图像处理的本质就是最小化损失函数。
我们将D的损失函数用log表示，一个是D自身的损失函数，根据log(D)+log(1-G) 也就是一个log(1+x)图，这种图当D趋近于1时log趋近为0也就是损失越小。
而G趋近于0时结果越趋近于log1，也就是在x轴下方损失越大。 同理G的损失为log(G)即可。这样俩者损失函数就有了本质区别，G需要找个临界点，让G和D的损失正好无法判别出。
而log(D)就是我们开始初始化w和b生成的结果，我们第一部越好这个log(D)就越好。

下面根据上面分析，解释DCGAN:

1.将pooling层用卷积层替代(对于判别模型:容许网络学习自己的空间下采样(特征提取),对于生成模型:容许它学习自己的空间上采样(特征离散)。)
也就是没有池化层，都用卷积层取代。
其实就是:
G网络开始是通过1个噪音向量，通过全连接生成一个通道数很长的但长宽很小的特征图，然后不断地反卷积扩大成我们需要的图片大小的维度。
D网络开始是个普通大小特征维度的图片，我们不断的卷积提取特征一直提取到维度为1，也就是个二分类问题，判别出这个是0还是1.  

2.在generator喝discriminator使用batchnorm:
(1)解决初始化差的问题。
(2)帮助梯度传播到每一层。(原来GAN中梯度容易离散化。)
(3)防止generator把所有样本都收敛到同一点。(原来的GAN容易在生成样本时收敛到一个点。这里本质就是加了正则，即使初始化是随机值，效果也不错。)

3.在CNN中移除全连接层，所有结构都是卷积层。

4.在generator的除了输出层外的所有层使用ReLU激活函数,输出层采用tanh激活函数。(tanh:  (e^(x)-e^(-x)) /(e^x+e^(-x)) )
  tanh激活函数曲线是以0为中心，x大于0y都大于0，然后随着x越来越大y的增幅越来越小。下面同理。也就是类似log使其更平滑。

5.在discriminator所有层使用LeakyRELU


