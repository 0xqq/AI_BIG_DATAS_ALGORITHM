
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import input_data

#tensorflow模拟神经网络
mnist = input_data.read_data_sets("Untitled Folder/",one_hot=True)
print(type(mnist))
print((mnist.train.num_examples))
print((mnist.test.num_examples))

trainimg   = mnist.train.images
trainlabel = mnist.train.labels
testimg    = mnist.test.images
testlabel  = mnist.test.labels
print(trainimg.shape)
#因为训练集的特征数为784,我们这里神经元设置为1/3,第二层为1/6
num_hidden1 = 256
num_hidden2 = 128
num_input = 784
num_classes = 10
X = tf.placeholder("float",[None,num_input])
Y = tf.placeholder("float",[None,num_classes])
#我们创建标准正态分布W数据集，mean = 0,方差为0.1
sttdev = 0.1
weights = {
    "w1":tf.Variable(tf.random_normal([num_input,num_hidden1],stddev=sttdev)),
    "w2":tf.Variable(tf.random_normal([num_hidden1,num_hidden2],stddev =sttdev )),
    "out":tf.Variable(tf.random_normal([num_hidden2,num_classes],stddev= sttdev))
}
basic = {
    "b1":tf.Variable(tf.random_normal([num_hidden1],stddev = sttdev)),
    "b2":tf.Variable(tf.random_normal([num_hidden2],stddev = sttdev)),
    "out":tf.Variable(tf.random_normal([num_classes],stddev=sttdev))
}

#正向传播函数
def positiveHiddenLayer(X,W,b):
    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(X,W["w1"]),b["b1"]))
    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(layer1,W["w2"]),b["b2"]))
    return (tf.matmul(layer2,W["out"]) + b["out"])
    
#预测值
pre = positiveHiddenLayer(X,weights,basic)
#计算损失函数,利用softmax里的Log损失函数
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pre,Y),reduction_indices=1)
#梯度下降最优化,学习率初始设置为0.01
optm = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)
#求score,利用所有值比较转为float再求平均
score = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pre,1),tf.argmax(Y,1))),"float")

#初始化
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
#设置训练周期,每次多少个batches,每多少次输出一次
training_nums = 30
batches = 100
steps = 5
for i in range(training_nums):
    losses = 0.0
    nums = int(trainimg.shape[0]/batches)
    for j in range(nums):
        batchx,batchy = mnist.train.next_batch(batches) #这个函数是将我们所有数据集都按照这个batch掉，也就是每次都分割100个
        feeds = {X:batchx,Y:batchy}
        #接下来run出损失函数，先run最优化，再run最终loss值。
        sess.run(optm,feed_dict=feeds)
        losses+=sess.run(loss,feed_dict=feeds)
    losses = losses/nums
    if nums % steps==0:
        feed1 = {X:batchx,Y:batchy}
        train_score = sess.run(score,feed_dict=feed1)
        feed2 = {X:testimg,Y:testlabel}
        test_score = sess.run(score,feed_dict=feed2)
        print("Epoch: %03d/03d Loss:%.9f train_score:%.3f test_score:%.3f"%(i,training_nums,losses,train_score,test_score))
