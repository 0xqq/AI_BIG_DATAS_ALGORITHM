一.XGB


二.LGB

三.随机森林

四.blending融合

五.stacking融合

六.管道机制


七.模型调参
调参
调参这事很复杂，有很多经验、方法，实际的比赛中往往还是需要一些玄学。调参最常用的方法就是GridSearch和RandomSearch。GridSearch是给定每个待调参数的几个选择，然后排列组合出所有可能性（就像网格一样），做Cross Validation，然后挑选出最好的那组参数组合。RandomSerach很类似，只是不直接给定参数的有限个取值可能，而是给出一个参数分布，从这个分布中随机采样一定个数的取值。
调参的方法理解了，那具体调什么参数呢？Zillow Prize比赛里，主要用的模型是XGBoost和LightGBM。下面列出一些主要用到的参数，更多的还是直接看文档。

XGBoost:

booster: gblinear/gbtree 基分类器用线性分类器还是决策树
max_depth: 树最大深度
learning_rate：学习率
alpha：L1正则化系数
lambda：L2正则化系数
subsample: 训练时使用样本的比例
LightGBM：

num_leaves: 叶子节点的个数
max_depth：最大深度，控制分裂的深度
learning_rate: 学习率
objective: 损失函数（mse, huber loss, fair loss等）
min_data_in_leaf: 叶子节点必须包含的最少样本数
feature_fraction: 训练时使用feature的比例
bagging_fraction: 训练时使用样本的比例
调参的时候需要理解这些参数到底是什么意思，如果过拟合了应该增大还是减小某个参数，这样才能有目的而不是盲目地调参。当然，想要找到最佳的参数很多时候需要一些经验和运气。也不需要极致追求最佳参数，大多数情况下找到一组相对不错的参数就可以了，往往还有别的方法来提升总成绩。

八.模型权重分析
现在大多数模型在训练完成后都会给出获取特征权重的接口，这时的权重是直接反映了一个特征在这个模型中的作用。
这是判断一个特征是否有用的重要方法。例如，原始特征有卧室数量和卫生间数量，我们自己创造了一个特征房间总数（卧室数量+卫生间数量）。
这时可以用这种方法判断这个新特征是否有效。
