这里我只是简单讲解，因为我数学能力有限，没有深入研究，只是为了应用而研究，而不是为了研究而研究，所以水平有限。

一.boosting
Boosting是一族可将弱学习器提升为强学习器的算法.
1.解决问题
1）在每一轮如何改变训练数据的权值或概率分布？
通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样本的权值，而误分的样本在后续受到更多的关注.
2）通过什么方式来组合弱分类器？
通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值.
而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型.

2.核心思想
核心都是求残差:
误差: y-f1(x) 也就是真实值-预测值
残差y的导数/f1(x)导数做偏导。假误差，也就是残差。gbdt是一阶导。
而Xgboost是二阶导数。
Xgboost
损失函数=正则项+损失函数。它的损失函数是进行二阶泰勒展开。也是梯度下降。

二.AdaBoost:
开始每个样本都加个权重值，开始大家都一样重要。每次训练中得到新的模型。
数据有对错，增加错误率高的样本的权重，减少错误率低的样本的权重。使得下次错的样本被分类器重点照顾使其准确率越来越高，
进行N次迭代，会得到N个简单分类器然后经过加权取均值得到最终模型。这次的权重是错误率低的权重大。我们设定的阈值: 错误率足够小，
迭代次数达到一定程度，或者迭代多少次错误率没发生变化。
本质是: 将基分类器的线性组合作为强分类器 => 每轮的强分类器结果=每轮各个样本被弱分类器分到的结果*样本权值之和。最终求概率是加sign转换。

三.GBDT:
都用回归树，用来回归预测，也可以用分类。(设定阈值)。每一颗树是之前所有树的结论和参差(负梯度)。这个残差是一个加预测值后能得真实值的累加量。
比如A的真实年龄是12，预测为6岁。差了6岁。残差为6.那么第二棵树就把A的年龄设为6岁去学习。如果第二棵树能把A分到6岁的叶子节点(分
到叶子节点就是无残差了)，那么累加两棵树的结论就是A的真实年龄。如果第二棵树结论是5岁，则A仍然存在1岁残差。继续学。
优势:每一步残差计算增大了分错样本的权重，而已经分对的样本都趋向于0.这样后面的树也专注于分错的样本。

GBDT和AdaBoost：GBDT是为了减少上次的残差进而消除残差。我们可以在残差的梯度方向上建立一个新的模型。
所以本质是使得之前残差往梯度方向减少，每次一小步逼近准确值。
只有1阶导。
优点:非线性多，泛化能力强，不需要过多特征工程
缺点:串行不好并行化，计算复杂度高，所以不适合高纬稀疏矩阵。
Adaboost是按分类对错，分配不同权重。本质是错分的样本权重越来越大，更被重视，进而每次找到错误率更低的分类器，最后投票加权得到最好的。


四.XGB:
基于GBDT的缺点，诞生了XGBoost.
在gbdt只做梯度上升非线性的前提下，还支持线性分类器(gblinear)
相当于带L1,L2正则化项的逻辑回归或线性回归。解决分类和回归。
同时还拥有二阶泰勒展开，可以自定义损失函数，必须里面有一二阶求导这也是kaggle比赛常用高级方法。损失函数中加了正则项来控制模型的复杂度。
包含叶子节点个数，每个叶子节点输出的score的L2模的平方和。这样防止过拟合也是他的优势。

具体过程:
xgboost每进行完一次迭代，会将叶子节点权重乘以学习率，削弱每颗树的影响，所以一般把学习率设置小一点，迭代设置大一些。还借鉴随
机森林方法支持列抽样，降低过拟合减少计算。
对于特征值有缺失的样本，xgboost可以自动学习出它的分裂方向。实现层用内置交叉验证法。
注意:xgboost并行不是tree并行，它也是完成一次迭代再进行下一次，第t次迭代包含了前t-1次迭代的预测值。xgboost并行是在特征粒度上的。
因为决策树最消耗性能的步骤就是对特征值排序找到分割(这是个大循环)。所以xgboost训练前先排序，保存block结构(类似kafka原理)。然
后迭代时共享这个结构减少计算量。block结构就是并行的本质。计算出各个特征的增益，选择增益最大的去分裂(总结就是特征粒度的并行，block结构，预排序。)

优势:
(1)树形结构加了正则项优化，防止过拟合
(2)用了二阶导数信息(速率)
(3)使用列抽样防止过拟合。
(4)节点分类算法能自动利用特征的稀疏性
(5)样本先排序以block存储，利于并行计算。
(6)本质是对树的叶子分数做惩罚，确保树的简单性
(7)支持分布式计算，可以用在yarn上。

五.LGB:
也是基于决策树的分布式梯度提升框架
与XGboost区别:
(1)占用内存更低，只保存特征离散化后的值。用8位整型存储就够了，内存消耗为原来1/8
(2)计算提高:牺牲部分精确性提升切分效率。预排序算法每遍历一个特征值就需要计算一次
分裂的增益，而直方图算法只需要k次(k是常数)。时间复杂度从O(data的feature)到O（k的feature）
XGBOOST本质:XGBoost是预排序,因为每个分割点都需要计算增益消耗大。排序后特征对梯度是随机访问而不是顺序访问，这样无法
对cache优化，磁盘上顺序与随机性能差距很大。
LightGBM本质:
histogram算法。思想是把连续浮点特征离散成k个整数。构造宽度为k的直方图。遍历数据时根据离散后的值为索引在直方图中累积统计
量，遍历一次数据后，直方图累积了需要的统计量，根据直方图的离散值遍历寻找最优的分割点。

(3)决策树生长策略
XGBoost:一次数据同时分裂在同一层，容易多线程优化不容易过拟合但带来不必要的开销。因为很多叶子分裂增益比较低，没必要
LightGBM: 每次都从叶子分裂增益最大的叶子分裂。可以做差加速，一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图
做差得到，通常构造直方图需要遍历该叶子上所有数据。但直方图做差仅需要遍历直方图的k个桶。所以速度更快。
还可以直接输入类别特征不需要0/1展开。并增加类别特征的的决策规则。

总结:
LightGBM优点:Cache优化，直方图的稀疏特征优化。内存消耗更低。
缺点:精确度可能有微小降低。

六.随机森林和bagging思想

1.Bagging即套袋法，其算法过程如下：
(1)从原始样本集中抽取训练集.每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，
而有些样本可能一次都没有被抽中）.共进行k轮抽取，得到k个训练集.（k个训练集相互独立）
(2)每次使用一个训练集得到一个模型，k个训练集共得到k个模型.（注：根据具体问题采用不同的分类或回归方法，如决策树、神经网络等）
(3)对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果. 

2.随机森林:
(1)随机森林在bagging的基础上更进一步：
(2)样本的随机：从样本集中用Bootstrap随机选取n个样本
(3)特征的随机：从所有属性中随机选取K个属性，选择最佳分割属性作为节点建立CART决策树（泛化的理解，这里面也可以是其他
类型的分类器，比如SVM、Logistics）
(4)重复以上两步m次，即建立了m棵CART决策树
(5)这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类（投票机制有一票否决制、少数服从多数、加权多数）


七.stacking融合
https://blog.csdn.net/data_scientist/article/details/78900265
Stacking模型是指将多种分类器组合在一起来取得更好表现的一种集成学习模型。一般情况下，Stacking模型分为两层
。第一层中我们训练多个不同的模型，然后再以第一层训练的各个模型的输出作为输入来训练第二层的模型，以得到一个最终的输出。
八.blending融合
Blending与Stacking大致相同，只是Blending的主要区别在于训练集不是通过K-Fold的CV策略来获得预测值从而生成第二阶段模
型的特征，而是建立一个Holdout集，例如10%的训
练数据，第二阶段的stacker模型就基于第一阶段模型对这10%训练数据的预测值进行拟合。说白了，就是把Stacking流程中的K-Fold CV 改成 HoldOut CV。
Blending的优点在于：
1.比stacking简单（因为不用进行k次的交叉验证来获得stacker feature）
2.避开了一个信息泄露问题：generlizers和stacker使用了不一样的数据集
3.在团队建模过程中，不需要给队友分享自己的随机种子
而缺点在于：
1.使用了很少的数据
2.blender可能会过拟合（其实大概率是第一点导致的）
3.stacking使用多次的CV会比较稳健


九.管道机制
https://blog.csdn.net/cheng9981/article/details/61918129
1.Pipeline:
管道可以用于将多个估计器链接成一个。 这是有用的，因为在处理数据中经常有固定的步骤序列，例如特征选择，归一化和分类。 管道在这里有两个目的：
方便：您只需调用fit和预测一次您的数据，以适应一个完整的估计量序列。
联合参数选择：可以一次性在管线中的所有估计量的参数上进行网格搜索。
流水线中的所有估计器，除了最后一个，必须是变换器（即必须具有变换方法）。 最后一个估计器可以是任何类型（变换器，分类器等）。

2.FeatureUnion：复合特征空间

FeatureUnion将多个转换器对象组合成一个新的转换器，结合了它们的输出。 FeatureUnion获取转换器对象的列表。 在
拟合期间，这些中的每一个独立地拟合数据。 对于变换数据，变换器被并行应用，并且它们输出的样本向量端对端地连接成较大的向量。
FeatureUnion具有与Pipeline相同的目的 - 方便和联合参数估计和验证。
FeatureUnion和Pipeline可以组合以创建复杂模型。
（FeatureUnion无法检查两个转换器是否可能产生相同的特征，当特征集不相交时，它只产生一个并集，并确保它们是调用者的责任。）
FeatureUnion使用（key，value）对的列表构建，其中key是您要给予给定变换的名称（任意字符串;它仅用作标识符），value是一个估计器对象

3.make_pipeline:
与Pipeline相似，但是他是处理的是数据本身，而不是最后拟合模型，所以我们这里用来处理数据

4.make_union
理解为make_pipeline的FeatureUnion即可。


十.模型调参
调参
调参这事很复杂，有很多经验、方法，实际的比赛中往往还是需要一些玄学。调参最常用的方法就是GridSearch和RandomSearch。GridSearch是给定每个待调参数的几个选择，然后排列组合出所有可能性（就像网格一样），做Cross Validation，然后挑选出最好的那组参数组合。RandomSerach很类似，只是不直接给定参数的有限个取值可能，而是给出一个参数分布，从这个分布中随机采样一定个数的取值。
调参的方法理解了，那具体调什么参数呢？Zillow Prize比赛里，主要用的模型是XGBoost和LightGBM。下面列出一些主要用到的参数，更多的还是直接看文档。

XGBoost:

booster: gblinear/gbtree 基分类器用线性分类器还是决策树
max_depth: 树最大深度
learning_rate：学习率
alpha：L1正则化系数
lambda：L2正则化系数
subsample: 训练时使用样本的比例
LightGBM：

num_leaves: 叶子节点的个数
max_depth：最大深度，控制分裂的深度
learning_rate: 学习率
objective: 损失函数（mse, huber loss, fair loss等）
min_data_in_leaf: 叶子节点必须包含的最少样本数
feature_fraction: 训练时使用feature的比例
bagging_fraction: 训练时使用样本的比例
调参的时候需要理解这些参数到底是什么意思，如果过拟合了应该增大还是减小某个参数，这样才能有目的而不是盲目地调参。
当然，想要找到最佳的参数很多时候需要一些经验和运气。也不需要极致追求最佳参数，大多数情况下找到一组相对不错的参数就可以了
，往往还有别的方法来提升总成绩。

十一.模型权重分析
现在大多数模型在训练完成后都会给出获取特征权重的接口，这时的权重是直接反映了一个特征在这个模型中的作用。
这是判断一个特征是否有用的重要方法。例如，原始特征有卧室数量和卫生间数量，我们自己创造了一个特征房间总数（卧室数量+卫生间数量）。
这时可以用这种方法判断这个新特征是否有效。
