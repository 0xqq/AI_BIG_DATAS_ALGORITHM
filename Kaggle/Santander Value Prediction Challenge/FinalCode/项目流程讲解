前言:很高兴第一参加正规比赛获得了9%的名次。  https://www.kaggle.com/jinrongming
通过这次比赛，我学习了很多统计学习学处理特征工程知识，以及算法的应用。下面我来讲解下我如何分析这个项目，并如何使用这些技术来完成它。

一.业务分析。
    这个阶段很重要，我们要分析业务，主要是直接获取我们大家知道的有用特征，并结合明面的特征，去挖掘新的特征。比如我举个栗子: 一个数据集中，有个特征
是生日，而这个数据集是个销售预测的项目，那么你会联想到什么吗? 很简单，我们可以拆分生日，取出里面的年份和月份，这样可以计算出样本的生日，进而推出年龄。
然后将年龄离散化，并用K-means /RandomForset去聚类，分类，这样可以看到哪些人群它的消费是什么水平，然后我们可以适当调节权重，来拟合他们。比如 把它离散
了300个类，然后再聚合成5个类，这5个类通过占比，调节权重为0.5,0.2,0.05,0.1,0.15.去拟合线性方程。 我这里只是举个例子，很多方式都可以去做。机器学习
深度学习，还是强化学习，最终都是为了优化一个损失值，让绝大多数数据都可以接近一个准确值，而这个就是需要参数的调节。所以我们的本质，是调节参数。
    
    不过说了那么多，其实这次我没用到那些特点，因为我们这次业务很明显，是个银行加密数据。 4500样本，4993维度，去predict 45000样本 4992维度数据。全是
数值型。而且是大量稀疏矩阵。目的是找到客户的潜在价值，预测他们的价值分数，以便通过价值我们去向客户推荐什么样的业务。当然这是后话，因为这次比赛只是
让我们预测价值。所以是个回归，大量稀疏矩阵的问题，样本数为4500，维度4993，维度>样本数。的案例，同时，也告诉一个重要特征，评估指标为rmsle,也就是取
log的rmse.这点告诉我们，要正态化数据，将其化为指数，我在后期构造特征时也用到了这个细节。

    做完业务分析后，我们大概联想出几个方案，并去掉几个方案。去掉的方案有：DeepLearning,强化学习。 原因如下:
 (1)DeepLearning: 深度学习说白了就是将多种线性回归器离散化，并形成了循环迭代，无限制调参，并从中拼接/提取特征，所以需要个长期迭代处理，而长期迭代需要
 数据量的保证，而训练集只有4500，所以pass
 (2)强化学习：必须有大量数据和时间数据才可以做。pass
 
 我选择的几个方案:
 (1)大量树形回归器，最后stacking/blending融合，配合一个Lasso调参。
 (2)大量树形回归器，最后加权平均，这也是我这里用的方法。
 (3)GBDT+LR: 这种适合处理一定数量的稀疏，但处理大量稀疏效果不一定好，原因很简单:树形结构在分特征时，很容易第一次就将很多特征性相似的样本划分一起。
 而这些样本可能出现大量0，这样会导致恰好拟合一个好模型，从而放入Lasso调参时，形成一个较少又大的参数，因为0的参数不做计算，所以假设强特征W1，会特别大。
 导致其他样本不受到影响，所以也就是只配合我们这个数据集，如果换数据集时，就会导致效果很差，这是过拟合本质，也就是参数少，大，所以才有正则化，但这是后话，我们防止过拟合最终是让w变小，多，让他有能力去拟合更多数据集。所以
 用单一的树形配合线性去做大量稀疏数据不一定好，这种方法我们就取消了。
 (4)大量线性+树形融合。线性拥有强大正则来防止过拟合，而树形可以有效区分数据。用稳定性回归选择，去迭代选择特征，去做线性处理，用RF,GBDT,XGB,LGB去做
 特征性选择去拟合不同树形回归器，最后调参融合。
 
 由于要找工作原因，我这里用了比较简单的(2)方法。而且我后续主要把时间还是用在特征工程。
 
 二.数据探索
 (1) 我们比较特征数据之间的方差(主要看特征性是否强弱)，偏差(影响数据是否容易欠拟合的，偏差大的噪点/离群点多),峰度(因为我们最后要处理成正态数据，这个
 指标是观察正态数据是否离群点多，看是否容易过拟合。),协方差(特征之间的线性关系强弱，比较特征与特征之间的方差关系，如果大，说明俩者关系比较密切。)。
 然后进行排序统计，统计明显的特征，后续看是否对其进行处理，比如drop,nan值变为0或者补充平均值，或者中位数等等。 如果内存够，可以可视化。
 
 (2)PCA主成分分析。
 PCA任务主要有俩个，一个是降维，第二是使线性不相关的特征转为特征相关的，也就是连续数值变为离散化。而它的降维方法，就是变为离散化的本质。将多维度数据
 映射到低维度，为了防止线相交，将其离散化，每次映射都是正交状态。
 然后设定阈值，一般是0.85,选取大概降低到多少维度可以是独立特征性强的特征，这是为我们后面特征提取准备，同时，PCA也可以做相关性处理，可以利用PCA
 将特征变成独立特征的特点，做独立强相关性的特征进行拼接，(因为我们不知道数据特点，所以用这个方法。)。为什么不直接用强相关性的特征做拼接呢?因为
 这样可能导致构造出没用的特种。
 我举个栗子：一个汽车数据集，它有俩个维度： 每小时多少公里数  每小时多少英里数。 第二个是为了给老外看的，所以这俩这相关性肯定强，甚至可能是1.如果
 不用PCA处理，直接通过corr来拼接，容易拼接成这种没用的特征，进而容易导致过拟合。
 
 (3)其他降维分析， SparseRandomProjection(快速降维，利用随机性，最后通过它映射到数据上，构造特征有时可以不错)
 ) TSVD 等都可以尝试，TSVD更适合处理大量稀疏(容易产生0)，PCA适合处理大量强相关数据(容易产生噪点) 
 
 (4)特征性比较 稳定性随机线性法 RandomLasso -》 比较线性特征。 XGB LGB RF GBDT 都可以尝试，去比较，并映射到PCA上，看看噪点是否多，以便做处理
 (补平均值/中位数，drop.)
 
 三.特征工程。
 分析完数据后，开始特征工程，我用了大量的特征工程处理: 
 1.数据处理
 (1)噪点处理：  客户业务价值小于10美元。 设置Fasle处理。
 (2)弱特征处理：drop 同数值，线性相同，并将弱线性关系进行
 (2)scale处理：因为我们这次决定用树形结构，其实不用归一化/标准化，但为了更好拟合，可以将一些弱特征做scale，而弱特征我们的范围是 平均值-3*标准差，
 平均值+3*标准差。
 (4)最后尝试用柯尔莫哥洛夫-斯米尔诺夫测试测试训练数据和测试数据。
 这是一个双侧测试的零假设, 是否从相同的连续分布抽取2独立样本 (见更多)。如果一个特征在训练集上的分布与测试集不同, 
 我们应该删除这个特性, 因为在训练中我们学到的东西不能一概而论。THRESHOLD_P_VALUE 和 THRESHOLD_STATISTIC 是超参数。
 这里的数据是数值型，且测试和训练数据必须相同才可以用。但不一定会好，需要实验。这里取消了
 2.特征处理：
 RF GBDT XGB LGB 提取特征，分别给四个模型使用，或者取交集，为了省时间，我采用了取交集，而前期PCA部分让我大概了解多少特征提取比较好，选取800个取交集。
 然后用PCA降维到100维度上，使其变为线性可区分，设置相关性CORR，取0.6-1之间进行强相关拼接。 另外XGB有个特殊方法，就是特征比较，可以用一个一个特征，
 分别去拟合XGB的一个特征与另外特征，这样比较该特征与其他特征关系，所以取一个最强特征比较，然后得到一组rmse值，取前面几个，作为一个强特征，可以用
 来拼接一些特征，或者构造平均值，最大值以便增加强特征的权重，为了更好拟合。
 
 最后我们结合俩组特征(四个树形提取的共同特征并拼接强相关性的结果 与 XGB 通过特征组合提取的特征)，我们将前一组特征去取最大值，平均值，方差等数值，
 并除了偏度和峰度方差其他都Log化，这也是因为评估指标是Log的标准去执行。然后后面XGB与前面取交集再构造一些类似特征，这样是增加强特征权重。
 
 四。最后的处理。
 管道的机制很简单，必须遵守fit transform 然后fit拟合数据，也就是拟合我们的特征，transform将特征拼接到我们的DataFrame上，返回，必须这样做，否则出错。
 管道俩种格式：最后一个必须是模型，用来最后拟合，还有一个是只需要处理数据的。中间还有个并行处理的特征拼接，必须返回的是np.array格式的特征。
 然后我们由于要模型融合，所以用后一种管道，去处理数据的，将数据分别带入我们构建的三种模型，最后取平均值法融合。
 最后防止过拟合，我们再用其他方法，调参来处理，形成5个提交结果，最后根据各自不同的分数，加权融合，这样可以防止过拟合。
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
