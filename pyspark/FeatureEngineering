
import os
from pyspark import SparkConf, SparkContext
from pyspark.mllib.stat import Statistics
from pyspark.mllib.linalg import Vectors
from pyspark.sql import SQLContext, Row
from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, StringIndexer
from pyspark.ml.feature import CountVectorizer, HashingTF, IDF, Word2Vec
from pyspark.ml.feature import PolynomialExpansion, OneHotEncoder
from pyspark.ml.feature import Normalizer, StandardScaler, MinMaxScaler
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.feature import RFormula, PCA

#常用特征工程的基本操作

#拆分空格放置列表中，并都是小写形式。
def tokenizer(rdd_df,input_name,out_name):
    """
    对给定的DataFrame中对应列的数据采用Tokenizer方式进行文本数据的划分
    :param sentence_df:  输入的DataFrame
    :param split_column_name:  输入的划分数据列名称
    :param word_column_name:  给定进行文本数据划分后的DataFrame中的单词对应的列名称，要求列不存在
    :return:
    """
    tokenizer_df = Tokenizer(inputCol=input_name,outputCol=out_name)
    tokenizer_result_df = tokenizer_df.transform(rdd_df)
    return tokenizer_result_df

#正则表达式拆分
def regexTokenizer(rdd_df,input_name,out_name):
    """
     对给定的DataFrame中对应列的数据采用RegexTokenizer方式进行文本数据的划分
     :param sentence_df:  输入的DataFrame
     :param split_column_name:  输入的划分数据列名称
     :param word_column_name:  给定进行文本数据划分后的DataFrame中的单词对应的列名称，要求列不存在
     :return:
     """
    # 1. 构建模型
    """
    minTokenLength=1, 最终划分出来的单词，最小长度，如果小于该长度的单词，直接被删除掉
    gaps=True, 标记符，功能：给定pattern正则字符串的作用，当gaps为True的时候，pattern给定的是分割符对应的正则字符串；如果为False的时候，pattern给定的是数据对应的正则字符串
    pattern="\\s+", 给定正则字符串
    inputCol=None, 给定输入的列名称
    outputCol=None 给定输出的列名称
    """
    regexTokenizer_df = RegexTokenizer(minTokenLength=2,gaps=False,pattern = '\\w+',inputCol=input_name,outputCol=out_name)
    regexTokenizer_result_df = regexTokenizer_df.transform(rdd_df)
    return regexTokenizer_result_df

#停止词拆分
def remove_stopwords(rdd_df,input_name,out_name):
    """
    对给定的DataFrame中的对应列数据单词做停止词过滤的操作
    :param sentence_df:  输入的DataFrame
    :param word_column_name:  给定的输入的列名称
    :param filter_word_column_name: 给定过滤之后对应的列名称
    :return:
    """
    # 1. 构建模型
    """
    stopWords=None, 给定停止词列表， 默认为：http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words
    caseSensitive=False 进行单词过滤的时候是否考虑大小写
    inputCol=None, 给定输入的列名称
    outputCol=None 给定输出的列名称
    """
    remove_stopwords_df = StopWordsRemover(inputCol=input_name,outputCol=out_name,stopWords=['and', 'hi', 'i', 'about', 'are'])
    remove_stopwords_result_df = remove_stopwords_df.transform(rdd_df)
    return remove_stopwords_result_df

#String类型的类别转换为数值类型的数据
def string_indexer(rdd_df,input_name,out_name):
    """
    对于类别数据给定数字编号，比那好按照出现的次数从多到少给定序号
    :param category_df:  输入的DataFrame
    :param input_string_category_column_name:  输入的字符串类型的列名称
    :param output_indexer_column_name:  给定最终输出的索引列名称
    :return:
    """
    # 1. 构建模型
    """
    inputCol=None, 给定输入的列名称
    outputCol=None 给定输出的列名称
    handleInvalid="error" 设置当进行category类别转换的时候，如果模型中不存在对应的类别，那么做何种操作？可选参数：error(抛出异常)、skip(数据过滤)
    """
    string_indexer_df = StringIndexer(inputCol=input_name,outputCol=out_name,handleInvalid="skip")
    string_indexer_model = string_indexer_df.fit(rdd_df)
    string_indexer_result = string_indexer_model.transform(rdd_df)
    return string_indexer_model,string_indexer_result

#词袋法
def count_tf(word):
    """
    对文本数据进行count tf的转换，即词袋法
    :param word_tf: 给定的DataFrame， 要求输入的列类型必须是单词组成的vector向量类型
    :return:
    """
    # 1. 算法构建
    """
    minTF=1.0 => 当单词的TF值大于等于该值的时候，该单词才会统计其出现的次数
    minDF=1.0 => 当单词的DF值大于等于该值的时候，该单词才会统计其出现的次数
    vocabSize=1 << 18 => 允许最多的特征属性是多少
    inputCol=None => 输入列名称，要求列数据类型是Vector
    outputCol=None => Count TF转换之后的输出DataFrame中对应的列名称，要求列不存在
    """
    count_df = CountVectorizer(inputCol='word',outputCol='features')
    count_model = count_df.fit(word)
    count_result = count_model.transform(word)
    return count_result

#hash词袋法
def hash_tf(word):
    """
    对文本数据进行hash tf的转换
    :param word_tf: 给定的DataFrame， 要求输入的列类型必须是单词组成的vector向量类型
    :return:
    """
    # 1. 算法构建
    """
    numFeatures=1 << 18 => 给定我们转换之后的特征属性的数量
    inputCol=None => 输入列名称，要求列数据类型是Vector
    outputCol=None => Count TF转换之后的输出DataFrame中对应的列名称，要求列不存在
    """
    hash_df = HashingTF(numFeatures=5,inputCol='word',outputCol='features')
    hash_result = hash_df.transform(word) #这里只有转换，没有计算的。
    return hash_result

#TD_IDF
def tf_idf(word):
    """
    对做完TF转换的DataFrame数据做一个IDF转换:做完后hash值一样的会归并在一起，所以会起到降维作用。也就是hash一样变成一个向量。
    :param word_tf: 给定的DataFrame， 要求输入的列类型必须是单词的TF值组成的vector向量类型
    :return:
    """
    # 1. 算法构建
    """
    minDocFreq=0, 当特征值大于等于该值的时候，才会计算IDF，否则直接赋值为0
    inputCol=None => 输入列名称，要求列数据类型是Vector
    outputCol=None => TF-IDF转换之后的输出DataFrame中对应的列名称，要求列不存在
    """
    tf_idf_df = IDF(minDocFreq=1,inputCol='features',outputCol='features1')
    tf_idf_model = tf_idf_df.fit(word)
    tf_idf_result = tf_idf_model.transform(word)
    return tf_idf_result

#WORD_2_VEC
def word_2_vec(word):
    """
    对文本数据进行Word2Vec转换
    :param word_tf: 给定的DataFrame， 要求输入的列类型必须是单词组成的vector向量类型
    :return:
    """
    # 1. 算法构建
    """
    vectorSize=100, 转换成为的词向量中的维度数目，默认为100
    minCount=5, 在转换过程中，单词至少出现的数量，默认为5
    maxIter=1,模型构建过程中的迭代次数，默认为1
    inputCol=None => 输入列名称，要求列数据类型是Vector
    outputCol=None => Count TF转换之后的输出DataFrame中对应的列名称，要求列不存在
    """
    w2v = Word2Vec(vectorSize=5, minCount=1, maxIter=10, inputCol='word', outputCol='features')
    # 2. 模型训练(计算出具有那些单词作为特征属性)
    w2v_model = w2v.fit(word)
    # 3. 使用模型对数据做转换操作
    result_df = w2v_model.transform(word)
    # 4. 结果返回
    return result_df

#多项式展开
def poly_expan(word_df):
    #展开是不需要计算，所以没有fit,相当于x1^2,x2^2,x1.x2这种格式，一般用于增维。
    poly = PolynomialExpansion(degree = 2, inputCol = 'value', outputCol = 'features')
    poly_result = poly.transform(word_df)
    return poly_result

#亚编码
def one_hot_encoder(word_df):
    #这里有个是删除最后一列，一般不用
    one_hot = OneHotEncoder(dropLast = False,inputCol="King",outputCol="features")
    one_result = one_hot.transform(word_df)
    return one_result

#归一化
def normalizer(word_df):
    """
       对数据做一个归一化操作
       :param df:
       :return:
       """
    # 1. 算法对象构建
    # p: 进行归一化的时候，分母进行什么样子的操作，设置为2.0表示使用当前样本对应的特征属性对应的二范式作为分母(所有特征值的平方和开根号)
    normar = Normalizer(p = 3.0,inputCol="value",outputCol="features")
    normal_result = normar.transform(word_df)
    return normal_result

#标准化
def standard_scaler(word_df):
    standard = StandardScaler(withMean=True,inputCol="value",outputCol="features")
    standard_model = standard.fit(word_df)
    standard_result = standard_model.transform(word_df)
    return standard_result

#区间缩放法
def max_min_scaler(word_df):
    #y = x-xmin/max-xmin,所以算法里有计算最小值/最大值，需要fit操作
    max_min_df = MinMaxScaler(min = 0.0,max = 5.0,inputCol="value",outputCol="features")
    max_min_model = max_min_df.fit(word_df)
    max_min_result = max_min_model.transform(word_df)
    return max_min_result


def print_iter(iter):
    for v in iter:
        print(v)

#if __name__ == '__main___':
#创建SparkConf环境和SparkContext上下文对象，并以此创建SQLContext对象
conf = SparkConf().setMaster('local[*]').setAppName('King')
sc = SparkContext(conf = conf)
sqlContext = SQLContext(sparkContext=sc)
#
# #下面是汇总统计和相关性常用函数
rdd4 = sc.parallelize([
    Vectors.dense([0, 2, 3]),
    Vectors.dense([4, 8, 16]),
    Vectors.dense([-7, 8, -9]),
    Vectors.dense([10, -10, 12])
])
rdd7 = sc.parallelize([[1,2,3],[4,5,6],[6,7,8]])
rdd5 = sc.parallelize([1.0, 1.5, 2.0, 2.5, 0.9, 0.86, 0.85, 0.1])
rdd6 = sc.parallelize([10.0, 14.0, 19.0, 30.0, 11.0, 9.5, 9.4, 0.0])
rdd8 = rdd7.map(lambda x:float(x[0]+x[1]+x[2]/3))  #每一行计算还是用sparkcore/sparksql
print(rdd8.collect())
summary = Statistics.colStats(rdd4)
#下面都是特征的数，也就是列，不是每个样本的。(行)，如果取行，就用sparkcore/sparksql的python相关函数计算。
print("类型: ",type(summary))
print("平均值: ",summary.mean())
print("最大值: ",summary.max())
print("方差: ",summary.variance())
print("最小值: ",summary.min())
print("样本数据量: ",summary.count())
print("各个特征属性不为0的样本数目: ",summary.numNonzeros())
print("L1正则: ",summary.normL1())
print("L2正则: ",summary.normL2())
print("各个特征属性之间的相关性: ",Statistics.corr(rdd4,method='pearson'))
print("rdd5,rdd6的相关性: ",Statistics.corr(rdd5,rdd6,method='pearson'))

#下面开始特征工程操作
# 因为我们要转化为DateFrame,将对象转化为结构体,速度加快，而结构体是按照行存储,所以要map到每一行，用Row函数转换为行。
rdd1 = sc.parallelize(
    [
        "Hi I heard about spark",
        "I wish java And could use case classes",
        "Logistic regression models are neat",
        "Softmax regression models and lasso regression"
    ]
).map(lambda line:Row(sentence = line))
rdd2 = sc.parallelize(
    [
        "a", "b", "c", "d", "a", "b", "b", "c", "d", "b"
    ]
).map(lambda line:Row(category = line))
rdd3 = sc.parallelize(
    [
        "a", "b", "c", "d", "e"
    ]
).map(lambda line:Row(category_test = line))

#转化成DateFrame对象,通过SQLContext对象住哪华
rdd1_df = sqlContext.createDataFrame(rdd1)
rdd2_df = sqlContext.createDataFrame(rdd2)
rdd3_df = sqlContext.createDataFrame(rdd3)
#truncate设置False:可以让内容完全输出，如果不设置默认True，这样输出都按照最小行显示，多余内容被...取代
#rdd1_df.show(truncate=False)

# 一、文本数据分词
"""
可以使用jieba来分词；也可以使用SparkCore、SparkSQL根据数据的特征对数据做分词；还可以使用spark mllib自带的API来作词
"""
# 1.1 Tokenizer: 默认按照空格对DataFrame中的对应列的数据进行数据的划分
tokenizer_result_df = tokenizer(rdd1_df,'sentence','King')
print(tokenizer_result_df.schema)
tokenizer_result_df.show(truncate = False)

# 1.2 RegexTokenizer: 根据给定的正则的相关字符串参数，对对应列数据做文本单词的划分
regexTokenizer_result_df = regexTokenizer(rdd1_df,'sentence','King')
print(regexTokenizer_result_df.schema)
regexTokenizer_result_df.show(truncate = False)

# 1.3 StopWordsRemover：删除给定DataFrame中的停止词
#注意:输入的必须是列表，通常是Tokenizer对象(划分后的格式),注意第二个元素是tokenizer_result_df的key
stop_words_remove_result_df = remove_stopwords(tokenizer_result_df,'King','Words')
stop_words_remove_result_df.show(truncate = False)

# 1.4 String类型的类别转换为数值类型的数据,然后按照各个词的数量先后顺序依次排列，这里b最多，所以转为0.0，然后第二多的为1.0，依次类推
string_indexer_model, category_result_df = string_indexer(rdd2_df, 'category', 'King')
category_result_df.show(truncate=False)
string_indexer_model.transform(rdd3_df).show(truncate = False)

# 二、单词转换为特征向量
"""
解决方案：
1. 词袋法：以单词或者单词的hash值作为特征属性，然后计算当前文本再各个特征属性上出现的数量作为特征属性值，从而统计出当前文本对应的特征向量
2. TF-IDF：在词袋法的基础上，加入单词的逆文档频率(单词在所有文档中出现频率越高,越不重要。 )
3. Word2Vec：将单词数据做一个哑编码，然后将哑编码之后的向量作为样本特征输入到神经网络中，最终输出一个给定维度大小的特征向量
"""

#我们取停止词的df来做
word_df = stop_words_remove_result_df.select("Words").toDF('word')
word_df.show(truncate = False)

# 2.1 count tf操作
word_count_result_df = count_tf(word_df)
word_count_result_df.show(truncate=False)
# 2.2 hash tf操作
word_hash_result_df = hash_tf(word_df)
word_hash_result_df.show(truncate=False)
# 2.3 TF-IDF操作,在TF基础上做IDF操作，也就是查看文本频数，所以输入是tf值。
tf_idf_result_df = tf_idf(word_count_result_df)
tf_idf_result_df.show(truncate=False)
# 2.4 Word2Vec操作
word_2_vec_result_df = word_2_vec(word_df)
word_2_vec_result_df.show(truncate=False)

# 三、特征转换
"""
主要操作：
1. 直接使用SparkCore/SparkSQL的相关API对数据进行转换操作，比如: 指数化、对数化、区间化/分区/分桶.....
2. 使用Spark MLlib提供的专门进行数据特征转换的API进行操作，比如：多项式扩展、哑编码、归一化、标准化、区间缩放法等
"""
double_vector_value_df = sqlContext.createDataFrame(sc.parallelize([
    Vectors.dense([1.0, 2.0, 3.0]),
    Vectors.dense([4.0, 5.0, 6.0]),
    Vectors.dense([7.0, 8.0, 9.0])
]).map(lambda t: Row(value=t)))
# 3.1 多项式扩展
poly_result_df = poly_expan(double_vector_value_df)
poly_result_df.show(truncate=False)
# 3.2 哑编码
one_hot_result_df = one_hot_encoder(category_result_df)
one_hot_result_df.show(truncate=False)
# 3.3 归一化处理
norm_result_df = normalizer(double_vector_value_df)
norm_result_df.show(truncate=False)
# 3.4 标准化处理
standard_scaler_result_df = standard_scaler(double_vector_value_df)
standard_scaler_result_df.show(truncate=False)
# 3.5 区间缩放法
min_max_scaler_result_df = max_min_scaler(double_vector_value_df)
min_max_scaler_result_df.show(truncate=False)

#3.6 将多列数据合并成为一列, 合并成为的列数据类型为：Vector
#age, sex, phone, type
#phone: 130 131 155联通，138 187 188移动，其它是电信的
person_rdd = sc.parallelize([
    (21.0, 'M', '131xxxxxxxx', 1),
    (22.0, 'F', '138xxxxxxxx', 1),
    (33.0, 'F', '138xxxxxxxx', 1),
    (34.0, 'M', '131xxxxxxxx', 1),
    (45.0, 'M', '187xxxxxxxx', 1),
    (46.0, 'M', '188xxxxxxxx', 0),
    (24.0, 'F', '155xxxxxxxx', 0),
    (18.0, 'M', '130xxxxxxxx', 0),
    (19.0, 'M', '133xxxxxxxx', 0)
])

def transform_age(age):
    if age>=30.0:
        return 3.0
    elif age>=20.0 and age<30.0:
        return 2.0
    elif age>=10.0 and age<20.0:
        return 1.0
    else:
        return 0.0

def transform_cell(cell):
    if cell in ('130','131','133'):
        return 1.0
    elif cell == '155':
        return 2.0
    else:
        return 3.0

# person_rdd_df =  sqlContext.createDataFrame(person_rdd).map(lambda line:(line[0],transform_age(line[0]),line[1],line[2][:3],transform_cell(line[2][:3]),line[3])) \
#     .map(lambda line:Row(age1 = line[0],age2 = line[1],sex = line[2],cell1 = line[3],cell2 = line[4],p_type = line[5]))


person_rdd1 = person_rdd.map(lambda line:(line[0],transform_age(line[0]),line[1],line[2][:3],transform_cell(line[2][:3]),line[3])) \
    .map(lambda line:Row(age1 = line[0],age2 = line[1],sex = line[2],cell1 = line[3],cell2 = line[4],p_type = line[5]))
person_rdd_df = sqlContext.createDataFrame(person_rdd1)
#将sex和cell转化为数值类型
sex_df2_result = StringIndexer(inputCol="sex",outputCol="sex_int").fit(person_rdd_df).transform(person_rdd_df)
#因为dateframe本质还是rdd,所以根据rdd依赖原则和DAG图原则，是分stage的，所以我们fit上个dateframe的rdd。
cell1_df_result = StringIndexer(inputCol="cell1",outputCol="cell1_int1").fit(sex_df2_result).transform(sex_df2_result)


#亚编码分类
sex2_df = OneHotEncoder(inputCol="sex_int",outputCol="sex_cor").transform(cell1_df_result)
cell1_test_df = OneHotEncoder(inputCol="cell1_int1",outputCol="cell1_test").transform(sex2_df)
cell2_test_df = OneHotEncoder(inputCol="cell2",outputCol="cell2_test").transform(cell1_test_df)

#合并所有项
vector_df = VectorAssembler(inputCols=["age1","age2","sex_cor","cell1_test","cell2_test","p_type"],outputCol="features").transform(cell2_test_df)
vector_df.show(truncate=False)

# 四、特征选择
"""
一般根据特征与特征之间的相关性等指标进行选择，以及根据特征属性和目标属性之间的相关性进行选择
策略：
1. 如果多个特征属性之间存在着比较强的相关性的，那么删除特征属性，只留下其中的某一个特征即可
2. 选择影响目标属性比较大的特征属性，比如和目标属性比较协方差比较大的特征属性留下
3.一般都用corr那个方法来做
"""
# 4.1 RFormula：直接组合(了解即可)
# rformula = RFormula(formula='p_type ~ age + age2 + sex + phone + operator', featuresCol='features', labelCol='label')
# rformula_model = rformula.fit(person_rdd_df)
# rformula_model.transform(person_rdd_df).show(truncate=False)

# 五、降维
"""
一般可以比较形象的理解为：合并特征数据，将多个特征的数据合并成为一个；一般都是通过矩阵的线性变换从而达到合并的效果的/降维的效果的
"""

pca = PCA(k=2, inputCol='features', outputCol='pca_features')
pca_model = pca.fit(vector_df)
pca_model.transform(vector_df).show(truncate=False)


















