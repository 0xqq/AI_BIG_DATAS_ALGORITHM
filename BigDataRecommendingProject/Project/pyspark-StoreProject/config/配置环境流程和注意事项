一.Linux
1.配置:3G内存，20G硬盘,Native模式

2.修改主机名 hostname:查看主机名
          hostname   host_name  ：临时修改主机名
3.永久修改:  vim  /etc/sysconfig/network
          cat /etc/sysconfig/network 
4.重启centos
5.IP、子网掩码、网关、DNS		
这里需要注意，我们的子网与DNS最好一样，点击虚拟机左上角的编辑里的虚拟网络编辑可以查到，在nat选项里，
里面也可以查到我们能用到的IP地址范围，必须遵守这个范围，否则会出错。	
6.重启网络服务
查看ip地址：ifconfig
service  network  restart
7.修改配置文件  /etc/sysconfig/network-scripts/ifcfg-eth0
主机名与IP的映射：/etc/hosts

添加一条
192.168.134.200	hadoop-senior01.ibeifeng.com
		
配置windows的本地映射
C:\Windows\System32\drivers\etc\hosts
		
弄完了别忘了 ping 192.168.134.200  或者 hadoop-senior01.ibeifeng.com

8.关闭防火墙和selinux
  service iptables stop
  chkconfig  iptables  off
  
  注意:
  chkconfig用于很多命令的开机启动，比如mysql的:
  chkconfig mysqld on
  
  vim  /etc/selinux/config
  SELINUX=disabled

9.sudo的配置
	-》应用：临时获取管理员权限执行某些操作
	-》配置sudo：visudo:只有root用户才能执行
		root    ALL=(ALL)       ALL
		表root用户可以通过任何机器以任何身份执行任何命令
		用户	主机名=（以什么用户的身份执行）	可以执行的命令
		beifeng	ALL=(root)	/sbin/service iptables status
		使用：
			sudo command
		beifeng ALL=(root)      /sbin/service iptables status
		beifeng ALL=(root)      NOPASSWD:/sbin/service iptables start
	-》beifeng获取所有权限
		beifeng	ALL=(root)		NOPASSWD:ALL
	-》相关文档
		man sudoers 5

10.Linux磁盘管理
	RAID
	hadoop不需要做RAID
	磁盘类型
	IDE
		hda,hdb
	SCSI：s	
		sda:第一个磁盘（sda1是第一个主分区，sda2第二个主分区... 最多四个
		
		sdb:第二个磁盘
		分区：/dev/sda1  /dev/sda2 /dev/sda3 /dev/sda4
		在CentOS系统中本地硬盘识别为/dev/sda，再把U盘挂载上去后就会识别为/dev/sdb
			
	SATA
	SSD
	相关命令
		fdisk:管理磁盘的命令
		-l:查看当前系统的磁盘分区
		Linux上的存储
		磁盘-》分区-》格式化（得到文件系统类型）-》文件夹
		Linux中分区：2+1,3+1
		主分区
		扩展分区：
		创建逻辑分区
			sda5,sda6……
		逻辑卷组（pvcreate,vgcreate,lvcreate       lvextend,resize2fs）
			pvcreate：动态逻辑卷分区
			vg:动态逻辑卷组
			lv:动态逻辑卷
			主分区+扩展分区《=4
		用于查看当前磁盘利用率
			df -h
	-》添加硬盘，创建分区
		-》添加硬盘
			硬盘进行分区
			fdisk dev_path
			fdisk /dev/sdb
			新建主分区
				新建分区
				格式化
				mkfs -t ext4  /dev/sdb1
				挂载
				mount -l 
				mount /dev/sdb1  /mnt/primary
				重新启动以后还有没有？
			新建扩展分区
				创建分区
				格式化
				mkfs -t ext4  /dev/sdb5
				挂载测试
				mount /dev/sdb5  /mnt/logic
			实现永久挂载
				vim  /etc/fstab
				/dev/sdb1  /mnt/primary     ext4    defaults        0 0
				/dev/sdb5  /mnt/logic       ext4    defaults        0 0
			重启检查
			
Linux中的软件管理
	-》软件的类型
		-》源码
			-》底层用C、C++编写的 
				-》./configure    :预编译
				-》make			  ：编译
				-》make install	  ：安装
			-》底层用Java编写的
				-》maven
		-》二进制
	-》rpm包
		-》rpm
			zlib-1.2.3-29.el6.i686.rpm
			软件的名称-版本-操作系统-操作系统的位数.rpm
		-》查看已安装包
			rpm  -qa | grep java
		-》安装rpm包
			rpm  -ivh  package_path
			rpm -ivh Packages/zlib-devel-1.2.3-29.el6.x86_64.rpm 
		-》卸载
			rpm  -e   package_name
			rpm -e zlib-devel-1.2.3-29.el6.x86_64
			rpm -e --nodeps java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64
			不考虑依赖强制删除
		-》依赖问题：
			rpm -ivh Packages/zlib-devel-1.2.3-29.el6.i686.rpm
		-》查看某个命令来自哪个包
			whereis man
			rpm -qf /usr/bin/man
			rpm -qf  command_path
		-》查看某个包装了哪些文件
			rpm -ql  package_name
			rpm -ql tzdata-java-2013g-1.el6.noarch
	-》yum 安装
		-》本地必须要有yum源
			ll /etc/yum.repos.d/
		-》系统可以联网
		-》查看哪些包可以安装
			yum list
			yum list installed
		-》安装
			yum install -y name1  name2 
			yum install  -y  name
		-》删除
			yum remove  -y name
	-》安装JDK
		-》包上传到Linux
		-》tar解压到/opt/modules/
		-》全局设置JAVA_HOME
			vim /etc/profile
			export  JAVA_HOME=/opt/modules/java-
			export  PATH=$PATH:$JAVA_HOME/bin
		-》source  /etc/profile
		-》java -version
		

二.mysql,tcal以及其他工具包:
配置元数据MySQL
1、Derby数据库只能启动一个实例
2、配置元数据的存储
3、安装MySQL
	查询当前系统有没有装MySQL
	$ sudo rpm -qa | grep mysql
	$ sudo rpm -e --nodeps mysql-libs-5.1.66-2.el6_3.x86_64
	在线安装MySQL
	$ sudo yum -y install mysql-server
	检查MySQL服务有没有启动
	sudo service mysqld status
	设置开机启动
	$ sudo chkconfig mysqld on
	设置MySQL管理员密码
	$ mysqladmin -u root password '123456'
	进入MySQL
	mysql -uroot -p
4、配置hive的相关企业配置
	创建一个hive-site.xml
	cp hive-default.xml.template hive-site.xml
	
创建好mysql后,需要设置用户权限
1.create user 'root' identified by 'root';
2.给root授权,这是用本地进行访问时给root用户的权限
grant all on *.* to root@localhost identified by 'root';
3.同理也得给master授权
grant all on *.* to root@master identified by 'root';
4.创建数据库
create database mahout;

5.解压tcl包，需要有gcc编译器
  yum install gcc
  yum install gcc-c++
  
免密码登陆

直接打
ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
再打：
cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys

第二种:
ssh-keygen -t rsa
ssh-copy-id 主机名
 
id_rsa私钥
id_rsa.pub公钥
常见错误： 1、格式化 不要多次格式化，如果要格式化，先删除tmp目录 2、集群ID不一致

三.安装hadoop

1. 配置 core-site.xml hdfs管理者namenode的地址和存放数据的tmp目录

<configuration>
<property>
<name>fs.defaultFS</name>
<value>hdfs://master:9000</value>
</property>
<property>
<name>hadoop.tmp.dir</name>
<value>/home/master/tmp</value>
</property>
</configuration>

2.hdfs-site.xml，指定副本数个数
<property>
<name>dfs.replication</name>
<value>1</value>
</property>

经过查看:
/home/jin/opt/cdh5.3.6/hadoop-2.5.0-cdh5.3.6/data/tmp/dfs/data/current/BP-1586701327-192.168.153.200-1501304933041/current/finalized/subdir0/subdir0
里面的blk是存放块文件的,里面是json格式的

/home/jin/opt/cdh5.3.6/hadoop-2.5.0-cdh5.3.6/data/tmp/dfs/name/current
是存放namenode的元数据,里面有edits和fsimage

/home/jin/opt/cdh5.3.6/hadoop-2.5.0-cdh5.3.6/data/tmp/dfs/namesecondary/current

是存放 namesecondary 的元数据,里面有edits和fsimage

3.slaves，
指定从节点位置，包括datanode和nodemanager
格式化namenode，对于元数据进行初始化，否则无法读取元数据
正常三台分布式配置如下。我们这里是本地就默认localhost
        
hadoop-senior01.ibeifeng.com
hadoop-senior02.ibeifeng.com
hadoop-senior03.ibeifeng.com

4.mapred-site.xml文件名称，指定MR运行在yarn上
 <property>
 <name>mapreduce.framework.name</name>
 <value>yarn</value>
 </property>
 -》yarn-site.xml
 <property>
 <name>yarn.nodemanager.aux-services</name>
 <value>mapreduce_shuffle</value>
 </property>
 <property>
     <name>yarn.resourcemanager.hostname</name>
     <value>hadoopMaster</value>
 </property>
 yarn外部管理界面端口号：8088
 HDFS外部管理界面端口号：50070
 secondarynamenode外部管理界面端口号：50090

5.Hadoop自带历史服务器
    1、mapred-site.xml：
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>hadoop-senior01.ibeifeng.com:10020</value>
    </property>
     
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>hadoop-senior01.ibeifeng.com:19888</value>
    </property>
    
开启:
sbin/mr-jobhistory-daemon.sh start historyserver
 
6.日志聚集
    ->yarn-site.xml
<property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
     
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>106800</value>
    </property>
 
7.secondarynamenode
    1、namenode元数据-》内存-》启动后
    2、namenode元数据-》本地文件系统文件中-》启动前
    3、格式化-》fsimage镜像文件
    4、edits编辑日志文件-》用来记录用户行为记录数据
    5、减少namenode重启的时间、合并文件
    6、secondarynamenode读取两类文件，加载到内存
            -》写到一个新的 fsimage 镜像文件
            -》再生成一个 edits 编辑日志文件
    dfs.blocksize
所以原来的没了，这样相当于找了一个内存让他重新生成，而内存速度快，所以它具有重启速度快的特点。端口号:
50090

在hdfs-site.xml下配置:
   <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>

    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop-senior01.ibeifeng.com:50090</value>
    </property>
    
    自定义块大小属性，字节
    
PS:hadoop常见错误:
常见错误：
    1、格式化
    不要多次格式化，如果要格式化，先删除tmp目录
    2、集群ID不一致
    
    /home/jin/opt/cdh5.3.6/hadoop-2.5.0-cdh5.3.6/data/tmp/dfs/data/current 这里可以查看id地址。
    clusterID=CID-3ca65fdf-ee12-4d06-be68-1cc39b479f68
    clusterID=CID-3ca65fdf-ee12-4d06-be68-1cc39b479f68
    3、端口号被占用
    4、多用户混用，造成PID文件冲突
 
Hadoop配置文件
    默认、自定义
    每个模块对应一个文件
    系统启动会先加载默认配置文件
    默认配置文件存放在jar中
    自定义配置文件优先级高于默认配置文件
     
    如果maven pom配置有问题，在这个目录下都删除再重新配置。
    repository/org/apache/hadoop
    
    启动eclipse前需要输入:
    
    root用户下 ： xhost + (别忘加空格)
    或者普通用户下:
    sudo xhost + (别忘加空格)
    
    创建文件后别忘这个:
    sudo chown -R jin:jin * 
    
    我们所说的job是我们要提交的任务。

四.安装redis
redis 需要的内存是比较大的，只有类型列表，没有表。一定程度上也可以持久化到磁盘上。
必须在root用户下做
进入/home/master/tcl8.6.1 也就是在tcl目录下。

输入:
cd unix &&
./configure --prefix=/usr \
--mandir=/usr/share/man \
--without-tzdata \
$([ $(uname -m) = x86_64 ] && echo --enable-64bit) &&
make &&
sed -e "s@^\(TCL_SRC_DIR='\).*@\1/usr/include'@" \
-e "/TCL_B/s@='\(-L\)\?.*unix@='\1/usr/lib@" \
-i tclConfig.sh
make install &&  
make install-private-headers &&  
ln -v -sf tclsh8.6 /usr/bin/tclsh && 
chmod -v 755 /usr/lib/libtcl8.6.so

top查看虚拟内存

下面是正式安装redis

解压后进入redis目录，编译:
make -j 4
编译好后，cd src
make test
回到redis目录，输入
make install
然后启动redis,因为我们每次都用到，让它启动在后台。
nohup redis-server &
完事后输入
tail -f nohup.out
查看日志
也可以查看进程
ps -ef|grep redis-server
每次启动redis客户端前，要输入redis-server 启动服务端，再克隆一个界面启动客户端
进入客户端
redis-cli
输入ping 如果显示pong就是启动成功了。

五.安装Kafka
这里使用kafka自带的zookeeper集群
进入kafka后
cd config
可以查看zookeeper信息
便于操作可以配置bash_profile环境

#JAVA
export JAVA_HOME=/home/master/software/jdk1.7.0_71
export PATH=$PATH:$JAVA_HOME/bin

#HADOOP
export HADOOP_HOME=/home/master/software/hadoop-2.6.5
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

#MYSQL
export MYSQL_HOME=/home/master/software/mysql
export PATH=$PATH:$MYSQL_HOME/bin

#KAFKA
export KAFKA_HOME=/home/master/software/kafka_2.10-0.8.2.2
export PATH=$PATH:$KAFKA_HOME/bin

#SPARK_HOME
export SPARK_HOME=/home/master/software/spark-1.6.2-bin-hadoop2.6
export PATH=$PATH:$SPARK_HOME/bin

因为也是开机启动zookeeper,所以也用后台方式启动，回到kafka目录，输入
nohup bin/zookeeper-server-start.sh config/zookeeper.properties &
再输入
tail -F nohup.out
查看日志
开启broker
在config/server.propers 查看信息
启动:
 nohup bin/kafka-server-start.sh config/server.properties &
查看日志信息
tail -F nohup.out  

六.安装spark
设置好后，开启命令
进入spark目录
运行spark:
spark-submit --class org.apache.spark.examples.SparkPi \
--master yarn-client \
--num-executors 3 \
--driver-memory 1g \
--executor-memory 1g \
--executor-cores 1 \
lib/spark-examples*.jar \
10
上面这命令我们一般不用，而是用spark的交互式命令:
spark-shell --master local[2]

七. 配置hbase文件
vi hbase-site.xml
<configuration>
//设置hbase客户端启动
 <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
 </property>
//设置hbase存放目录，在hdfs创建个/hbase文件目录
 <property>
        <name>hbase.root.dir</name>
        <value>/hbase</value>
 </property>
</configuration>

vi hbase-env.sh
ESC /ZOOKEEPER 这是Citrl+F操作，找到zookeeper看是否配置，我们直接用自带即可。

start-hbase.sh 打开进程看看。
会发现给出提示 JAVA_HOME is not 设置。
去设置java
vi hbase-env.sh
找到JAVA_HOME。改成自己的。

hbase shell
出错:
java.lang.IncompatibleClassChangeError: Found class jline.Terminal, but interface was expected
解决
问题原因：/usr/java/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar 版本低了
解决办法： 
rm -rf  /usr/java/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar
cp /usr/java/apache-hive-2.1.1-bin/lib/jline-2.12.jar /usr/java/hadoop-2.6.5/share/hadoop/yarn/lib/

八.Flume  1.5版本
常见问题汇总:

https://blog.csdn.net/lijinqi1987/article/details/77449889

flume怎么找到hdfs？
-》找HADOOP_HOME
-》从core-site
-》直接写明hdfs的绝对路径
-》需要hdfs的jar包？
-》类型：
-》flume在hadoop集群中
	-》修改flume-env.sh，添加jdk路径
-》flume在hadoop集群中，hdfs做了高可用
	-》第一步同上
	-》将core-site和hdfs-site拷贝到conf目录
-》flume不在hadoop集群中
	-》前两步同上
	-》将相关jar包放到lib目录
	-》命令
			
Usage: bin/flume-ng <command> [options]...

模板：bin/flume-ng agent  --conf  flume_conf_dir  --name  agent_name --conf-file  file_path  -Dflume.root.logger=INFO,console

为Flume采集数据创建HDFS目录
hdfs dfs -mkdir -p /DataCollection/weblogs

在Linuex创建Flume采集后的数据的存储目录(会发生后缀名改变)
sudo mkdir -p /home/master/tmp/flume/weblogs

测试：
source是我们创建的 /home/master/tmp/flume/weblogs目录下的数据 。
Sink 是 HDFS sink：
写文件到HDFS 我们创建的 /DataCollections/weblogs 目录
设置 hdfs.rollInterval 为 0 来禁用基于时间的文件轮转
设置 hdfs.rollCount 为 0 来禁用基于事件的文件轮转
设置 hdfs.rollSize 为 524288 来启用基于尺寸的文件轮转为 512K
设置 hdfs.fileType 为 DataStream 写原始文本文文件(不是序列文
件格式)
 Channel 是 Memory Channel：
 使用 capacity 属性可以存储 100000 个事件 。通常设置大。
 使用 transactionCapacity 属性设置 10000 个事务容量、通常比capacity小10倍。

在flume-env.sh.template改成flume-env.sh.再把里面java_home修改。
再修改flume-conf.properties.template成flume-conf.properties。vi到里面
ESC后修改agent名字: :%s+agent.+wlagent. 改成wlagent.

先配置source
# For each one of the sources, the type is defined
wlagent.sources.weblogSrc.type = spooldir
wlagent.sources.weblogSrc.spoolDir =/home/master/tmp/flume/weblogs_sqooldir  //第二行要告诉路径

配置跟HDFS相关
# Each sink's type must be defined
wlagent.sinks.hdfsSink.type = hdfs
wlagent.sinks.hdfsSink.type.path =/dylan/weblogs      //注意这第二行都要告诉路径，这里是HDFS上
wlagent.sinks.hdfsSink.hdfs.rollInterval =0
wlagent.sinks.hdfsSink.hdfs.rollCount =0
wlagent.sinks.hdfsSink.hdfs.rollSize = 5242880   //都是设置roll
wlagent.sinks.hdfsSink.hdfs.fileType = DataStream  //原始文本文件

wlagent.channels.memoryChannel.type = memory

wlagent.channels.memoryChannel.capacity = 100000
wlagent.channels.memoryChannel.transactionCapacity = 10000

开始运行agent
bin/flume-ng agent -n wlagent --conf /conf --conf-file /home/master/software/apache-flume-1.5.0-bin/conf/flume-conf.properties
可能出 Unsupported major.minor version 52.0 错误说明你的java版本过低。
将日志文件复制到tmp/flume/web... 中。此时flume上的日志会变化，开始处理我们复制过去的日志文件。处理完的文件，会打上.completed 。没打上的就是没处理完。
处理过程中会产生一些可能发生的错误。
java 的内存溢出。内存不够了。
因为自身java的堆内内存是20M，很低的。
vi flume-env.sh
export JAVA_OPTS=" -Xmx500m"
记住一般都在xxx-env.sh修改java内存。

再重新启动，此时不用担心前面处理的文件，因为默认机制会继续下个agent处理。
如果还有溢出
ps -ef|grep flume

注意:这里会有flume的memeryChannel中transactionCapacity和sink的batchsize需要注意事项
详细看: https://blog.csdn.net/joy6331/article/details/51279670




启动时修改日志级别的命令  console:终端的意思。
bin/flume-ng agent -n wlagent --conf/$FLUME_HOME/conf --conf-file flume-conf.properties -Dflume.root.logger=INFO,console, 

最后hdfs dfs -ls /dylan/weblogs  查看日志是否上传到HDFS上。
可以hdfs dfs -cat /dylan/weblogs/文件名  或者  hdfs dfs -cat /dylan/weblogs/文件名|cat
此时我们看到的是其他时间日志，因为重启过服务器，因为channel是memory,而1-16日数据会在内存中丢失，所以如果保险就改成filechannel.
解决方案：将原始日志丢失的重新cp到weblogs文件中，flume会自动直接处理。

小提示: rm -rf logs.2016-12-1[1-7] 可以删除11号到17号的日志

九.Sqoop
直接安装即可

十.Linux的 IDEA

十一.Mahout

十二.SparkMlib



