import pickle
import numpy as np
import pandas as pd
import xgboost as xgb
import lightgbm as lgb
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.model_selection._split import check_cv
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_selection import VarianceThreshold
from scipy.stats import skew, kurtosis
from scipy import sparse
from sklearn.cross_validation import StratifiedKFold
from sklearn.cross_validation import KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.random_projection import SparseRandomProjection
import matplotlib
import itertools
import operator
import warnings
warnings.filterwarnings("ignore")
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.base import TransformerMixin
from sklearn import cross_validation
from matplotlib import pylab as plt

'''
常见错误异常
1.ValueError: cannot reindex from a duplicate axis
原因(1)：原始索引中可能有重复的值。找到他们这样做：
解决: df[df.index.duplicated()]
原因(2):新建一个空的DataFrame，只有列索引，没有对应的数据，当你需要引用对应的Series数据到DataFrame中时
如果每个Series的index都不一样，那么添加时就会出现NaN值，甚至是错误提示
解决:使用reset_index()方法来重置它们的索引，以便后续的操作。
print(df.reset_index())

2.
'''

'''
from sklearn.cross_validation import cross_val_score, ShuffleSplit
from sklearn.datasets import load_boston
from sklearn.ensemble import RandomForestRegressor

#Load boston housing dataset as an example
boston = load_boston()
X = boston["data"]
Y = boston["target"]
names = boston["feature_names"]

rf = RandomForestRegressor(n_estimators=20, max_depth=4)
scores = []
for i in range(X.shape[1]):
     score = cross_val_score(rf, X[:, i:i+1], Y, scoring="r2",
                              cv=ShuffleSplit(len(X), 3, .3))
     scores.append((round(np.mean(score), 3), names[i]))
print sorted(scores, reverse=True)
'''




train = pd.read_csv('D:\\kaggle比赛\\装潢公司销量预测\\data\\train.csv',index_col = 0)
test = pd.read_csv('D:\\kaggle比赛\\装潢公司销量预测\\data\\test.csv',index_col = 0)
store = pd.read_csv('D:\\kaggle比赛\\装潢公司销量预测\\data\\store.csv',index_col = 0)

#join它们，并提取目标函数为了下面分析数据做准备，同时提取出数值型和非数值型
#这里直接去Sales大于0的，小于等于0没意义
train = train[train['Sales']>0]
train_df = pd.merge(train,store,on = 'Store',how = 'left')
test_df = pd.merge(test,store,on = 'Store',how = 'left')
train_y = train_df['Sales']
print(train_df.shape," ",test_df.shape)

#注意:此时不能做可视化，因为我们是left join的数据，会出现大量nan值。我们可以先做偏差风度等查看特征性
# def obchar():
#     def min_x(x):
#         return x.mean()
#     def max_x(x):
#         return x.max()
#     def std_x(x):
#         return x.std()
#     def median_x(x):
#         return x.median()
#     def count_x(x):
#         return x.count()
#     train_df_feature_numeric = train_df.loc[:,feature_numeric]
#     train_num = pd.DataFrame({'count':train_df_feature_numeric.apply(count_x),
#                               'min':train_df_feature_numeric.apply(min_x),
#                               'max':train_df_feature_numeric.apply(max_x),
#                               'std':train_df_feature_numeric.apply(std_x),
#                               'median':train_df_feature_numeric.apply(median_x),
#                               'skew':train_df_feature_numeric.apply(skew),
#                               'kurtosis':train_df_feature_numeric.apply(kurtosis)})
# 由上面可推断出: CompetitionDistance CompetitionOpenSinceMonth  Promo2SinceWeek Promo2SinceYear 特征性很强
#同理推出: PromoInterval 特征性很强，我们需要对这种时间类的特征性强数据，进行离散化，使其数据更平滑，更趋近于正态
#分布。而距离这块，我们会重点关注其维度。
#而这里的日期根据业务取决于促销活动和节假日影响，所以我们离散化俩种日期，一个是查看是否促销，一个月份离散化。
#(1)拆分训练集函数
#注意:train_df取大于0的数
#日期分成12个月份
date_num_train = pd.DataFrame(
    columns=['year', 'month', 'day', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'
        , 'promojan', 'promofeb', 'promomar', 'promoapr', 'promomay', 'promojun', 'promojul', 'promoaug', 'promosep'
        , 'promooct', 'promonov', 'promodec'])
print(train_df.Date.head())
date_num_train['year'] = train_df.Date.apply(lambda x:x.split('/')[0]).astype(float)
date_num_train['month'] = train_df.Date.apply(lambda x:x.split('/')[1]).astype(float)
date_num_train['day'] = train_df.Date.apply(lambda x:x.split('/')[2]).astype(float)

#离散化月时间点，构建稀疏矩阵，主要是离散化时间。
date_num_train['jan'] = date_num_train.month.apply(lambda x:1 if x == 1. else 0)
date_num_train['feb'] = date_num_train.month.apply(lambda x:1 if x == 2. else 0)
date_num_train['mar'] = date_num_train.month.apply(lambda x:1 if x == 3. else 0)
date_num_train['apr'] = date_num_train.month.apply(lambda x:1 if x == 4. else 0)
date_num_train['may'] = date_num_train.month.apply(lambda x:1 if x == 5. else 0)
date_num_train['jun'] = date_num_train.month.apply(lambda x:1 if x == 6. else 0)
date_num_train['jul'] = date_num_train.month.apply(lambda x:1 if x == 7. else 0)
date_num_train['aug'] = date_num_train.month.apply(lambda x:1 if x == 8. else 0)
date_num_train['sep'] = date_num_train.month.apply(lambda x:1 if x == 9. else 0)
date_num_train['oct'] = date_num_train.month.apply(lambda x:1 if x == 10. else 0)
date_num_train['nov'] = date_num_train.month.apply(lambda x:1 if x == 11. else 0)
date_num_train['dec'] = date_num_train.month.apply(lambda x:1 if x == 12. else 0)

#离散化促销活动时间点，以便使其平滑化
print(train_df.PromoInterval)
date_num_train['promojan'] = train_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Jan" in x else 0)
date_num_train['promofeb'] = train_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Feb" in x else 0)
date_num_train['promomar'] = train_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Mar" in x else 0)
date_num_train['promoapr'] = train_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Apr" in x else 0)
date_num_train['promomay'] = train_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "May" in x else 0)
date_num_train['promojun'] = train_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Jun" in x else 0)
date_num_train['promojul'] = train_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Jul" in x else 0)
date_num_train['promoaug'] = train_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Aug" in x else 0)
date_num_train['promosep'] = train_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Sep" in x else 0)
date_num_train['promooct'] = train_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Oct" in x else 0)
date_num_train['promonov'] = train_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Nov" in x else 0)
date_num_train['promodec'] = train_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Dec" in x else 0)
train_df = pd.concat((train_df,date_num_train),axis = 1)
print(train_df.shape)
train_df.head()

#拆分测试集
date_num_test = pd.DataFrame(
columns=['year','month','day','jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec'
,'promojan' ,'promofeb' ,'promomar' ,'promoapr' ,'promomay' ,'promojun' ,'promojul' ,'promoaug' ,'promosep'
,'promooct' ,'promonov' ,'promodec'])
print(test_df.Date.head())
date_num_test['year'] = test_df.Date.apply(lambda x:x.split('-')[0]).astype(float)
date_num_test['month'] = test_df.Date.apply(lambda x:x.split('-')[1]).astype(float)
date_num_test['day'] = test_df.Date.apply(lambda x:x.split('-')[2]).astype(float)

#离散化月时间点，构建稀疏矩阵，主要是离散化时间。
date_num_test['jan'] = date_num_test.month.apply(lambda x:1 if x == 1. else 0)
date_num_test['feb'] = date_num_test.month.apply(lambda x:1 if x == 2. else 0)
date_num_test['mar'] = date_num_test.month.apply(lambda x:1 if x == 3. else 0)
date_num_test['apr'] = date_num_test.month.apply(lambda x:1 if x == 4. else 0)
date_num_test['may'] = date_num_test.month.apply(lambda x:1 if x == 5. else 0)
date_num_test['jun'] = date_num_test.month.apply(lambda x:1 if x == 6. else 0)
date_num_test['jul'] = date_num_test.month.apply(lambda x:1 if x == 7. else 0)
date_num_test['aug'] = date_num_test.month.apply(lambda x:1 if x == 8. else 0)
date_num_test['sep'] = date_num_test.month.apply(lambda x:1 if x == 9. else 0)
date_num_test['oct'] = date_num_test.month.apply(lambda x:1 if x == 10. else 0)
date_num_test['nov'] = date_num_test.month.apply(lambda x:1 if x == 11. else 0)
date_num_test['dec'] = date_num_test.month.apply(lambda x:1 if x == 12. else 0)

#离散化促销活动时间点，以便使其平滑化,这里x不能属于float类型，所以我们增加了俩层判断，下面可解释
date_num_test['promojan'] = test_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Jan" in x else 0)
date_num_test['promofeb'] = test_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Feb" in x else 0)
date_num_test['promomar'] = test_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Mar" in x else 0)
date_num_test['promoapr'] = test_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Apr" in x else 0)
date_num_test['promomay'] = test_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "May" in x else 0)
date_num_test['promojun'] = test_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Jun" in x else 0)
date_num_test['promojul'] = test_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Jul" in x else 0)
date_num_test['promoaug'] = test_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Aug" in x else 0)
date_num_test['promosep'] = test_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Sep" in x else 0)
date_num_test['promooct'] = test_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Oct" in x else 0)
date_num_test['promonov'] = test_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Nov" in x else 0)
date_num_test['promodec'] = test_df.PromoInterval.apply(lambda x: 0 if isinstance(x, float) else 1 if "Dec" in x else 0)
test_df = pd.concat((test_df,date_num_test),axis = 1)
print(test_df.shape)
print(test_df.head()," ",train_df.head())

#查看缺省值情况
train_df.isnull().sum().sort_values(ascending=False)
#nan值过多的我直接删去
train_df.drop('Promo2SinceWeek',axis = 1,inplace=True)
train_df.drop('Promo2SinceYear',axis = 1,inplace=True)
train_df.drop('PromoInterval',axis = 1,inplace=True)
train_df.drop('CompetitionOpenSinceMonth',axis = 1,inplace=True)
train_df.drop('CompetitionOpenSinceYear',axis = 1,inplace=True)
#查看缺省值情况
print(train_df.isnull().sum().sort_values(ascending=False))
#根据业务情况，customer,distance取均值，open剩下都取1,记住要改变原对象，所以加inplace = True
train_df.CompetitionDistance.fillna(train_df.CompetitionDistance.mean(),inplace=True)
print(train_df.isnull().sum().sort_values(ascending=False))

#查看缺省值情况
test_df.isnull().sum().sort_values(ascending=False)
#nan值过多的我直接删去
test_df.drop('Promo2SinceWeek',axis = 1,inplace=True)
test_df.drop('Promo2SinceYear',axis = 1,inplace=True)
test_df.drop('PromoInterval',axis = 1,inplace=True)
test_df.drop('CompetitionOpenSinceMonth',axis = 1,inplace=True)
test_df.drop('CompetitionOpenSinceYear',axis = 1,inplace=True)
#查看缺省值情况
print(test_df.isnull().sum().sort_values(ascending=False).head(5))
#根据业务情况，customer,distance取均值，open剩下都取1,记住要改变原对象，所以加inplace = True
test_df.CompetitionDistance.fillna(test_df.CompetitionDistance.mean(),inplace=True)
test_df.Open.fillna(1.,inplace = True)
print(test_df.isnull().sum().sort_values(ascending=False))

#考虑到离散化后的特征改变，以及后面可能用到深度学习对特征敏感，所以进行归一化操作。
#提取数值型和非数值型
print(train_df.shape," ",test_df.shape)
features = test_df.columns.tolist()
number_types = ['int16','int32','int64','float16','float32','float64']
feature_numeric = test_df.select_dtypes(include = number_types).columns.tolist()
feature_non_numeric = [i for i in features if i not in feature_numeric]
print(len(features)," ",len(feature_numeric)," ",len(feature_non_numeric))
print(train_df.shape," ",test_df.shape)
print(feature_non_numeric)
#(1)非数值型进行编码操作
#注意: loc 里的坐标都是从1开始算,而且不是索引，是横纵坐标的行列名称。
#这里我们用ix
#遇到的问题:
#（1） TypeError: 'DataFrame' object is not callable
#其实就是变量名和python自身的global变量重名了
#详细解决：https://blog.csdn.net/junbin_h/article/details/68066143
#(2)bad input shape()
#transform里也是要求列表格式，用list转换。
#详细解决:https://jingyan.baidu.com/article/ce09321b846bd72bff858f14.html
'''
bb = train[0:3][['Date','DayOfWeek']] 左开右闭
1      2015/7/31          5
2      2015/7/31          5
3      2015/7/31          5
'''
#这里由于运行速度太慢，先注释掉，后期合成时再打开。
label = LabelEncoder()
for i in range(844338):
    list1 = train_df[i:i+1][feature_non_numeric]
    label.fit(list(list1))
    train_df[i:i+1][feature_non_numeric] = label.transform(list(list1))

#(2)数值型进行标准化操作
#此时如果标准化，因为是稀疏矩阵，会出现大量nan值。
# number_math_train = train_df.columns[train_df.dtypes!='object']
# number_math_test = test_df.columns[test_df.dtypes!='object']
# print(list(number_math_train))
# print(list(number_math_test))
# num_mean_train = train_df.loc[:,number_math_train].min()
# num_mean_test = test_df.loc[:,number_math_test].min()
# num_std_train = train.loc[:,number_math_train].max()
# num_std_test = test_df.loc[:,number_math_test].max()
# train_df.loc[:,number_math_train] = (train_df.loc[:,number_math_train]-num_mean_train)/(num_std_train-num_mean_train)
# test_df.loc[:,number_math_test] = (test_df.loc[:,number_math_test]-num_mean_test)/(num_std_test-num_mean_train)
#
# print(train_df.head())
# print(test_df.isnull().sum().sort_values(ascending=False))
# print(train_df.isnull().sum().sort_values(ascending=False))

from sklearn.model_selection import train_test_split

def ToWeight(y):
    w = np.zeros(y.shape, dtype=float)
    ind = y != 0
    w[ind] = 1./(y[ind]**2)
    return w

def rmspe(yhat, y):
    w = ToWeight(y)
    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))
    return rmspe

def rmspe_xg(yhat, y):
    # y = y.values
    y = y.get_label()
    y = np.exp(y) - 1
    yhat = np.exp(yhat) - 1
    w = ToWeight(y)
    rmspe = np.sqrt(np.mean(w * (y - yhat)**2))
    return "rmspe", rmspe

#分离数据,做交叉验证
X_train,y_train = train_test_split(train_df,test_size=0.2,random_state = 2018)

#
# #构建XGB数据模型
# xgb_val = xgb.DMatrix(X_test,label=y_test)  #交叉集
# xgb_train = xgb.DMatrix(X_train,label=y_train)  #训练集
#
# #开始创建模型，
# xgb_params = {
#     'objective': 'reg:linear',
#     'booster': 'gbtree',
#     'learning_rate': 0.02,
#     'max_depth': 10,
#     'max_leaf_nodes': 100,
#     'eta':0.01
#     'min_child_weight': 3,
#     'subsample': 0.87,
#     'colsample_bytree': 0.679,
#     'colsample_bylevel': 0.50,
#     'n_jobs': -1,
#     'random_state': 2018
# }
#
# plst = list(xgb_params.items())
# num_rounds = 2000
# watchlist = [(xgb_train,"train"),(xgb_val,"val")]
#
# #交叉验证
#
# watchlist
# result_xgb = xgb.cv(params, xgb_train, num_boost_round=200, nfold=4,
#                 early_stopping_rounds=15, verbose_eval=True,
#                 folds=StratifiedKFold(n_splits = num_cv,shuffle = True,
#                 random_state = 5).get_n_splits(X_train.values))






