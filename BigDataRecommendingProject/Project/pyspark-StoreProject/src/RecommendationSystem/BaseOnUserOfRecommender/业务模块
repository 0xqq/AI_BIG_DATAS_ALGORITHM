
一.业务描述:
基于用户的历史行为记录和用户人口统计信息以及他们参加的活动，来预测用户的感兴趣事件，以便我们后续对用户推荐他们所感兴趣的
事件。所以这是个基于用户历史行为的推荐系统。我们最多推荐200个活动。

二.数据描述:
train.csv  有六列：   用户，事​​件， 邀请，时间戳，感兴趣和不感兴趣。  Test.csv  包含与train.csv相同的列，除了感兴趣和不感兴趣。
每行对应于在我们的应用程序中向用户显示的事件。  事件是标识我们系统中的事件的标识。  用户是代表我们系统中的用户的ID。
邀请 是一个二进制变量，表示用户是否被邀请参加该活动。 timestamp是一个ISO-8601 UTC时间字符串，
表示当用户在我们的应用程序中看到事件时的大致时间（+/- 2小时）。 有兴趣是一个二进制变量，用于指示用户是否单击此事件的“感兴趣”按钮;
如果用户点击兴趣，则为1，如果用户没有点击该按钮，则为0。同样， not_interested是一个二进制变量，指示用户是否单击此事件的“不感兴趣”按钮;
如果用户点击该按钮则为1，否则为0。这可能是用户看到的事件，并点击既不感兴趣，也没有兴趣，因此存在包含0,0为值的行 感兴趣，not_interested。

users.csv包含我们的一些用户（包括出现在火车和测试文件中的所有用户）的人口统计数据，它包含以下列：  user_id，  locale，
birthyear，  gender，  joinedAt，  位置和  时区。 user_id是我们系统中用户的ID。  locale是一个表示用户语言环境的字符串，
它应该是格式 language _ territory。birthyear 是代表用户出生年份的4位整数。 性别是男性还是女性，取决于用户的性别。
joinedAt是一个ISO-8601 UTC时间字符串，表示用户第一次使用我们的应用程序的时间。  位置  是表示用户位置的字符串（如果知道）。
时区是一个有符号整数，表示用户的UTC偏移量（以分钟为单位）。

user_friends.csv包含有关此用户的社交数据，并包含两列：用户和朋友。  用户是我们系统中的用户ID， 朋友是用户朋友ID的空格分隔列表。

events.csv  包含有关我们系统中事件的数据，并且有110列。前九栏是  event_id，user_id，start_time，city，state，zip，
country， lat和lng。 event_id是事件的ID，user_id  是创建事件的用户的ID。  城市， 州， 邮政编码和
国家代表关于场地位置的更多细节（如果知道）。  lat和lng  是代表场地经纬度坐标的花车，四舍五入到小数点后三位。
start_time 是表示事件计划何时开始的ISO-8601 UTC时间字符串。最后101列需要更多解释; 首先，我们确定了100个最常见的词干
（通过Porter Stemming获得）出现在我们事件的大型随机子集的名称或描述中。最后101列是count_1， count_2，...， count_100，
count_other，其中 count_N是一个整数，表示第N个最常用词干出现在该事件的名称或描述中的次数。  count_other
是其余词干不是100个最常见词干之一的词的数量。

event_attendees.csv包含关于哪些用户参加了各种活动的信息，并且具有以下列：  event_id，是，可能，
受邀以及否。event_id 标识事件。是的，也许， 邀请，并没有为用户ID代表用户的空间分隔的名单谁表示，
他们打算，也许会，邀请，或不打算事件。


三.业务数据分析:表面看是个回归，其实我们可以把它当做是二分类，最终目的我们是判断用户是否感兴趣。然后将感兴趣的事件形成一个列表，
进行排序，最终进行排序，然后推荐给相应的用户。
而根据常识，我们可以判断出一些强特征，可以一些特征组合可能会影响我们的目标值,我个人总结如下:
(1)gender:性别，男性女性感兴趣事件大部分是不一样的，所以方差一定大，可以列为强特征。
(2)location: 地理位置，不同区域，不同省市的人，感兴趣的事件也可能不一样，但这个特征性强弱可能不会很大。
(3)friends: 朋友。如果用户的朋友对某些事件感兴趣，根据KNN原则，该用户可能会对这个事感兴趣
(4)event 文本中有个词频统计，这也是一个很强的特征，我们可以计算它的TF-IDF值，因为活动次数越多，就说明越大概率会参加这个。
(5)lat和lng:是活动场地的经纬度，也是个强特征，因为好区域的活动，会得到更多人的喜好。
(6)lat和lng+county+zip 这种区域连接，会拼接成一个影响大的特征，因为同区域，或者好城市离的近的活动，都是参加概率比较大的特征。
(7) 性别+friends，可能会是个强特征，因为他会说明男性和女性的朋友更喜欢参加哪些活动。
(8)词频+性别;可能是个强特征，因为它会说明男性或女性更喜欢参加哪些活动。
(9)活动时间+其他因素，可以构造出一些强特征，因为很多活动跟时间是挂钩的，比如感恩节相关活动，春节推荐相关活动。
总结：这种多特征的问题，其实用FM/DCN更适合处理，但这次我们是学习矩阵相似度来做的项目，所以不用这样处理。另外，这个项目数据量不算太大，我认为
数据量不大的情况下，用机器学习更适合。再者，这个项目我们需要知道哪些特征属于哪些事件，哪些特征，属于哪些人，哪些特征组合可以对事件，人进行分类。
这些都是深度学习解决不了的问题。


'''
总结数据：
(1)用户历史数据 => 对事件是否感兴趣/是否参加
(2)用户社交数据 => 朋友圈
(3)event相关的数据 => event相关的信息
'''

'''
思路:
(1)要把更多维度的信息纳入考量(不光是历史数据(CF)，还有用户的社交数据,事件相关数据(可以理解为item相关数据))
(2)协同过滤是基于 用户-事件  历史交互数据 =》只针对历史数据
(3)需要把社交数据 和 事件相关信息 作为影响最后结果的因素纳入考量
(4)做分类模型，每一个人 感兴趣/不感兴趣 是target,其他影响的结果是feature。
(5)影响结果的feature包括由协同过滤出的推荐度。简单意思就是我们协同过滤可以得到
用户是否喜欢，这个结果值我们当做一个feature,用于我们的模型中。
'''

'''
整体总结:还有一些特征拼接/构造，需要数据探索，或者做的过程中通过实验得到，数据探索阶段我就不在这个代码演示了，CTR.预测等项目中
我会有很多方法演示，还有kaggle项目中。另外无论哪种推荐，我们都要关注他的基准线，也就是指标，有的是AUC,有的是召回率。有的还是其他。
Spark中有LFM，隐因子模型，对稀疏问题更好，挖掘出有用信息，原理参照FM，嵌入式计算，
'''

'''
我们这次项目一些小知识介绍：
（1）pickle是一个二进制格式dump到本地，适合咱们这种低内存电脑存储特征。
（2）LR算法是对特征敏感度特别高的算法，所以我们会在之前做标准化/归一化(normalize)（废话）
（3）鉴于这次项目是个大数据量，需要多次特征工程，所以我们是用pickle机制，每处理一次特征，就存在本地上，最后根据模型来回调整。
（4）推荐度是通过计算相似度矩阵的相似度加权平均得到。
'''

'''
相似度计算：
(1)构建用户-用户的相似度矩阵的相似度计算时，除了历史行为计算外，还可以根据用户信息来计算相似度。
(2)构建事件-事件的相似度矩阵的相似度计算时，除了历史行为计算外，还可以根据文本信息相似度，也就是事件出现的频次(文本信息出来的关键词频次)
[6,6,5,4,0] 和 [5,5,4,3,0] 就是相似信息。
'''

'''
使用的小知识讲解：
(1)rfind() 返回字符串最后一次出现的位置(从右向左查询)，如果没有匹配项则返回-1。
(2)rindex() 返回子字符串 str 在字符串中最后出现的位置，如果没有匹配的字符串会报异常，你可以指定可选参数[beg:end]设置查找的区间。
(3)defaultdict: 之前在使用字典的时候, 用的比较随意, 只是简单的使用dict.
然而这样在使用不存在的key的时候发生KeyError这样的一个报错, 这时候就该 defaultdict 登场了.
(4)locale https://blog.csdn.net/imnisen1992/article/details/53333212
这个单词中文翻译成地区或者地域，其实这个单词包含的意义要宽泛很多。locale 是根据计算机用户所使用的语言，
所在国家或者地区，以及当地的文化传统所定义的一个软件运行时的语言环境。通常情况下它可以按照涉及使用习惯分为12大类:
(5)scipy.io.mmwrite:（target，a，comment =''，field = None，precision = None，symmetry = None ）[source]
将稀疏或密集数组a写入Matrix Market文件类目标。

(6)itertools.combinations(iterable, r) https://www.cnblogs.com/yu-zhang/p/3556232.html
创建一个迭代器，返回iterable中所有长度为r的子序列，返回的子序列中的项按输入iterable中的顺序排序:
举例:
>>> list(combinations(range(3),2))
[(0, 1), (0, 2), (1, 2)]
>>> list(combinations(range(3),3))
[(0, 1, 2)]
>>> list(combinations(range(3),1))
[(0,), (1,), (2,)]
'''

'''
https://blog.csdn.net/pipisorry/article/details/48814183
总结下相似度应用场景
1.余弦相似度。  --》内容  scipy.spatial.distance.cosine
(文本为主，也就是基于内容的相似度急速那。其他领域也行，如果是关心数值或距离的业务，个人不太推荐。)
主要用在文本相似度，先用tf-idf计算后，再用余弦相似度计算。
它本质是通过cos表示方向上差异，对距离不敏感，如果你的业务需要关心距离，最好先每个值减去一个均值。
用户对内容评分来区分兴趣的相似度和差异，同时修正了用户间可能存在的度量标准不统一的问题（因为余弦距离对绝对数值不敏感）

2.欧式距离(相关性计算)。 --》用户   scipy.spatial.distance.correlation
(用户行为指标分析用户的相似度或差异。常用在数值之间差异)
与余弦相反，强调距离，体现个体数值绝对差异，如果业务是需要从维度数值大小体现差异，就用欧式距离。

3.皮尔逊相关性 （常用做调整余弦相似度的算法，通常用作物品之间的相似度） --》物品
估算样本的协方差和标准差，可得到样本相关系数(样本皮尔逊系数)

4.杰卡德:  --》 内容 scipy.spatial.distance.jaccard
主要用在文本相似度可以用出现相同词个数进行计算。
意思是两个集合的交集除以并集。

5.平均平方差异（MSD） --》 用户偏好相似度。
operatorname {MSE}(T)={\sqrt {(x_{1}-y_{1})^{2}+(x_{2}-y_{2})^{2}+\cdots +(x_{n}-y_{n})^{2}}}={\sqrt {\sum _{{i=1}}^{n}(x_{i}-y_{i})^{2}}}$$
计算两个用户在打分中均方差，表现比PC差，因为它没有考虑到用户偏好或产品的被欣赏程度之间的负相关。
有论文比较过PC的MAE(平均绝对误差(MAE)),MAE就是误差求和除以总数。 要优于 MSD 和SRC
