前言: 首先，我对强化学习的理解，是一个基于图论算法的系统。我们的基准点在于俩个：输入数据每次都不一样。 每次都要找到一个最佳路径代替原有路径。
基于这俩个也造就了强化学习动态的能力，而我们推荐算法的缺点就是很难长期动态的提供客户需要的数据，无论是基于文本内容的推荐，还是基于用户行为的推荐。
都有这个缺点，我在这个模块中，主要在基于用户行为上使用强化学习做文章，而基于文本内容的是用了多种MLP融合+管道+并发机制。后续如果有时间我会继续更新。

以下大部分为个人理解，如有错误，请见谅。

一.组成:
Q值:挡墙状态state完成action得到的分数。可以理解为一组数据建模后得到的最后损失值或者AUC值。
agent:每次学习的行动，是通过当前环境状态而决定。这个行动就是改变我们的权重系数与超参数。
environment: 当前环境状况，在我这个项目里，你可以理解为当前的数据。
strategy：策略。是一组agent产生的结果，这个结果的好坏由奖励结果决定，在我的项目里，你可以理解为模型。
reward：奖励。通过一组agent得到效果好坏值，你可以理解为损失值。

简单说，就是一个具有动态的有监督学习方式，但他还不是有监督学习，因为有监督学习需要一一映射关系，也就是一组数据，一个
模型，一个损失，一个target.

个人理解：我们通过先前的数据，不同的尝试不同的策略得到一系列奖励。然后遇到一个新的场景时，我们利用
我们之前学习出的各种策略去做决策，如果这个新的场景我们没见过，我们会重新学习，并存储到我们的策略库中。所以这个用在机器人+游戏领域非常好。

二.电商中强化学习能解决什么问题？
电商这种项目，它的前提就是业务支撑，也就是说，电商的相关技术，是在业务的前提下诞生出来的。
所以我们首先要想的是这个技术是否可行，是否可以给人民服务，
是否带来我们的经济效益，是否能解决我们存在的问题。

1.是否可行?
可以，因为现今电商主要是基于大数据平台+java ee开发，而大数据平台现在逐渐融合深度学习框架，也支持了大量的机器学习框架，强化学习可以用tensorflow
gym等组件完成，所以移植到大数据平台完全没问题。

2.是否为人民服务
可以，因为当前时代，是个服务行业的时代。人们每天的需求在不断变化，我们推荐系统的本质，是帮助用户过滤掉他们不喜欢的，直接给用户提供喜欢的数据，节省了
用户的时间。而只要是人，他就是一个不断地思考，不断变化的事务。所以在搭建用户画像的基础上，我们需要实现一种动态处理不同数据的模型去提供用户可能将要喜欢
的数据。这样也减少了开发者的时间成本。

3.是否带来经济效益
我们每次都是针对不同数据集，创造不同feature,创造不同模型，最后融合，评估，上线，流程很长，浪费了很多人力成本，而强化学习可以减少这些，还可以节约开发
者的开发时间。同而带来经济效益

4.是否解决我们存在的问题。
在各大推荐领域中，都存在时效性问题，而强化学习可以通过不同输入数据，来决策出最好的模型，正好解决了这一问题。

三.主要分类
1.粗略分类:
1).Policy based: 关注点是找到最优策略。
2).Value based:关注点是找到最优奖励综合。
3).Action based:关注点是每一步的最优行动。

2.细致分类:
1)Model-free:不尝试去理解环境, 环境给什么就是什么，一步一步等待真实世界的反馈, 再根据反馈采取下一步行动。
2）Model-based：先理解真实世界是怎样的, 并建立一个模型来模拟现实世界的反馈，通过想象来预判断接下来将要发生的所有情况，
然后选择这些想象情况中最好的那种，并依据这种情况来采取下一步的策略。它比 Model-free 多出了一个虚拟环境，还有想象力。
3)Policy based：通过感官分析所处的环境, 直接输出下一步要采取的各种动作的概率, 然后根据概率采取行动。
4)Value based：输出的是所有动作的价值, 根据最高价值来选动作，这类方法不能选取连续的动作。
5)Monte-carlo update：游戏开始后, 要等待游戏结束, 然后再总结这一回合中的所有转折点, 再更新行为准则。
6)Temporal-difference update：在游戏进行中每一步都在更新, 不用等待游戏的结束, 这样就能边玩边学习了。
7)On-policy：必须本人在场, 并且一定是本人边玩边学习
8)Off-policy：可以选择自己玩, 也可以选择看着别人玩, 通过看别人玩来学习别人的行为准则。

四.主要算法
1.Sarsa
SARSA 利用马尔科夫性质，只利用了下一步信息, 让系统按照策略指引进行探索，在探索每一步都进行状态价值的更新，更新公式如下所示：
q(s,a) = q(s,a) + a(r + γq(s’,a’) - q(s,a))
q是结果值，s 为当前状态，a 是当前采取的动作，s’ 为下一步状态，a’ 是下一个状态采取的动作，r 是系统获得的奖励， α 是学习率， γ 是衰减因子。

2.Q learning
Q Learning 的算法框架和 SARSA 类似, 也是让系统按照策略指引进行探索，在探索每一步都进行状态价值的更新。
关键在于 Q Learning 和 SARSA 的更新公式不一样，Q Learning 的更新公式如下：
q(s,a) = q(s,a) + a{r + MAXa'{γq(s’,a’)} - q(s,a)}
简单说就是每次取最大的行动结果，有个排序选择最大值的效果，时间上降低，但准确性会有提升。

3.Policy Gradients
系统会从一个固定或者随机起始状态出发，策略梯度让系统探索环境，生成一个从起始状态到终止状态的状态-动作-奖励序列，s1,a1,r1,.....,sT,aT,rT，
在第 t 时刻，我们让 gt=rt+γrt+1+... 等于 q(st,a) ，从而求解策略梯度优化问题。
简单说就是加了时间这个因素。让结果分时间段的去学习出最优问题，大袋梯度最优。

4.Actor-Critic
算法分为两个部分：Actor 和 Critic。Actor 更新策略， Critic 更新价值。Critic 就可以用之前介绍的 SARSA 或者 Q Learning 算法。

5.Monte-carlo learning
用当前策略探索产生一个完整的状态-动作-奖励序列: 
s1,a1,r1,....,sk,ak,rk∼π
在序列第一次碰到或者每次碰到一个状态 s 时，计算其衰减奖励:
最后更新状态价值:

6.Deep-Q-Network
DQN 算法的主要做法是 Experience Replay，将系统探索环境得到的数据储存起来，然后随机采样样本更新深度神经网络的参数。
它也是在每个 action 和 environment state 下达到最大回报，不同的是加了一些改进，加入了经验回放和决斗网络架构。

我们可以理解为XGB中的随机采样+神经网络的前向反向传播的结合+自身更新max action结果值 最终得到一个有经验回放的最大奖励汇报。
日后的高级强化学习都是基于这个。









