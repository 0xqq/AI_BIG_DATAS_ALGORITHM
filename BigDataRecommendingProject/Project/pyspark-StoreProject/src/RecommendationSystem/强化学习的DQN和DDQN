DQN:  https://blog.csdn.net/u013236946/article/details/72871858
DDQN: https://blog.csdn.net/u013236946/article/details/73161586
DDPG、NAF : https://blog.csdn.net/u013236946/article/details/73243310

一.DQN：
1.DRL
Q-learning中，如果当前状态和空间是高维持续时，因为维度太多，很难实现。
而深度学习CNN就是提取复杂特征的模型，DQN将CNN与强化学习RL结合。直接从高维原始数据学习控制策略。将这种深度学习结合强化学习叫DRL。
过程：CNN输入是原始数据，作为状态state,输出是每个动作Action对应的价值评估Value Function(Q值。)

2.DL（深度学习）+RL（强化学习）结合存在的问题。
a. DL需要大量标签样本进行监督学习，RL只有reward奖励值返回值，而且伴随噪声。稀疏问题(很多状态state执行action后奖励为0)等问题。
b. DL是独立样本，而RL是前后state状态相关
c. DL目标分布固定，RL分布一直变化，因为RL的每次数据都是不同数据作为输入的，这样导致训练好一个数据集后，下一次又要重新训练。
d. 实验证明-非线性网络表示函数时都有不稳定问题。

3.DQN解决方案
a. 使用reward奖励值来构造标签，解决问题a
b. 通过经验池(experience replay) 解决相关性和非静态分布问题(对应问题b,c)
c. 使用CNN(MainNet) 产生当前Q值，使用另外一个CNN(Target)产生Target Q值，解决问题4.

4.详细过程
(1).构造标签
DQN中CNN作用： 高维度且连续状态下的Q-Table 做函数拟合（如果是三项式，就2个点拟合，四项式，就3个点拟合，相当于求方程）
函数优化问题：先确定损失函数，然后求梯度，使用随机梯度下降等方法更新参数。

Q-learning更新公式如下：
Q∗(s,a)=Q(s,a)+α(r+γmaxa′Q(s′,a′)−Q(s,a))
损失函数:
L(θ)=E[(TargetQ−Q(s,a;θ))2]
其中 θ 是网络参数，目标为
TargetQ=r+γmaxa′Q(s′,a′;θ)
显然Loss Function是基于Q-Learning更新公式的第二项确定的，两个公式意义相同，都是使当前的Q值逼近Target Q值。
接下来，求 L(θ) 关于 θ 的梯度，使用SGD等方法更新网络参数 θ。

(2)经验池（experience replay）
经验池的功能主要是解决相关性及非静态分布问题。具体做法是把每个时间步agent与环境交互得到的转移样本 (st,at,rt,st+1) 储存到回放记忆单元，
要训练时就随机拿出一些（minibatch）来训练。（其实就是将游戏的过程打成碎片存储，训练时随机抽取就避免了相关性问题）

(3)目标网络
在Nature 2015版本的DQN中提出了这个改进，使用另一个网络（这里称为TargetNet）产生Target Q值。具体地，
Q(s,a;θi) 表示当前网络MainNet的输出，用来评估当前状态动作对的值函数；Q(s,a;θ−i) 表示TargetNet的输出，
代入上面求 TargetQ 值的公式中得到目标Q值。根据上面的Loss Function更新MainNet的参数，每经过N轮迭代，将MainNet的参数复制给TargetNet。
引入TargetNet后，再一段时间里目标Q值使保持不变的，一定程度降低了当前Q值和目标Q值的相关性，提高了算法稳定性。

(4)网络流程
即输入层接三个卷积层后，接两个全连接层，输出为每个动作的Q值

(5)总结
DQN是第一个将深度学习模型与强化学习结合在一起从而成功地直接从高维的输入学习控制策略。

创新点：

基于Q-Learning构造Loss Function（不算很新，过往使用线性和非线性函数拟合Q-Table时就是这样做）。
通过experience replay（经验池）解决相关性及非静态分布问题；
使用TargetNet解决稳定性问题。
优点：

算法通用性，可玩不同游戏；
End-to-End 训练方式；
可生产大量样本供监督学习。
缺点：

无法应用于连续动作控制；
只能处理只需短时记忆问题，无法处理需长时记忆问题（后续研究提出了使用LSTM等改进方法）；
CNN不一定收敛，需精良调参。

二.DDQN：
论文（Hasselt等人）发现并证明了传统的DQN普遍会过高估计Action的Q值，而且估计误差会随Action的个数增加而增加。
如果高估不是均匀的，则会导致某个次优的Action高估的Q值超过了最优Action的Q值，永远无法找到最优的策略。
作者在他2010年提出的Double Q-Learning的基础上，将该方法引入了DQN中。具体操作是对要学习的Target Q值生成方式进行修改，
原版的DQN中是使用TargetNet产生Target Q值，即
TargetQ=r+γmaxa′Q(s′,a′;θ−i)
其中θ−i是TargetNet的参数。
在DDQN中，先用MainNet找到 maxa′Q(s′,a′;θi) 的Action（θi是MainNet的参数），再去TargetNet中找到这个Action的Q值以构成Target Q值，
这个Q值在TargetNet中不一定是最大的，因此可以避免选到被高估的次优Action。最终要学习的Loss Function为： 
L(θ)=E[(TargetQ−Q(s,a;θi))2]

TargetQ=r+γQ(s′,maxa′Q(s′,a′;θi);θ−i)

除此之外，其他设置与DQN一致。实验表明，DDQN能够估计出更准确出Q值，在一些Atari2600游戏中可获得更稳定有效的策略。
Loss Function 的构造流程图 
这里写图片描述

二、Dueling-DQN
在许多基于视觉的感知的DRL任务中，不同的状态动作对的值函数是不同的，但是在某些状态下，值函数的大小与动作无关。
根据以上思想，Wang等人提出了一种竞争网络结构（dueling network）作为DQN的网络模型。

先来看结构

这里写图片描述

如上图所示，第一个模型是一般的DQN网络模型，即输入层接三个卷积层后，接两个全连接层，输出为每个动作的Q值。

而（第二个模型）竞争网络（dueling net）将卷积层提取的抽象特征分流到两个支路中。其中上路代表状态值函数 V(s)，
表示静态的状态环境本身具有的价值；下路代表依赖状态的动作优势函数 A(a) （advantage function），表示选择某个Action额外带来的价值。
最后这两路再聚合再一起得到每个动作的Q值。 
这里写图片描述

这种竞争结构能学到在没有动作的影响下环境状态的价值 V(s)。如下图，在训练过程中，上下两行图表示不同时刻，左右两列表示属于 V(s) 和 A(a)，
（通过某种方法处理后）图中红色区域代表 V(s) 和 A(a) 所关注的地方。V(s) 关注于地平线上是否有车辆出现（此时动作的选择影响不大）
以及分数；A(a) 则更关心会立即造成碰撞的车辆，此时动作的选择很重要。 
这里写图片描述

公式： 
状态价值函数表示为 
V(s;θ,β)

动作优势函数表示为 
A(s,a;θ,α)

动作Q值为两者相加 
Q(s,a;θ,α,β)=V(s;θ,β)+A(s,a;θ,α)

其中 θ 是卷积层参数，β 和 α 是两支路全连接层参数。 
而在实际中，一般要将动作优势流设置为单独动作优势函数减去某状态下所有动作优势函数的平均值 
Q(s,a;θ,α,β)=V(s;θ,β)+(A(s,a;θ,α)−1|A|∑a′A(s,a′;θ,α))

这样做可以保证该状态下各动作的优势函数相对排序不变，而且可以缩小 Q 值的范围，去除多余的自由度，提高算法稳定性。
论文中dueling net结合了DDQN以及优先级采样（Prioritized Experience Replay）的训练方式。

Prioritized Experience Replay 
简单来说，经验池中TD误差（r+γmaxa′Q(s′,a′;θ−)−Q(s,a;θ)）绝对值越大的样本被抽取出来训练的概率越大，加快了最优策略的学习。

实验证明，Dueling-DDQN估计的值函数更加精确。在频繁出现 agent 采取不同动作但对应值函数相等的情形下，Dueling-DDQN优势最明显。


三.DDPG、NAF:
一、存在的问题
DQN是一个面向离散控制的算法，即输出的动作是离散的。对应到Atari 游戏中，只需要几个离散的键盘或手柄按键进行控制。

然而在实际中，控制问题则是连续的，高维的，比如一个具有6个关节的机械臂，每个关节的角度输出是连续值，假设范围是0°~360°，归一化后为（-1，1）。若把每个关节角取值范围离散化，比如精度到0.01，则一个关节有200个取值，那么6个关节共有2006个取值，若进一步提升这个精度，取值的数量将成倍增加，而且动作的数量将随着自由度的增加呈指数型增长。所以根本无法用传统的DQN方法解决。

解决方法 
使用Policy-Based方法，通过各种策略梯度方法直接优化用深度神经网络参数化表示的策略，即网络的输出就是动作。

二、DDPG
深度确定性策略梯度（Deep Deterministic Policy Gradient， DDPG）算法是Lillicrap 等人利用 DQN 扩展 Q 
学习算法的思路对确定性策略梯度（Deterministic Policy Gradient， DPG）方法进行改造，提出的一种基于行动者-评论家（Actor-Critic，AC）
框架的算法，该算法可用于解决连续动作空间上的 DRL 问题。

这里写图片描述

随机性策略和确定性策略：

随机性策略，策略输出的是动作的概率，比如上一篇A3C博客提到的连续动作控制，使用的是一个正态分布对动作进行采样选择，即每个动作都有概率被选到；
优点，将探索和改进集成到一个策略中；缺点，需要大量训练数据。
确定性策略，策略输出即是动作；优点，需要采样的数据少，算法效率高；缺点，无法探索环境。
在真实场景下机器人的操控任务中，在线收集并利用大量训练数据会产生十分昂贵的代价， 并且动作连续的特性使得在线抽取批量轨迹的方式无法达到令人满意
的覆盖面， 这些问题会导致局部最优解的出现。

然而使用确定性策略无法探索环境，如何解决？ 
利用off-policy学习方法。off-policy是指采样的策略和改进的策略不是同一个策略。类似于DQN，使用随机策略产生样本存放到经验回放机制中，训练时随
机抽取样本，改进的是当前的确定性策略。整个确定性策略的学习框架采用AC的方法。

DDPG公式 
在DDPG中，分别使用参数为 θμ 和 θQ 的深度神经网络来表示确定性策略 a=π(s|θμ) 和动作值函数 Q(s,a|θQ)。其中，策略网络用来更新策略，对应 AC 
框架中的行动者；值网络用来逼近状态动作对的值函数， 并提供梯度信息， 对应 AC 框架中的评论家。目标函数被定义为带折扣的总回报： 
J(θμ)=Eθμ[r1+γr2+γ2r3+⋯]

通过随机梯度法对目标函数进行端对端的优化（注意，目标是提高总回报 J）。Silver等人证明了目标函数关于 θμ 的梯度等价于Q值函数关于 θμ 的期望梯度： 
∂J(θμ)∂θμ=Es[∂Q(s,a|θQ)∂θμ]
根据确定性策略 a=π(s|θμ) 可得： 
∂J(θμ)∂θμ=Es[∂Q(s,a|θQ)∂a∂π(s|θμ)∂θμ]
沿着提升 Q 值的方向更新策略网络的参数。
通过 DQN中更新值网络的方法来更新评论家网络，梯度信息为： 
∂L(θQ)∂θQ=Es,a,r,s′∼D[(TargetQ−Q(s,a|θQ))∂Q(s,a|θQ)∂θQ]
TargetQ=r+γQ′(s′,π(s′|θμ′)|θQ′)
其中 θμ′ 和 θQ′ 分别表示目标策略网络和目标值网络的参数，用梯度下降方式更新值网络。

算法伪代码 
这里写图片描述

区别于DQN，DQN每隔一定的迭代次数后，将MainNet参数复制给TargetNet；而DDPG中TargetNet的参数每次迭代都以微小量逼近MainNet的参数。

网络训练流程图 
这里写图片描述

实验表明， DDPG 不仅在一系列连续动作空间的任务中表现稳定，而且求得最优解所需要的时间步也远远少于 DQN。与基于值函数的 DRL 方法相比，
基于 AC 框架的深度策略梯度方法优化策略效率更高、 求解速度更快。

DDPG缺点： 
不适用于随机环境的场景

三、NAF
Shixiang等人的论文中共有两个算法，第一个是NAF，第二个是基于模型（Model-based）加速的NAF。这里只介绍简单的NAF。

DDPG的问题： 
需要训练两个网络即策略网络和值网络

解决方法 
归一化优势函数（normalized advantage functions ，NAF）只需要训练一个网络。

NAF公式： 
NAF的目的之一是要将深度神经网络Q-Learning应用于连续动作空间，而要用Q-Learing进行训练必须要知道目标Q值（TargetQ）。

和前面博客Dueling-DDQN介绍的dueling net思想类似，动作值函数可以表示为状态值函数 V 与动作价值函数 A 的和，即
Q(x,u|θQ)=V(x|θV)+A(x,u|θA)

其中 x 表示状态State，u 表示动作Action，θ 是对应的网络参数，A(x,u|θA) 可以看成动作 u 在状态 x 下的优势。我们的目的就是要使策
略网络输出的动作 u 所对应的Q值最大。 
如果能使 ∀x,uA(x,u|θA)⩽0，则 ∀x,uQ(x,u|θQ)⩽V(x|θV)。在状态 x 下最优的动作 u 的动作优势函数 A(x,u|θA)=0，所以对应最优动作
的值函数Q(x,u|θQ)=V(x|θV)，这样就很容易构造出TargetQ值了。具体的做法是令
A(x,u|θA)=−12(u−μ(x|θμ))TP(x|θP)(u−μ(x|θμ))

P(x|θP)是一个关于状态的正定矩阵，因为正定矩阵可以进行楚列斯基（Cholesky）分解，即
P(x|θP)=L(x|θP)L(x|θP)T
L(x|θP)是对角线都是正数的下三角矩阵，且是唯一的。
最终算法的Loss Function为 
L(θQ)=E[(TargetQ−Q(xt,ut|θQ))2]
TargetQ=rt+γV′(xt+1|θQ′)
Q(xt,ut|θQ)=V(xt|θV)+A(xt,ut|θA)
使用DQN的训练方式训练。

算法伪代码 
这里写图片描述

网络训练流程图 
这里写图片描述

网络输出的是下三角矩阵L，动作 u，状态值函数 V
异步NAF训练机械臂 
Shixiang等人还使用了异步NAF训练机械臂开门。 
这里写图片描述

该算法具有一个训练线程（trainer thread）和多个收集样本线程（collector thread），collector thread将收集到的样本存于经验
回放机制中，供trainer thread训练。

这个异步NAF算法和A3C算法的不同之处在于： 
异步NAF是off-policy，collector thread不提供梯度信息； 
A3C是on-policy，每个线程agent都提供梯度信息。
