CF:https://www.sohu.com/a/155583569_505802

一.步骤
总共分三步:

1.提取文本/物品的强特征：将每一个物品，或者一个文本内容，提取出强特征，并用这些特征打上标签。比如： NBA文本: 篮球，运动员，身高1.8以上，健身，比赛。 这些重要文本就可以归于
篮球相关文本。 如果是物品:XL,黑色，成熟，休闲，夏天。 这些特征可以推断出属于中等尺寸男性的夏装。

2.提取用户的强特征： 将每一个用户打上标签，比如这个用户的特征是: 每个月网上消费1000，喜欢浏览游戏网站，偶尔喜欢浏览学校网站，吃饭时间很少用支付宝/微信去买饭。这些特点
能给这个用户打成爱玩游戏的学生的标签。

3相似度计算： .最后用相似度计算，最好是相似计算，计算距离那种，计算物品的特征与用户的特征，然后在用户没浏览过的物品/文本去推荐一个相似度最高的物品/文本。


二.解法
1 提取文本/物品的强特征：主要分俩种形式描述他的属性:结构化/非结构化。
(1)结构化：意义明确，有范围:交友网站->item就是人->身高,学历，籍贯。->直接可用。
(2)非结构化:意义不明，取值无范围:写的博客内容等。我们需要转化为结构化数据。
方法：信息检索。
信息检索来源：向量空间模型-TF-IDF
具体内容： 文章集合： D = {d1,d2...dn}  即所有文章的总集合。
          词典集合： T = {t1,t2...tn}  文章出现的词的集合。
最终目标： 使用一个向量表示一篇文章。dj = (w1j,w2j,...wnj) w1j表示第一个词t1的在文章j的权重，越大越重要。所以我们就是为了求权重。
具体求法： TF-IDF 词频-逆文档频率。第j篇文章与词典第k个词的tf-idf为：TF-IDF(tk,dj) = TF(tk,dj) . log(N/nk)
公式解析： TF(tk,dj) 是第k个词在文章j中出现的次数，而nk是所有文章中包括第k个词的文章数量。
最终处理： wk,j = TF-IDF(tk,dj) / ∑|T| TF-IDF (ts,dj) ^2  
最后总结： 通过公式我们得知:如果这个词在这个文章中TF-IDF权重大，那么它在这个文章中出现次数多，才越重要。但如果所有文章出现这个词都多，那么也说明
          这个词不太重要，所以除以了log(N/nk) 也就是说这个词在其他文章出现少，就这个j文章出现多，满足了这俩个条件，它的权重就大，我们就认为他重要。
          
2.提取用户的强特征：
通过这些特征判断用户好坏，常用的大部分都是机器学习，下面一一列举下:

(1)KNN(本质就是基于用户历史喜欢的item,根据现在的item去求相似度)
对于一个新的item，最近邻方法首先找用户u已经评判过并与此新item最相似的k个item，然后依据用户u对这k个item的喜好程度来判断其对此新item的喜好程度。
这种做法和CF中的item-based kNN很相似，差别在于这里的item相似度是根据item的属性向量计算得到，而CF中是根据所有用户对item的评分计算得到。
对于这个方法，比较关键的可能就是如何通过item的属性向量计算item之间的两两相似度。[2]中建议对于结构化数据，相似度计算使用欧几里得距离；
而如果使用向量空间模型（VSM）来表示item的话，则相似度计算可以使用cosine。关于相似度，可以看我相似度文章里。
个人感觉是一个很弱智的算法，但很多地方也很适用，总之我认为只能当个辅助算法吧。

(2)Rocchio 算法
通过信息检索处理相关的反馈。
比如：你搜索 ： "苹果"。
搜索引擎不知道你要搜索的是要吃的苹果，还是不能吃的苹果。 所以会给你很多结果。 然后你点击你觉得相关的结果(这就是相关反馈)。
搜索引擎会根据你第二次点击的相关反馈，修改查询向量取值，重新计算网页得分，把你第二次点击的结果相似结果排在前面。
比如你最开始搜索“苹果”时，对应的查询向量是{“苹果” : 1}。而当你点击了一些与Mac、iPhone相关的结果后，
搜索引擎会把你的查询向量修改为{“苹果” : 1, “Mac” : 0.8, “iPhone” : 0.7}，通过这个新的查询向量，搜索引擎就能比
较明确地知道你要找的是不能吃的苹果了。Rocchio算法的作用就是用来修改你的查询向量的：
{“苹果” : 1}  --> {“苹果” : 1, “Mac” : 0.8, “iPhone” : 0.7}。
通过如下公式获取用户 u 的Wu向量

具体公式： Wu = β * 1/|Ir| * ∑Wj - γ * 1/|Inr| ∑ Wk  求和里都是W属于相应的I

公式解析：Wj表示item j的属性。 Ir和Inr表示已知的用户u喜欢和不喜欢的item集合。β是正反馈的权重。γ是负反馈的权重。反馈的值由系统给定。
         所以大概意思就是正反馈的向量集合*正反馈权重 - 负反馈的向量集合*负反馈权重。
         
最后，获得Wu后，对于给定的item j 我们使用Wu和Wj的相似度代表用户u对j的喜好程度。 Rocchio算法的一个好处是Wu可以根据用户的反馈实时更新，其更新
代价很小。这个算法类似强化学习，在线学习。

3.决策树升级版-随机森林——》XGB/LGB
 当item的属性较少而且是结构化属性时，决策树一般会是个好的选择。这种情况下决策树可以产生简单直观、容易让人理解的结果。
 而且我们可以把决策树的决策过程展示给用户u，
 告诉他为什么这些item会被推荐。但是如果item的属性较多，且都来源于非结构化数据（如item是文章），那么决策树的效果可能并不会很好。
 所以就用了随机森林，将样本分成很多树，根据权重去解析，每次都用交叉熵来判断权重，最后选出好的特征并给出相应的权重，也即是特征的重要性。
 但缺点是速度太慢,后来用了更高级的XGB,LGB取代，这个后续我再说明。
 
 4.线性分类，简称LC,类似SVM。
 对于我们这里的二类问题，线性分类器（LC）尝试在高维空间找一个平面，使得这个平面尽量分开两类点。也就是说，一类点尽可能在平面的某一边，
 而另一类点尽可能在平面的另一边。
 仍以学习用户u的分类模型为例。Wj表示item j的属性向量，那么LC尝试在Wj空间中找平面Cu.Wj，
 使得此平面尽量分开用户u喜欢与不喜欢的item。其中的Cu就是我们要学习的参数了。最常用的学习Cu的方法就是梯度下降法了，其更新过程如下：
 Cu(t+1) = Cu(t) - 学习率*(Cu(t) * wJ - Yui) * Wj
 其中的上角标t表示第t次迭代，Yui表示用户u对item j的打分（例如喜欢则值为1，不喜欢则值为-1）。
 它控制每步迭代变化多大，由系统给定。
和Rocchio算法一样，上面更新公式的好处就是它可以以很小的代价进行实时更新，实时调整用户u对应的Cu。
说到这里，很多童鞋可能会想起一些著名的线性分类器：Logistic Regression和Linear SVM等等，它们当然能胜任我们这里的分类任务。
[2]中提到Linear SVM用在文本分类上能获得相当不错的效果:)。
如果item属性Wj的每个分量都是0/1取值的话（如item为文章，Wj的第k个分量为1表示词典中第k个词在item j中，
为0表示第k个词不在item j中），那么还有一种很有意思的启发式更新Image19的算法：Winnow算法。
[4]中就是使用Winnow算法来获得user profile的。

5. 朴素贝叶斯算法（Naive Bayes，简称NB）
NB算法就像它的简称一样，牛逼！NB经常被用来做文本分类，它假设在给定一篇文章的类别后，其中各个词出现的概率相互独立。
它的假设虽然很不靠谱，但是它的结果往往惊人地好。再加上NB的代码实现比较简单，所以它往往是很多分类问题里最先被尝试的算法。
我们现在的profile learning问题中包括两个类别：用户u喜欢的item，以及他不喜欢的item。在给定一个item的类别后，
其各个属性的取值概率互相独立。我们可以利用用户u的历史喜好数据训练NB，之后再用训练好的NB对给定的item做分类。
NB的介绍很多，这里就不再啰嗦了，有不清楚的童鞋可以参考NB Wiki

三. Recommendation Generation

      如果上一步Profile Learning中使用的是分类模型（如DT、LC和NB），那么我们只要把模型预测的用户最可能感兴趣的n个item作为推荐返回给用户即可。而如果Profile Learning中使用的直接学习用户属性的方法（如Rocchio算法），那么我们只要把与用户属性最相关的n个item作为推荐返回给用户即可。其中的用户属性与item属性的相关性可以使用如cosine等相似度度量获得。

 四.兴趣迁移——衰减机制
  我们大家会不会想到，我们的兴趣点可能是会随时间改变的呢？比如这段时间苹果出了一款新产品，我关注一下，但一个月后，我可能就完全不在意这件事了，但是可能苹果相关的关键词还一直在我的关键词表里，那会不会导致我依然收到相似的我已经不关心的新闻的推荐呢？也就是如何处理这种兴趣迁移问题呢？
为了解决这个问题，我们可以引入一个衰减机制，即让用户的关键词表中的每个关键词喜好程度都按一定周期保持衰减。考虑到不同词的TFIDF值可能差异已经在不同的数量级，我们考虑用指数衰减的形式来相对进行公平的衰减。即引入一个系数，，我们每隔一段时间，对所有用户的所有关键词喜好程度进行*的衰减，那么就完成了模拟用户兴趣迁移的过程。
当然，一直衰减下去，也会使得一些本来就已经完全不感兴趣的关键词可能衰减到了0.0000001了，还在衰减，还死皮赖脸地待在词表里占位置，那么自然而然，我们可以设置一个阈值L，规定对每个用户的每次衰减更新完成后，将词表里喜好值小于L的关键词直接清除

CB的优点：

1. 用户之间的独立性（User Independence）：既然每个用户的profile都是依据他本身对item的喜好获得的，自然就与他人的行为无关。
而CF刚好相反，CF需要利用很多其他人的数据。CB的这种用户独立性带来的一个显著好处是别人不管对item如何作弊（比如利用多个账号把某个产品的排名刷上去）
都不会影响到自己。

2. 好的可解释性（Transparency）：如果需要向用户解释为什么推荐了这些产品给他，你只要告诉他这些产品有某某属性，这些属性跟你的品味很匹配等等。

3. 新的item可以立刻得到推荐（New Item Problem）：只要一个新item加进item库，它就马上可以被推荐，被推荐的机会和老的item是一致的。
而CF对于新item就很无奈，只有当此新item被某些用户喜欢过（或打过分），它才可能被推荐给其他用户。所以，如果一个纯CF的推荐系统，
新加进来的item就永远不会被推荐:( 。

CB的缺点：

1. item的特征抽取一般很难（Limited Content Analysis）：如果系统中的item是文档（如个性化阅读中），
那么我们现在可以比较容易地使用信息检索里的方法来“比较精确地”抽取出item的特征。但很多情况下我们很难从item中抽取出准确刻画item的特征，
比如电影推荐中item是电影，社会化网络推荐中item是人，这些item属性都不好抽。其实，几乎在所有实际情况中我们抽取的item特征都仅能代表item的一些方面，
不可能代表item的所有方面。这样带来的一个问题就是可能从两个item抽取出来的特征完全相同，这种情况下CB就完全无法区分这两个item了。
比如如果只能从电影里抽取出演员、导演，那么两部有相同演员和导演的电影对于CB来说就完全不可区分了。所以后面会有特征拼接做法，深度学习，强化学习的使用。

2. 无法挖掘出用户的潜在兴趣（Over-specialization）：既然CB的推荐只依赖于用户过去对某些item的喜好，它产生的推荐也都会和用户过去喜欢的item相似。
如果一个人以前只看与推荐有关的文章，那CB只会给他推荐更多与推荐相关的文章，它不会知道用户可能还喜欢数码。

3. 无法为新用户产生推荐（New User Problem）：新用户没有喜好历史，自然无法获得他的profile，所以也就无法为他产生推荐了。当然，这个问题CF也有。
CB应该算是第一代的个性化应用中最流行的推荐算法了。但由于它本身具有某些很难解决的缺点（如上面介绍的第1点），再加上在大多数情况下其精度都不是最好的，
目前大部分的推荐系统都是以其他算法为主（如CF），而辅以CB以解决主算法在某些情况下的不精确性（如解决新item问题）。但CB的作用是不可否认的，
只要具体应用中有可用的属性，那么基本都能在系统里看到CB的影子。组合CB和其他推荐算法的方法很多（我很久以后会写一篇博文详细介绍之），
最常用的可能是用CB来过滤其他算法的候选集，把一些不太合适的候选（比如不要给小孩推荐偏成人的书籍）去掉。




