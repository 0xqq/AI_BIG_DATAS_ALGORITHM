http://lusongsong.com/info/post/9829.html
http://m.elecfans.com/article/694164.html

一.系统概览
推荐系统，如果用形式化的方式去描述实际上是拟合一个用户对内容满意度的函数，这个函数需要输入三个维度的变量。

第一个维度： 内容特征。 比如平台特征，图像特征，文本特征，视频特征，问答，新闻标题。

第二个维度： 用户特征。 包括各种兴趣标签，职业、年龄、性别等，还有很多模型刻划出的隐式用户兴趣等。

第三个维度： 环境特征。 随着用户的随时随地移动，在工作场合，通勤，旅游等不同的场景，信息偏好有所偏移。

结合三方面的维度，模型会给出一个预估，即推测推荐内容在这一场景下对这一用户是否合适。

引申问题 ： 如何引入无法直接衡量的目标?
推荐模型中，点击率、阅读时间、点赞、评论、转发包括点赞都是可以量化的目标，能够用模型直接拟合做预估，看线上提升情况可以知道做的好不好。
但一个大体量的推荐系统，服务用户众多，不能完全由指标评估，引入数据指标以外的要素也很重要。

比如广告和特型内容频控。像问答卡片就是比较特殊的内容形式，其推荐的目标不完全是让用户浏览，还要考虑吸引用户回答为社区贡献内容。
这些内容和普通内容如何混排，怎样控制频控都需要考虑。

此外，平台出于内容生态和社会责任的考量，像低俗内容的打压，标题党、低质内容的打压，重要新闻的置顶、加权、强插，低级别账号内容降权都是算法本身无法完成，
需要进一步对内容进行干预。

传统经典推荐算法：
1.CF ->
2.CB -> 包含bayes,svm,knn处理。
3.LR ->
4.DNN -> FM FFM DeepFM DCN PNN DIN
5.GBDT -> RandomForest XGB LGB CATBoost

流行算法： (1)GBDT/XGB + LR 用XGB得到的特征叶子作为维度，结合LR得到最终结果
          (2) LR + DNN  DNN模型融合后用LR实时权重，得到最后结果。
          当然还会有很多。必须根据业务场景不同选择。

二.重要的特征
模型之后再看一下典型的推荐特征，主要有四类特征会对推荐起到比较重要的作用。

1.第一类是相关性特征(特征拼接)，就是评估内容的属性和与用户是否匹配。显性的匹配包括关键词匹配、分类匹配、来源匹配、主题匹配等。像FM模型中也有一些隐性匹配，
从用户向量与内容向量的距离可以得出。

2.第二类是环境特征(特征构造)，包括地理位置、时间。这些既是bias特征，也能以此构建一些匹配特征。

3.第三类是热度特征(冷启动)。包括全局热度、分类热度，主题热度，以及关键词热度等。内容热度信息在大的推荐系统特别在用户冷启动的时候非常有效。

4.第四类是协同特征(特征挖掘)，它可以在部分程度上帮助解决所谓算法越推越窄的问题。协同特征并非考虑用户已有历史。而是通过用户行为分析不同用户间相似性，
比如点击相似、兴趣分类相似、主题相似、兴趣词相似，甚至向量相似，从而扩展模型的探索能力。

三.在线训练。
1.基于大数据的流式处理框架 Storm / SparkStreaming集群实时处理样本数据(点击，展现，收藏，分享。)
2.每收集一定量的用户数据，就更新推荐模型。
3.模型参数存储在高性能服务器集群，包含几百亿原始特征，和数十亿向量特征。
具体步骤：
线上服务器记录实时特征 -> 导入Kafka文件队列 ->导入Storm集群/spark消费kafka数据 -> 拼接用户完整数据，客户端用户动作反馈，回传推荐的label构造样本
- >根据最新样本进行在线训练更新模型参数 -> 线上模型获取新知识。
这个步骤延迟是用户的动作反馈延时，因为文章推荐后用户不一定马上看，不考虑这部分时间，整个系统是实时的。

四.召回策略
目前的内容量非常大，加上小视频内容有千万级别，推荐系统不可能所有内容全部由模型预估。
所以需要设计一些召回策略，每次推荐时从海量内容中筛选出千级别的内容库。
召回策略最重要的要求是性能要极致，一般超时不能超过50毫秒。
倒排索引召回: 离线维护一个倒排,这个倒排的key可以是分类，topic,实体,来源等。排序考虑热度，新鲜度,动作等。线上召回可以迅速从倒排中根据
用户兴趣标签内容截断，高效的从很大内容库中筛选比较靠谱的小部分内容。

五.推荐系统的数据依赖
1.推荐模型的特征抽取需要用户侧和内容测的各种标签。
2.召回策略需要获取用户侧和内容侧的各种标签。
3.内容分析和用户标签挖掘是搭建推荐系统的基石。

六.内容分析
主要是文本分析。
1.用户兴趣建模。
简单说就是通过用户的历史行为，给用户打标签。
比如：用户看了个互联网标签的文章=》给用户加互联网这个标签，也就是添加一个特征。

另外，也可以直接帮助推荐特征。比如，魅族内容可以推荐给关注魅族的用户，这就是用户标签的匹配。

某时间推荐主频道效果不理想，就是推荐窄化。用户发现到具体的频道推荐(科技，体育)中阅读后，再回到主feed,推荐效果会更好。
因为整个模型是打通的，子频道探索空间较小，更容易满足用户需求。只通过单一信道反馈提高推荐准确率难度会比较大，子频道做的好很重要。
而这也需要好的内容分析。

文章有分类、关键词、topic、实体词等文本特征。当然不是没有文本特征，推荐系统就不能工作，推荐系统最早期应用在Amazon,
甚至沃尔玛时代就有，包括Netfilx做视频推荐也没有文本特征直接协同过滤推荐。但对资讯类产品而言，大部分是消费当天内容，
没有文本特征新内容冷启动非常困难，协同类特征无法解决文章冷启动问题。

推荐系统主要抽取的文本特征包括以下几类。
(1)语义标签类特征.
显式为文章打上语义标签。这部分标签是由人定义的特征，每个标签有明确的意义，标签体系是预定义的。 -- 一般用RNN来完成。语义判断。
此外还有隐式语义特征，主要是topic特征和关键词特征，
其中topic特征是对于词概率分布的描述，无明确意义;
而关键词特征会基于一些统一特征描述，无明确集合。   -- TF-IDF设置词权重。
越是粒度越细的文本特征，冷启动能力越强: eg :拜仁慕尼黑 体育

(2)文本相似度特征
曾经用户反馈最大的问题之一就是为什么总推荐重复的内容。这个问题的难点在于，
每个人对重复的定义不一样。举个例子，有人觉得这篇讲皇马和巴萨的文章，昨天已经看过类似内容，今天还说这两个队那就是重复。
但对于一个重度球迷而言，尤其是巴萨的球迷，恨不得所有报道都看一遍。解决这一问题需要根据判断相似文章的主题、行文、主体等内容，
根据这些特征做线上策略。 具体我在搜索词里有介绍。

(3)时空特征。
分析内容的发生地点以及时效性。比如武汉限行的事情推给北京用户可能就没有意义。最后还要考虑质量相关特征，判断内容是否低俗，色情，是否是软文，鸡汤。

总结:隐式语义特征已经可以很好的帮助推荐，而语义标签需要持续标注，新名词新概念不断出现，标注也要不断迭代。其做好的难度和资源投入要远大于隐式语义特征，
那为什么还需要语义标签?有一些产品上的需要，比如频道需要有明确定义的分类内容和容易理解的文本标签体系。语义标签的效果是检查一个公司NLP技术水平的试金石。

案例:
推荐系统的线上分类采用典型的层次化文本分类算法。最上面Root，下面第一层的分类是像科技、体育、财经、娱乐，体育这样的大类，再下面细分足球、
篮球、乒乓球、网球、田径、游泳...，足球再细分国际足球、中国足球，中国足球又细分中甲、中超、国家队...，相比单独的分类器，
利用层次化文本分类算法能更好地解决数据倾斜的问题。有一些例外是，如果要提高召回，可以看到我们连接了一些飞线。这套架构通用，但根据不同的问题难度，
每个元分类器可以异构，像有些分类SVM效果很好，有些要结合CNN，有些要结合RNN再处理一下。

上图是一个实体词识别算法的case。基于分词结果和词性标注选取候选，期间可能需要根据知识库做一些拼接，有些实体是几个词的组合，
要确定哪几个词结合在一起能映射实体的描述。如果结果映射多个实体还要通过词向量、topic分布甚至词频本身等去歧，最后计算一个相关性模型。

七.用户标签
1.简介
内容分析和用户标签是推荐系统的两大基石。内容分析涉及到机器学习的内容多一些，相比而言，用户标签工程挑战更大。
常用的用户标签包括用户感兴趣的类别和主题、关键词、来源、基于兴趣的用户聚类以及各种垂直兴趣特征(车型，体育球队，股票等)。
还有性别、年龄、地点等信息。性别信息通过用户第三方社交账号登录得到。年龄信息通常由模型预测，通过机型、阅读时间分布等预估。
常驻地点来自用户授权访问位置信息，在位置信息的基础上通过传统聚类的方法拿到常驻点。常驻点结合其他信息，可以推测用户的工作地点、
出差地点、旅游地点。这些用户标签非常有助于推荐。

2.策略
(1)、过滤噪声。通过停留时间短的点击，过滤标题党。
(2)、热点惩罚。对用户在一些热门文章(如前段时间PG One的新闻)上的动作做降权处理。理论上，传播范围较大的内容，置信度会下降。
(3)、时间衰减。用户兴趣会发生偏移，因此策略更偏向新的用户行为。因此，随着用户动作的增加，老的特征权重会随时间衰减，新动作贡献的特征权重会更大。
(4)、惩罚展现。如果一篇推荐给用户的文章没有被点击，相关特征(类别，关键词，来源)权重会被惩罚。当然同时，也要考虑全局背景，
是不是相关内容推送比较多，以及相关的关闭和dislike信号等。

3.用户标签批量计算框架
每天抽取昨天使用过的用户。
抽取这些用户过去俩个月的动作数据
在Hadoop集群批量计算结果。
Scribe收集原始日志 - > 导入Kafka文件队列 -> 导入Hadoop集群 -> MapReduce Job 批量计算 -> 写入高性能分布式存储系统(Hbase) ->线上读取
高性能分布式存储系统。

用户标签挖掘总体比较简单，主要还是刚刚提到的工程挑战。头条用户标签第一版是批量计算框架，流程比较简单，
每天抽取昨天的日活用户过去两个月的动作数据，在Hadoop集群上批量计算结果。

问题：
但问题在于，随着用户高速增长，兴趣模型种类和其他批量处理任务都在增加，涉及到的计算量太大。2014年，批量处理任务几百万用户标签更新的Hadoop任务，
当天完成已经开始勉强。集群计算资源紧张很容易影响其它工作，集中写入分布式存储系统的压力也开始增大，并且用户兴趣标签更新延迟越来越高。

面对这些挑战。2014年底今日头条上线了用户标签Storm集群流式计算系统。改成流式之后，只要有用户动作更新就更新标签，CPU代价比较小，可以节省80%的CPU时间，
大大降低了计算资源开销。同时，只需几十台机器就可以支撑每天数千万用户的兴趣模型更新，并且特征更新速度非常快，基本可以做到准实时。
这套系统从上线一直使用至今。

当然，我们也发现并非所有用户标签都需要流式系统。像用户的性别、年龄、常驻地点这些信息，不需要实时重复计算，就仍然保留daily更新。

八.评估分析







          
