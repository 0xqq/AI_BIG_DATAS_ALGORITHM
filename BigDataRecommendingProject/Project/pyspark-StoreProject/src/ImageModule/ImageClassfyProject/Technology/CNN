一.架构
  卷积+池化(Relu函数)+全连接+softmax分类。
  卷积表达式: s(t)=(X∗W)(t)
  二维卷积: s(i,j)=(X∗W)(i,j)=∑m∑nx(i+m,j+n)w(m,n)
  
  
二.基础概念
1.卷积：
  卷积过程不太好解释，你只需要知道一个卷积核里的相应系数相当于我们的w,分别与矩阵做内积的和就是下一层的第一个数。依次类推，每次走stride距离。
  所以再padding=SAME情况下，stride=1相当于卷积后维度没发生改变，简单公示就是维度/stride即可。 如果padding不是等于SAME,那就是维度/stide+1. 但一般
  都是SAME，因为不能忽略边框。
  这样做的缺点就是边框每次只提取一次计算，所以学习能力会降低，所以我们通常会加一个padding边框都是0的数，这一行边缘框也会学习至少2次。都是0是因为不影响
  内积的结果。
2.池化:
  卷积后，我们需要提取有用的特征，就进行池化，如果是2*2,就是矩阵中每2*2变成一个元素，同理3*3同理。里面还分max与ave,一个最大，一个取平均。我个人认为
  这俩个需要看业务分析，在图像领域中一般深色颜色数值比较高，取深色多的取max,如果想更稳定取ave,数据挖掘也同样道理，看你更希望取什么样的特征。
  如果是反卷积过程，池化就是倒过来，比如一个数变为四个数: 0,max,0,0 这类格式，只是把它想成四个方阵即可。 如果是平均数就是 1/4*特征值,1/4*特征值,
  1/4*特征值,1/4*特征值。
  关于池化，很少有人能解释原因，这也是深度学习可解释性差的原因，我们正能在代码中不断的做实验得到结果。而池化的理解就是特征的提取，至于放到哪，多少层卷积后
  池化好，都是通过各种CNN结构的前人总结出来的。

三.前向传播
  与DNN基本一样，公式: ReLu(al-1*W+b)
  如果是全连接:W*al-1+b 
  也就是都是上一层乘以这一层的filter的w再进行内积计算。最后套relu. 有时候会用到正则，batch_norm. 这里会夹在卷积与relu之间。
  而我们计算的公式就是上一层的输出结果=下一层的输入。 b就是输出的个数。
  一般来说是四维，个数+长*宽*通道数。 通道数就是我们设计的深度，这个通道数需要我们自己指定。
  而最后全连接层需要拉平成二维，也就是上面的个数*长*宽*通道数 拉平成 一个大的 长*宽。 因为全连接就是DNN的全部连接成一个二维矩阵。
  
三.反向传播
  1.池化层反向: 如果是ave: 就是把它扩大成原来卷积部分的矩阵，中间还是平均数，边缘都是0
               如果是max: 还是把它扩大成原来卷积部分的矩阵，将这几个数分散到各个四个或者9个格子区域中
  2.记住，我们正常的普通层还是叫隐藏层，只是经过卷积或者池化过程而已
  3.卷积->隐藏层: 这时候相当于反卷积，如果用矩阵表达，就是利用原来的卷积公式进行反向推倒，一般边缘层的乘数比较少，中间层的乘数比较多。最后得到原来的卷积
        结果比如对于a11的梯度，由于在4个等式中a11只和z11有乘积关系，从而我们有：
        ∇a11=δ11w11
　　　　 对于a12的梯度，由于在4个等式中a12和z12，z11有乘积关系，从而我们有：
        ∇a12=δ11w12+δ12w11
　　　  同样的道理我们得到：
        ∇a13=δ12w12
        ∇a21=δ11w21+δ21w11
        ∇a22=δ11w22+δ12w21+δ21w12+δ22w11  中间层
        ∇a23=δ12w22+δ22w12
        ∇a31=δ21w21
        ∇a32=δ21w22+δ22w21
        ∇a33=δ22w22
        
   4.全连接层:
       我们现在已经可以递推出每一层的梯度误差δl了，对于全连接层，可以按DNN的反向传播算法求该层W,b的梯度，而池化层并没有W,b,也不用求W,b的梯度。只有卷积层的W,b需要求出。
      
　　　　注意到卷积层z和W,b的关系为：
       zl=al−1∗Wl+b
　　　　因此我们有：
       ∂J(W,b)∂Wl=∂J(W,b)∂zl∂zl∂Wl=δl∗rot180(al−1)
　　　　由于我们有上一节的基础，大家应该清楚为什么这里求导后要旋转180度了。
　　　　而对于b,则稍微有些特殊，因为δl是三维张量，而b只是一个向量，不能像DNN那样直接和δl相等。通常的做法是将δl的各个子矩阵的项分别求和，得到一个误差向量，即为b的梯度：
       ∂J(W,b)∂bl=∑u,v(δl)u,v

四.总结:
  现在我们总结下CNN的反向传播算法，以最基本的批量梯度下降法为例来描述反向传播算法。

　输入：m个图片样本，CNN模型的层数L和所有隐藏层的类型，对于卷积层，要定义卷积核的大小K，卷积核子矩阵的维度F，填充大小P，步幅S。对于池化层，要定义池化区域大小k和池化标准（MAX或Average），对于全连接层，要定义全连接层的激活函数（输出层除外）和各层的神经元个数。梯度迭代参数迭代步长α,最大迭代次数MAX与停止迭代阈值ϵ
　输出：CNN模型各隐藏层与输出层的W,b
　1) 初始化各隐藏层与输出层的各W,b的值为一个随机值。

   2）for iter to 1 to MAX：

　2-1) for i =1 to m：

　　　a) 将CNN输入a1设置为xi对应的张量

　　　b) for l=2 to L-1，根据下面3种情况进行前向传播算法计算：

　　　b-1) 如果当前是全连接层：则有ai,l=σ(zi,l)=σ(Wlai,l−1+bl)
　　　b-2) 如果当前是卷积层：则有ai,l=σ(zi,l)=σ(Wl∗ai,l−1+bl)
　　　b-3) 如果当前是池化层：则有ai,l=pool(ai,l−1), 这里的pool指按照池化区域大小k和池化标准将输入张量缩小的过程。

　　　c) 对于输出层第L层: ai,L=softmax(zi,L)=softmax(WLai,L−1+bL)
　　　c) 通过损失函数计算输出层的δi,L
　　　d) for l= L-1 to 2, 根据下面3种情况进行进行反向传播算法计算:

　　　d-1)  如果当前是全连接层：δi,l=(Wl+1)Tδi,l+1⊙σ′(zi,l)
　　　d-2) 如果当前是卷积层：δi,l=δi,l+1∗rot180(Wl+1)⊙σ′(zi,l)
　　　d-3) 如果当前是池化层：δi,l=upsample(δi,l+1)⊙σ′(zi,l)
　2-2) for l = 2 to L，根据下面2种情况更新第l层的Wl,bl:

　　　2-2-1) 如果当前是全连接层：Wl=Wl−α∑i=1mδi,l(ai,l−1)T， bl=bl−α∑i=1mδi,l
　　　2-2-2) 如果当前是卷积层，对于每一个卷积核有：Wl=Wl−α∑i=1mδi,l∗rot180(ai,l−1)， bl=bl−α∑i=1m∑u,v(δi,l)u,v
　2-3) 如果所有W，b的变化值都小于停止迭代阈值ϵ，则跳出迭代循环到步骤3。

　3） 输出各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b。
  
  
 
