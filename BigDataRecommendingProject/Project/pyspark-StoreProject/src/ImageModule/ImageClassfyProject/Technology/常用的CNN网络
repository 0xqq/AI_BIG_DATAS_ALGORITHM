常用的CNN网络

小知识:
感受野:(就是out值)
在卷积神经网络CNN中，决定某一层输出结果中一个元素所对应的输入层的区域大小，被称作感受野receptive field。举个例子，
在maxpooling层中，如果它的kenerl size是2x2，输出结果中的每一个元素都是其对应输入的2x2的区域中的最大值，
所以这一层的感受野大小就是2。其实感受野的大小是由kernel size和stride size一起决定的，
公式是：rfsize = f(out, stride, ksize) = (out - 1) * stride + ksize，其中out是指上一层感受野的大小。


一.Alexnet
结构是:
卷积->池化->卷积->池化->卷积->卷积->卷积->池化->全连接->全连接->全连接直接输出。
第一个卷积:11*11*3 stride: 4 . 输入224，输出55 
经历ReLu.
双显卡，同时训练。
这里ReLu与卷积之间，进行局部标准化可以提高性能。也就是x-min/max-x
池化层是不重叠的，但AlexNet设计时是可以重叠，也就是他的边长与stride不是一样的。
也就是3*3的池化，stride为2.这样每走一次重叠一次。可能设计者认为这样效果更好，更容易拟合。
数据增强:原始图片大小为256*256，在图片上随机选取224*224的小块进行训练，
还可以这些小块进行水平翻转进一步增加数据量。另一种方法是使用PCA改变训练图像RGB通道的像素值。
优化:SGD
正则化:drop out

注意: 本人提出的问题点: 第三层使用了类似全连接的情况，也就是双显卡每个显卡的俩个特征图分别提取
到不同显卡上，我一直没找到这个原因所在，论文没给解释。我个人推断是之前连续做了2次maxpooling，为了防止欠你和，让拟合提高。
做个类似全连接操作，也就是不同层分别映射到不同显卡上，提高拟合能力。

总结:VGG没什么太多亮点，唯一不同是双显卡分别操作，第三层时进行类似全连接操作增加拟合。
开始都是大步长移动，层数很浅，效果不太好。但开始的各个功能基本都包含了。
其中池化层是用重叠技术可能要加强拟合能力。

二.Vgg
相比AlexNet，做了以下全面提升
1.基本特点
1.小卷积核: 卷积核全部替代为3*3 相比AlexNet的11*11,5*5,..3*3...更加抽象
2.小池化核: 相比AlexNet的3*3的池化核,VGG全部为2*2的池化核。
3.层数更深特征图更宽: 由于卷积核更小，导致更扩大了通道数。池化时更加缩小了宽和高,使得计算量增加放缓。
4.全连接转卷积: 网络测试阶段将训练阶段的三个全连接替换为三个卷积,测试重用训练时的参数。
使得测试得到的全卷积网络没有全连接限制。所以可以接受任意宽高为输入。

2.特点提出原因
(1).为什么设计小卷积核:
AlexNet最开始为大卷积核11*11,5*5.目的在于先捕捉到整体特征，第三层时再分别合并通道转为小卷积核，
是防止丢失掉更深层次的局部范围的特征相关性。所以是11*11->5*5->3*3合并->3*3->3*3

VGG之所以全用3*3,是因为计算量的原因，这里涉及到实时处理，移动端部署，是否易于训练。
还有感受野的原因，涉及到参数更新，特征图大小，特征提取是否够多，模型复杂度参数量等。

(2).计算量原因:
通过前人实验表明,大卷积与小卷积得到的参数量差距不大，但大卷积由于是11*11的显然计算量更大。
所以推出卷积核大小的特征图和卷积参数差别不大，越大卷积核计算量越大。

(3)感受野原因:
通过前人实验=》
二层3*3感受野相当于1层5*5的感受野。
三层3*3感受野相当于1层7*7的感受野。
如图:

从图上也得出，反向传播时，第一层(也就是最后一层)俩个神经元的参数是通过第一层的7个神经元，。同理，
如果是个大卷积层，会影响更多的输入神经元，这样大卷积层获取相同的感受野时会影响更多的神经元。

3.总结VGG使用小卷积的优势
(1).因为多个3*3与大卷积感受野相同，所以进行相同感受野时可以获取更多激活函数(relu进行不停的非线性导致更准确)，特征，所以获得更强识别能力。
(2).减少卷积参数，我们实验得到3层3*3感受野≈1层7*7.但参数是
3*(C*3*3*C) = 27C^2
一层7*7参数 = c*7*7*c = 49^2.
所以使用更少卷积参数，速度更快。
(4).小卷积有更多正则，conv3*3三个代替一个conv7*7.相当于分解，进一步提取特征。

5.小池化核
可以理解为设计小了提取更多特征，本质也是提取特征，使其更准确。
max-popling本质也是更容易捕捉图像上的变化，梯度变化，带来更大的局部信息差异性。

6.全连接和特征图
全连接:
：bias的初始值，由AlexNet的0变为0.1，该层初始化高斯分布的标准差，由AlexNet的0.01变为0.005。超参数的变化，
我的理解是，作者自己的感性理解指导认为，我以贡献bias来降低标准差，相当于标准差和bias间trade-off，
或许作者实验validate发现这个值比之前AlexNet设置的（std=0.01，bias=0）要更好。

特征图:
网络层数递增时,池化会逐渐忽略局部信息，特征图宽高经过池化依次变小，深度依次增大。这样特征信息从一开始输入的224*224*3
变换到7*7*512.特征分散到不同channel上。
所以才有了全连接层。将各个局部信息特征图映射到4096维度上。
原来的第5层是7*7*512压缩到1/5.所以作者在这里又设计一层4096作为缓冲。我理解是映射过程的学习慢慢来。也让bp学的更慢一点。
本质:卷积增加通道数，池化减少filter宽高。如果只是为了减少宽高，也可以用stride。
所以未来的网络可能没有池化。

卷积组:
前两组卷积形式一样，每组都是：conv-relu-conv-relu-pool；
中间三组卷积形式一样，每组都是：conv-relu-conv-relu-conv-relu-pool；
最后三个组全连接fc层，前两组fc，每组都是：fc-relu-dropout；最后一个fc仅有fc。
不难发现VGG有两种卷积组，第二种（[conv-relu]-[conv-relu]-[conv-relu]-pool）比第一种（[conv-relu]-[conv-relu]-pool） 
多了一个[conv-relu]。我的理解是：

7.个人理解:
多出的relu对网络中层进一步压榨提炼特征。结合一开始单张feature map的local信息更多一些，还没来得及把信息分摊到channel级别上，
那么往后就慢慢以增大conv filter的形式递增地扩大channel数，等到了网络的中层，channel数升得差不多了（信息分摊到channel上得差不多了）
，那么还想抽local的信息，就通过再加一个[conv-relu]的形式去压榨提炼特征。有点类似传统特征工程中，已有的特征在固定的模型下没有性能提升了，
那就用更多的非线性变换对已有的特征去做变换，产生更多的特征的意味；
多出的conv对网络中层进一步进行学习指导和控制不要将特征信息漂移到channel级别上。
上一点更多的是relu的带来的理解，那么多出的[conv-relu]中conv的意味就是模型更强的对数据分布学习过程的约束力/控制力，做到信息backprop可以回
传回来的学习指导。本身多了relu特征变换就加剧（权力释放），那么再用一个conv去控制（权力回收），也在指导网络中层的收敛；
其实conv本身关注单张feature map上的局部信息，也是在尝试去尽量平衡已经失衡的channel级别（depth）和local级别（width、height）之间的天平。
这个conv控制着特征的信息量不要过于向着channel级别偏移。

8.提炼出如下结论：
深度提升性能；
最佳模型：VGG16，从头到尾只有3x3卷积与2x2池化。简洁优美；
开源pretrained model。与开源深度学习框架Caffe结合使用，助力更多人来学习；
卷积可代替全连接。整体参数达1亿4千万，主要在于第一个全连接层，用卷积来代替后，参数量下降（这里的说法我认为是错的，替代前后用同样的输入尺寸，
网络参数量、feature map、计算量三者没有变化）且无精度损失。

LRN层作用不大。
越深的网络效果越好。
1*1的卷积也是很有效的，但是没有3*3的卷积好，大一些的卷积核可以学习更大的空间特征。


三.Resnet
VGG网络存在的问题:
计算机视觉里，特征的“等级”随增网络深度的加深而变高，研究表明，
网络的深度是实现好的效果的重要因素。然而梯度弥散/爆炸成为训练深层次的网络的障碍，导致无法收敛。
有一些方法可以弥补，如归一初始化，各层输入归一化，使得可以收敛的网络的深度提升为原来的十倍。然而，
虽然收敛了，但网络却开始退化了，即增加网络层数却导致更大的误差.通过在一个浅层网络基础上叠加y=x的层（称identity mappings，恒等映射），
可以让网络随深度增加而不退化。这反映了多层非线性网络无法逼近恒等映射网络。

Resnet的核心思想: F(x) = H(x) - x。如果F(x) = 0 就是恒等映射， 假如优化目标函数是逼近一个恒等映射, 而不是0映射，
那么学习找到对恒等映射的扰动会比重新学习一个映射函数要容易。可以看出，残差函数一般会有较小的响应波动，表明恒等映射是一个合理的预处理。

通俗点说: 我们数据通过隐层时，假设通过俩个隐层。分成了俩个路通过，一个路是正常直接通过，另外一个路是跳过这俩层。最后跳过后和经过2个隐层的输入值会和
想加，再经过relu，然后接着下俩个隐层来回反复操作。公式为: y = F(x,{Wi})+x
然后，在resnet中，每四个也就是没间隔3个，会发生一次通道数翻倍，此时跳过的那条线会做个非线性转换。
公式为:y = F(x,{Wi})+Wsx (Ws是这个通道数发生变换的线性转换)

为什么这样做可以解决梯度爆炸不收敛问题? 我个人理解:
其实这种跳转，本质就是调节权重的问题，一个路是直接跳到下俩层，一个路是通过卷积relu到下俩层。最后想加时会有
个调权重的机制，所以想加和经过relu。 当我们发现梯度不对劲时，通过调节不经过卷积的权重和经过卷积的权重，来平衡梯度，防止其爆炸，本质像正则化。

残差网络的确解决了退化的问题，在训练集和校验集上，都证明了的更深的网络错误率越小。

ResNet的其他优化：
，即将两个3x3的卷积层替换为1x1 + 3x3 + 1x1, 
新结构中的中间3x3的卷积层首先在一个降维1x1卷积层下减少了计算，（256深度通过1*1降低到64）
然后在另一个1x1的卷积层下做了还原，既保持了精度又减少了计算量。(中间经过3*3,然后再经过1*1，从64还原到256)
这样又间接的增加了深度，而且计算速度高于俩个3*3.


四.Densnet
1.核心原理: 网络每层的输入都是前面所有层输出的并集。

2.引申言论:
(1).首先，它说明了神经网络其实并不一定要是一个递进层级结构，也就是说网络中的某一层可以不仅仅依赖于紧邻的上一层的特征，
而可以依赖于更前面层学习的特征。想像一下在随机深度网络中，当第 l 层被扔掉之后，第 l+1 层就被直接连到了第 l-1 层；
当第 2 到了第 l 层都被扔掉之后，第 l+1 层就直接用到了第 1 层的特征。因此，随机深度网络其实可以看成一个具有随机密集连接的 DenseNet。

(2).其次，我们在训练的过程中随机扔掉很多层也不会破坏算法的收敛，说明了 ResNet 具有比较明显的冗余性，
网络中的每一层都只提取了很少的特征（即所谓的残差）。实际上，我们将训练好的 ResNet 随机的去掉几层，对网络的预测结果也不会产生太大的影响。
既然每一层学习的特征这么少，能不能降低它的计算量来减小冗余呢？

3.
DenseNet 的设计正是基于以上两点观察。我们让网络中的每一层都直接与其前面层相连，实现特征的重复利用；同时把网络的每一层设计得特别「窄」，
即只学习非常少的特征图（最极端情况就是每一层只学习一个特征图），达到降低冗余性的目的。这两点也是 DenseNet 与其他网络最主要的不同。
需要强调的是，第一点是第二点的前提，没有密集连接，我们是不可能把网络设计得太窄的，否则训练会出现欠拟合（under-fitting）现象，
即使 ResNet 也是如此。

DenseNet 有什么优点？
1.省参数。在 ImageNet 分类数据集上达到同样的准确率，DenseNet 所需的参数量不到 ResNet 的一半。
对于工业界而言，小模型可以显著地节省带宽，降低存储开销。
2.省计算。
3.抗过拟合。神经网络每一层提取的特征都相当于对输入数据的一个非线性变换，而随着深度的增加，
变换的复杂度也逐渐增加（更多非线性函数的复合）。相比于一般神经网络的分类器直接依赖于网络最后一层（复杂度最高）的特征，
DenseNet 可以综合利用浅层复杂度低的特征，因而更容易得到一个光滑的具有更好泛化性能的决策函数。

密集连接不会带来冗余吗？

这是一个很多人都在问的问题，因为「密集连接」这个词给人的第一感觉就是极大的增加了网络的参数量和计算量。
但实际上 DenseNet 比其他网络效率更高，其关键就在于网络每层计算量的减少以及特征的重复利用。
DenseNet 的每一层只需学习很少的特征，使得参数量和计算量显著减少。。

不少人跟我们反映过 DenseNet 在训练时对内存消耗非常厉害。(我也这么认为，并行操作加上并集操作，
有点类似空间换取计算复杂度/性能)这个问题其实是算法实现不优带来的。当前的深度学习框架对 DenseNet 的密集连接没有很好的支持
，我们只能借助于反复的拼接（Concatenation）操作，将之前层的输出与当前层的输出拼接在一起，然后传给下一层。对于大多数框架（
如 Torch 和 TensorFlow），每次拼接操作都会开辟新的内存来保存拼接后的特征。这样就导致一个 L 层的网络，
要消耗相当于 L(L+1)/2 层网络的内存（第 l 层的输出在内存里被存了 (L-l+1) 份）。
解决这个问题的思路其实并不难，我们只需要预先分配一块缓存，供网络中所有的拼接层（Concatenation Layer）共享使用，
这样 DenseNet 对内存的消耗便从平方级别降到了线性级别。说白了就是缓存机制提高内存空间。做个临时存储让其他层共享使用。
在梯度反传过程中，我们再把相应卷积层的输出复制到该缓存，就可以重构每一层的输入特征，进而计算梯度。
当然网络中由于 Batch Normalization 层的存在，实现起来还有一些需要注意的细节。

总的来说，训练 DenseNet 跟训练其他网络没有什么特殊的地方，对于训练 ResNet 的代码，
只需要把模型替换成 DenseNet 就可以了。如果想对 DenseNet 的模型做一些改进，我们有一些建议供参考：
每层开始的瓶颈层（1x1 卷积）对于减少参数量和计算量非常有用。
像 VGG 和 ResNet 那样每做一次下采样（down-sampling）之后都把层宽度（growth rate) 增加一倍，
可以提高 DenseNet 的计算效率（FLOPS efficiency）。
与其他网络一样，DenseNet 的深度和宽度应该均衡的变化，当然 DenseNet 每层的宽度要远小于其他模型。
每一层设计得较窄会降低 DenseNet 在 GPU 上的运算效率，但可能会提高在 CPU 上的运算效率。


五.Inception

特点:帮你决定选取什么样的卷积核。 我们之前总结，开始用1*1->3*3->1*1,然后全部3*3这样效果好，因为有降维，升维，以及3*3这种综合的特征提取，配合
残差，desnet的并集等操作。这样效果达到最优化。但Inception却又更特殊一步。
Inception层中，有多个卷积层结构（Conv）和Pooling结构（MaxPooling），它们利用了padding的原理，让经过这些结构的最终结果Shape不变。

亮点本质: 主要提出了Inceptionmodule结构（1*1，3*3，5*5的conv和3*3的pooling组合在一起）.
具体图看:https://blog.csdn.net/ybdesire/article/details/80628586

下面是各个维度特点:

C_1X1: 28x28x192的输入数据，与64个1x1的卷积核做卷积后，得到28x28x64的输出
C_3X3: 28x28x192的输入数据，与128个3x3的卷积核做卷积后，得到28x28x128的输出
C_5X5: 28x28x192的输入数据，与32个5x5的卷积核做卷积后，得到28x28x32的输出
MP: 28x28x192的输入数据，做MaxPooling后（带padding），得到28x28x32的输出
多个Inception层组合在一起，就构成了Inception网络。但这样直接计算，计算量很大，所以要利用1x1的卷积核，来降低计算量。

考虑如下问题，经过这一次卷积后，计算量为多少？

输入数据维度：28x28x192
卷积核大小：5x5x32
输出数据维度：28x28x32
输出数据共有28x28x32个值，每个值都要进行5x5x192次乘法（卷积中的）运算，所以一共要进行28x28x32x5x5x192次乘法计算，即120,422,400。

同样的问题，如果先经过1x1的卷积计算，再通过，如下图

输入数据维度：28x28x192
卷积核1大小：1x1x16
卷积核2大小：5x5x32
输出数据维度：28x28x32

经过第一个卷积层，要进行(28x28x16x1x1x192=2,408,448)次乘法计算，经过第二个卷积层，要进行（28x28x32x5x5x16=10,035,20）次乘法计算。总共计算次数也就是（2,408,448+10,035,20=12,443,648）次，比上面直接经过一个卷基层的计算量（120,422,400）要小很多。

可见，用好1x1的卷积核，就能降低计算量。

Inception网络
带有1x1卷积核的Inception层，就构成了Inception网络的基本单元


这样的多个Inception层组合在一起，就构成了Inception网络


当然，实际论文中的Inception网络还包括输出端的全连接层，还有Inception网络会从中间层引出多个输出，再连接SoftMax，这样能减少过拟合（让中间层也起作用）。

变种: 

InceptionV2:在卷积核ReLu层加了正则化，BN，在GAN中使用过，防止过拟合，因为开始过多的1*1一定程度加大了参数学习能力。
            将5*5被2个3*3取代，我上文中提到过，感受野一样，计算量减少。
InceptionV3:(1) 将7*7分解成两个一维的卷积（1*7,7*1），3*3也是一样（1*3,3*1），
                这样的好处，既可以加速计算（多余的计算能力可以用来加深网络），又可以将1个conv拆成2个conv，
                使得网络深度进一步增加，增加了网络的非线性，更加精细设计了35*35/17*17/8*8的模块。
            (2)增加网络宽度，网络输入从224*224变为了299*299。

InceptionV4:
Inception v4主要是增加了残差，所以通常使用这个算法做图像分类，跟ResNet差不多。

具体看https://blog.csdn.net/qq_14845119/article/details/73648100
      https://blog.csdn.net/ybdesire/article/details/80628586 这俩个图解。

Inception v4的亮点总结如下：

(1)将Inception模块和ResidualConnection结合，提出了Inception-ResNet-v1，Inception-ResNet-v2，使得训练加速收敛更快，精度更高。

(2)设计了更深的Inception-v4版本，效果和Inception-ResNet-v2相当。

(3)网络输入大小和V3一样，还是299*299

与ResNet本质区别:当然Inception系列本质是将数据分成多个管道，再拼接一起增加了通道数，这也是跟ResNet本质区别。
也就是了Inceptionmodule结构（1*1，3*3，5*5的conv和3*3的pooling组合在一起）. 有点像特征拼接，但这是增加了深度。

Inception-Resnet:
https://blog.csdn.net/googler_offer/article/details/79535222
主要是一个数据拆分通过各个卷积增加通道再拼接的过程，及其复杂，具体看相关论文和代码实现。

六.Xception
具体看: https://blog.csdn.net/whz1861/article/details/78395684

七.DPN
具体看:https://blog.csdn.net/u014380165/article/details/75676216
DPN特点:
DPN和ResNeXt（ResNet）的结构很相似。最开始一个7*7的卷积层和max pooling层，
然后是4个stage，每个stage包含几个sub-stage，再接着是一个global average pooling和全连接层，
最后是softmax层。重点在于stage里面的内容，也是DPN算法的核心。

因为DPN算法简单讲就是将ResNeXt和DenseNet融合成一个网络，因此在介绍DPN的每个stage里面的结构之前，
先简单过一下ResNet（ResNeXt和ResNet的子结构在宏观上是一样的）和DenseNet的核心内容。
下图中的（a）是ResNet的某个stage中的一部分。（a）的左边竖着的大矩形框表示输入输出内容，对一个输入x，
分两条线走，一条线还是x本身，另一条线是x经过1*1卷积，3*3卷积，1*1卷积（这三个卷积层的组合又称作bottleneck），
然后把这两条线的输出做一个element-wise addition，也就是对应值相加，就是（a）中的加号，得到的结果又变成下一个同样模块的输入，
几个这样的模块组合在一起就成了一个stage（比如Table1中的conv3）。（b）表示DenseNet的核心内容。（b）的左边竖着的多边形框表示输入输出内容，
对输入x，只走一条线，那就是经过几层卷积后和x做一个通道的合并（cancat），得到的结果又成了下一个小模块的输入，这样每一个小模块的输入都在不断累加，
举个例子：第二个小模块的输入包含第一个小模块的输出和第一个小模块的输入，以此类推。

这里写图片描述

DPN是怎么做呢？简单讲就是将Residual Network 和 Densely Connected Network融合在一起。
下图中的（d）和（e）是一个意思，所以就按（e）来讲吧。（e）中竖着的矩形框和多边形框的含义和前面一样。
具体在代码中，对于一个输入x（分两种情况：一种是如果x是整个网络第一个卷积层的输出或者某个stage的输出，会对x做一个卷积，
然后做slice，也就是将输出按照channel分成两部分：data_o1和data_o2，可以理解为（e）中竖着的矩形框和多边形框；
另一种是在stage内部的某个sub-stage的输出，输出本身就包含两部分：data_o1和data_o2），走两条线，一条线是保持data_o1和data_o2本身，
和ResNet类似；另一条线是对x做1*1卷积，3*3卷积，1*1卷积，然后再做slice得到两部分c1和c2，最后c1和data_o1做相加（element-wise addition）
得到sum，类似ResNet中的操作；c2和data_o2做通道合并（concat）得到dense（这样下一层就可以得到这一层的输出和这一层的输入），也就是最后返回两个值：
sum和dense。以上这个过程就是DPN中 一个stage中的一个sub-stage。有两个细节，一个是3*3的卷积采用的是group操作，类似ResNeXt，
另一个是在每个sub-stage的首尾都会对dense部分做一个通道的加宽操作。


总结：
作者提出的DPN网络可以理解为在ResNeXt的基础上引入了DenseNet的核心内容，使得模型对特征的利用更加充分。
原理方面并不难理解，而且在跑代码过程中也比较容易训练，同时文章中的实验也表明模型在分类和检测的数据集上都有不错的效果。
