一.感知机:
    如果谈深度学习，先要从感知机说起。感知机的前提条件是对线性可分的数据进行二分类。它完成不了多分类，所以深度学习在这基础上加上sigmod
和隐藏层(矩阵运算)完成了多分类。用数学表示就是:θ0+θ1x1+...+θnxn>0 ，让另一种类别的样本都满足θ0+θ1x1+...+θnxn<0 ，从而得到线性可分,也就是
超平面不唯一，有多个解。也可以用sinx表示，小于0为-1.大于0为1.
    它的损失函数是J(θ)=?∑xi∈My(i)θ?x(i)。是面到点距离和，这点跟SVM相似。
    
    
二.神经网络DNN
1.前向传播:
    基于感知机的二分类问题，神经网络DNN在此基础上加了俩个条件:隐藏层和sigmod激活函数。
    我们输入的样本，可以理解为每个样本的多个特征前面都有个参数w,然后还有个截距项b。也就是个线性回归模型。我们的最终目的是将所有样本的参数w和截距项b达到
损失值最小。而我们传播一次到输出位置前，需要经历多个sigmod函数，也就是逻辑回归的分类函数。所以咱们可以把DNN前向传播理解为多个LR回归在blending集成学习
的条件得到一个好的输出结果，如果是分类问题，就用sigmod函数激活，如果是回归问题(当然这里我们没有这个解释)，就取平均值。
    隐藏层:主要作用就是增强模型表达能力，通过矩阵计算公式求得，是个全连接操作。假设输入值的矩阵是Xm*n. 那么经历第一个s维隐藏层就是所有m个样本的n维
特征乘以每个s的神经元得到的一个结果是作为下一层的输入。依次类推，所以根blending很像。这种计算方式适用于DNN每一层。
    输出过程:
    假设有俩层都是3个神经元的隐藏层，最后输出一个结果。
    第一层是x1,x2,x3三个神经元。第二层为a1,a2,a3.那么第二层的输出结果就是
    a1^2 = w11^2.x1+w12^2.x2+w13^2.x3+b1^2
    a2^2 = w21^2.x1+w22^2.x2+w23^2.x3+b2^2
    a3^2 = w31^2.x1+w32^2.x2+w33^2.x3+b3^2
    请记住，根据矩阵乘法运算得出: w的总数为神经元的乘积
    所以下一层的输出就是我们上面提到的=LR模型。
    同理，我们最后输出层的结果是
    out = w11^2.a1^2+w12^2.a2^2+w13^2.a3+b1^2
    总结:L-1层有m个神经元，L层有n个神经元。 那么第L层线性系数w组成了一个 n*m 矩阵Wl,第l层的截距项b就是n*1的向量bl.
    第l-1层的输出a组成一个m*1的向量a^l-1. 第l层未激活前线性输出z组成一个n*1的向量zl。 第l层的输出a组成一个n*1的向量a^l。
    第l层输出为 al = Wl.al-1+bl
    所以，如果有N个层，我们总结的公式就是
    al = (Wlal-1+bl) 做个循环即可。这就是前向传播。

2.反向传播:
    本质:梯度下降，最优化问题，是局部最优。
    基本思路:
    (1)先确定损失函数
    我们这里用1/2||al-y||^2
    (2)第L层损失 = 1/2||al-y||^2 =  (Wlal-1+bl)-yreal 也就是我们上面的第L层公式-真实值
    (3)然后就是根据链式法则去求，这是个很漫长的过程，具体请看有道云笔记的反向传播。大概意思就是分俩种:
    
    (1)输出层到隐含层的权值更新
    先求出总误差E，然后从输出层到隐含层进行权值更新，这里是总误差E对隐含层W的偏导来更新，但直接求不出来，所以要经过
    E对输出结果out求偏导*out对隐含层的net求偏导*net对权值w求偏导。我们的net就是我们的输出分别乘以权重w的和+b.因为是反向传播。所以反推过去。
    这样的本质也是矩阵运算。
    (2)隐含层之间的权值更新
    记住,开始不用总误差E，而是分别误差按照我们上面的方法用链式法则求导，然后再求和，再更新W。
    这也是隐含层之间权值更新的不同。这个要比之前那个麻烦点，我在github上也更新了源码，欢迎有空来看。
    
    总结:BP反向传播本质:
    输出层->隐藏层 = 总误差对W偏导= 总误差对输出层偏导*输出层对隐含层偏导*隐含层对w偏导。 隐含层就是 y = w11*x1+w12*w2+....+b所以都是可求的。
    隐藏层->隐藏层 = 各个层误差直线传播对W偏导  = 该层误差对该层输出偏导*该层输出*该层隐含层偏导*该层隐含层对w偏导。 然后有多少个误差(每个误差对
    输出层是一一对应关系)之间都是通过了该层隐含的偏导，这个之间求个总和再对w更新权重即可。
    
3.项目中损失函数与激活函数的选择。
    (1)sigmod改进版来提高收敛程度:
       (针对所有类型项目)
       
       由sigmod图可知在俩边极端时数值增长过于缓慢，这样w更新到极值及其缓慢，所以需要用交叉熵损失函数改进。
       交叉熵损失函数: J(W,b,a,y)=?y?lna?(1?y)?ln(1?a).
       最后推倒出: (aL?y)⊙σ′(z)
       梯度最终成为预测值和真实值的差距。 通常肯定要比均方差损失好用。
       
    (2)对数似然损失函数和softmax激活函数进行DNN分类输出：
        (针对数据量连续可导的值来求多分类问题)
        
        softmax:假设一堆数V有j个，我们相求第i个数的概率。 pi = e^Vi/∑evj 也就是这个i的数指数除以j个数的指数求和。
        基于这个原理来做分类，这样连续可导的数就可以直接将其分类。
        如果是反向传播,就用对数似然函数 J(W,b,aL,y)=?∑kyklnaLk
        化简为: J(W,b,aL,y)=?lnaLi
        梯度为: ?J(W,b,aL,y)?bLi=aLi?1 记住梯度就是求偏导。
        简单说就是:
        归一化分类，每个元素都映射到0~1
        先将答案用e^x得出答案，再归一化，归一化是所有exp操作后总和除以各个值。
        e^x是为了让大的值更大，小的值更小。
        损失函数是-log(归一化后的值) 因为在0~1的log是负数，所以前面加负号。
        
    (3)梯度爆炸和梯度消失就用Relu.
        梯度消失就是每层权值都小于1一个小的权值*一个小的权值得到很接近与0的，这样逐渐下去就形成消失。做项目时损失值长期不变就是梯度消失。
        梯度爆炸就是每层权值都大有1，这样无限增大相乘，达到特别大发生爆炸，做项目时损失值跳的特别厉害就是梯度爆炸。
        Relu正好是max(0,x) 所以永远不会消失。小于0强制让他为0，至今理论依据没有。
    (4) tanh: 
        用于稍微多的层数神经元，具体还是要根据数据业务而定。
        
        这个我在GAN中使用过。图形有点像sigmod.
        tanh(z)=ez?e?z/ ez+e?z
        与sigmode关系: tanh(z)=2sigmoid(2z)?1 本质是落在[-1,1]上，它相当于机器学习的标准化。所以幅度没有sigmod这么大。
        我使用它的原则是: 在设计神经网络时，某部分神经元层数比较少，最后用sigmod返回，某部分神经元层数比较多，用tanh返回，当然具体
        还要根据这个神经元层数以及我们的数据量而定。
     
    (5) PReLU: ReLu的变种。
        这个要根据情况使用，如果测试时发现梯度变化幅度不是那么明显，可以尝试使用，如果梯度变化明显，还是用ReLu比较好。
        我们在这个项目里，也设计了PReLu:
        def leakReLu(x,leak = 0.2,name = "leakReLu"):
          if x is None:
            raise ValueError("输入的矩阵不能为空!")
          return tf.maximum(x,x*leak)
      
        特点不是直接变为0，而是进行一定幅度缩小。
        也就是max(ai.xi,xi)有点像正则。
       
    4.正则化
      (1)L1,L2正则
        J(W,b)=12m∑i=1m||aL?y||22+λ2m∑l=2L||w||22 整体来说还是跟机器学习差不多，作用也一样。
        在反向传播更新时，通常前面加α。这样不是1/m表示，而是α/m 不影响结果
      (2) batch_norm.：
          对整体batch进行正则，我在GAN中使用了。
           return tf.contrib.layers.batch_norm(x, decay=self.decay, 
           updates_collections=None, scale=True,epsilon=self.epsilon,
           zero_debias_moving_mean = self.zero_debias_moving_mean, is_training=train, scope=self.scope)
           具体可看官网解释。
       (3)集成学习
          用Bagging思想正则。也就是加权投票。我们对m个训练样本进行有放回的随机采样，构建N组m个样本数据集，然后分别用这N组数据集去训练
          DNN,即采用我们的前向传播算法和反向传播算法得到N个DNN模型的W,b参数组合，最后对N个DNN模型输出用加权平均法求出。
          一般N设置不要太多，否则太浪费时间和空间。 我们这次图像分类最后也就是用bagging思想最后融合得到的理想结果，但十分消耗性能。
          
        (4)dropput:这个懒得说了，原理就是隐藏一部分神经元，但使其得到的结果不变，这样间接的增强了神经元参数w和b的学习能力，强制性提高效果。
            隐藏不是永远消失，再下一次数据迭代前，会把DNN模型恢复到原来的全连接模型，再用随机方法处理掉隐藏层的神经元，然后迭代更新W和b.
        
        (5)dropout和bagging分析:
           dropout:每次梯度下降迭代时，需要训练数据分成若干(也就是Epoch)，然后分批迭代（每次是一个batch），每批数据迭代。此时隐藏掉部分神经元，用
           剩下残缺的DNN模型更新W和b,更新后，将残缺的DNN模型恢复成原始的DNN模型。恢复时机就是下次迭代前，我们最终目的就是增强神经元学习能力，
           增强w和b效果
           dropout与bagging区别: dropout中W和b是一套共享，所以残缺的DNN迭代时，更新的是同一组W,b。
           而Bagging是每个DNN独有W,b参数。相互独立。模型融合。
           所以我们的原始数据分批迭代是基于我们使用了dropout/.
        
       (6)DNN增强数据集正则化。
          增强模型泛化能力最好的办法是有更多更多的训练数据，但是在实际应用中，更多的训练数据往往很难得到。
          有时候我们不得不去自己想办法能无中生有，来增加训练数据集，进而得到让模型泛化能力更强的目的。

　　　　   对于我们传统的机器学习分类回归方法，增强数据集还是很难的。你无中生有出一组特征输入，却很难知道对应的特征输出是什么。
          但是对于DNN擅长的领域，比如图像识别，语音识别等则是有办法的。以图像识别领域为例，对于原始的数据集中的图像，
          我们可以将原始图像稍微的平移或者旋转一点点，则得到了一个新的图像。虽然这是一个新的图像，即样本的特征是新的，
          但是我们知道对应的特征输出和之前未平移旋转的图像是一样的。
          
          图像领域通常的方法有:
          a.修改图片尺寸（resize）
          image：需要改变尺寸的图片
          output_shape：输出图片的尺寸（height，weight）
          返回resize之后的图片
          img = imread('car.jpg')
          resized_image = resize(img,(1024,1280))
          imshow(resized_image)
          
          b.按比例缩放（rescale）
          cale：可以是单个的float数，表示缩放的倍数，也可以是一个float型的tuple，如[0.6,0.5]，
          表示将height和weight分别缩放为原来的0.6倍和0.5倍。
          
          img = imread('car.jpg')
          rescaled_img = rescale(img,[0.6,0.5])
          imshow(rescaled_img)
          
          c.加噪（noise）
            import numpy as np
            height,weight,channel = img.shape
            
            #随机生成5000个椒盐噪声
            for i in range(5000):
                x = np.random.randint(0,height)
                y = np.random.randint(0,weight)
                img[x ,y ,:] = 255
            
            imshow(img)
            
            d.反转（flip）
              import numpy as np
              from skimage.io import imread,imshow
              img = imread('car.jpg')
              vertical_flip = img[::-1,:,:]
              imshow(vertical_flip)
              
            e.旋转（rotate）
              angle：按照逆时针方向旋转的角度
              resize：旋转角度时是否改变图片尺寸
              center：旋转中心，默认中心为center=(heighr / 2 - 0.5, weight / 2 - 0.5)
              from skimage.transform import rotate
              img = imread('car.jpg')
              rotate_img = rotate(img,30)#逆时针旋转30°
              imshow(rotate_img)
              如果黑色点太多：可以这样:
              from keras.preprocessing import image

              def rotate(x, theta, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest', cval=0.):
              rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],
              [np.sin(theta), np.cos(theta), 0],
              [0, 0, 1]])
              h, w = x.shape[row_axis], x.shape[col_axis]
              transform_matrix = image.transform_matrix_offset_center(rotation_matrix, h, w)
              x = image.apply_transform(x, transform_matrix, channel_axis, fill_mode, cval)
              return x
              
              rotate_limit=(-30, 30)
              theta = np.pi / 180 * np.random.uniform(rotate_limit[0], rotate_limit[1]) #逆时针旋转角度
              #rotate_limit= 30 #自定义旋转角度
              #theta = np.pi /180 *rotate_limit #将其转换为PI
              img_rot = rotate(img, theta)
              imshow(img_rot)
              
            f.平移（shift）
            g.缩放变换（zoom）
            h.剪切（shear）
            i.对比度变换（contrast）
            j.随机通道偏移（channel shift）
              def random_channel_shift(x, intensity, channel_index=0):
                 x = np.rollaxis(x, channel_index, 0)
                 min_x, max_x = np.min(x), np.max(x)
                 channel_images = [np.clip(x_channel + np.random.uniform(-intensity, intensity), min_x, max_x)
                                   for x_channel in x]
                 x = np.stack(channel_images, axis=0)
                 x = np.rollaxis(x, 0, channel_index+1)
                 return x
              img_chsh = random_channel_shift(img, intensity = 0.05)
              
              imshow(img_chsh)
            k.PCA
              l,v,m = RGB_PCA(img)
              img = RGB_variations(img,l,v)
              imshow(img
  
  5.优化器
      (1)SD G随机梯度下降
      随机梯度下降（Stochastic gradient descent，SGD）对每个训练样本进行参数更新，每次执行都进行一次更新，且执行速度更快。
      (2)SGD 小批量梯度下降
        为了避免SGD和标准梯度下降中存在的问题，一个改进方法为小批量梯度下降（Mini Batch Gradient Descent），因为对每个批次中的n个训练样本，这种方法只执行一次更新。

        使用小批量梯度下降的优点是：
        
        1) 可以减少参数更新的波动，最终得到效果更好和更稳定的收敛。
        
        2) 还可以使用最新的深层学习库中通用的矩阵优化方法，使计算小批量数据的梯度更加高效。
        
        3) 通常来说，小批量样本的大小范围是从50到256，可以根据实际问题而有所不同。
        
        4) 在训练神经网络时，通常都会选择小批量梯度下降算法。
        
        这种方法有时候还是被成为SGD。
  

      使用梯度下降及其变体时面临的挑战
      
      1. 很难选择出合适的学习率。太小的学习率会导致网络收敛过于缓慢，而学习率太大可能会影响收敛，并导致损失函数在最小值上波动，甚至出现梯度发散。
      
      2. 此外，相同的学习率并不适用于所有的参数更新。如果训练集数据很稀疏，且特征频率非常不同，则不应该将其全部更新到相同的程度，但是对于很少出现的特征，应使用更大的更新率。
      
      3. 在神经网络中，最小化非凸误差函数的另一个关键挑战是避免陷于多个其他局部最小值中。实际上，问题并非源于局部极小值，而是来自鞍点，即一个维度向上倾斜且另一维度向下倾斜的点。这些鞍点通常被相同误差值的平面所包围，这使得SGD算法很难脱离出来，因为梯度在所有维度上接近于零。
      
      进一步优化梯度下降
      
      现在我们要讨论用于进一步优化梯度下降的各种算法。
      
      (3). 动量
      
      SGD方法中的高方差振荡使得网络很难稳定收敛，所以有研究者提出了一种称为动量（Momentum）的技术，通过优化相关方向的训练和弱化无关方向的振荡，来加速SGD训练。换句话说，这种新方法将上个步骤中更新向量的分量’γ’添加到当前更新向量。
      
      V(t)=γV(t?1)+η?(θ).J(θ)
      
      最后通过θ=θ?V(t)来更新参数。
      
      动量项γ通常设定为0.9，或相近的某个值。
      
      这里的动量与经典物理学中的动量是一致的，就像从山上投出一个球，在下落过程中收集动量，小球的速度不断增加。
      
      在参数更新过程中，其原理类似：
      
      1) 使网络能更优和更稳定的收敛；
      
      2) 减少振荡过程。
      
      当其梯度指向实际移动方向时，动量项γ增大；当梯度与实际移动方向相反时，γ减小。这种方式意味着动量项只对相关样本进行参数更新，减少了不必要的参数更新，从而得到更快且稳定的收敛，也减少了振荡过程。
      
      (4). Nesterov梯度加速法
      
      一位名叫Yurii Nesterov研究员，认为动量方法存在一个问题：
      
      如果一个滚下山坡的球，盲目沿着斜坡下滑，这是非常不合适的。一个更聪明的球应该要注意到它将要去哪，因此在上坡再次向上倾斜时小球应该进行减速。
      
      实际上，当小球达到曲线上的最低点时，动量相当高。由于高动量可能会导致其完全地错过最小值，因此小球不知道何时进行减速，故继续向上移动。
      
      Yurii Nesterov在1983年发表了一篇关于解决动量问题的论文，因此，我们把这种方法叫做Nestrov梯度加速法。
      
      在该方法中，他提出先根据之前的动量进行大步跳跃，然后计算梯度进行校正，从而实现参数更新。这种预更新方法能防止大幅振荡，不会错过最小值，并对参数更新更加敏感。
      
      Nesterov梯度加速法（NAG）是一种赋予了动量项预知能力的方法，通过使用动量项γV(t?1)来更改参数θ。通过计算θ?γV(t?1)，得到下一位置的参数近似值，这里的参数是一个粗略的概念。因此，我们不是通过计算当前参数θ的梯度值，而是通过相关参数的大致未来位置，来有效地预知未来：
      
      V(t)=γV(t?1)+η?(θ)J( θ?γV(t?1) )，然后使用θ=θ?V(t)来更新参数。
      
      现在，我们通过使网络更新与误差函数的斜率相适应，并依次加速SGD，也可根据每个参数的重要性来调整和更新对应参数，以执行更大或更小的更新幅度。
      
      (5). Adagrad方法
      
      Adagrad方法是通过参数来调整合适的学习率η，对稀疏参数进行大幅更新和对频繁参数进行小幅更新。因此，Adagrad方法非常适合处理稀疏数据。
      
      在时间步长中，Adagrad方法基于每个参数计算的过往梯度，为不同参数θ设置不同的学习率。
      
      先前，每个参数θ(i)使用相同的学习率，每次会对所有参数θ进行更新。在每个时间步t中，Adagrad方法为每个参数θ选取不同的学习率，更新对应参数，然后进行向量化。为了简单起见，我们把在t时刻参数θ(i)的损失函数梯度设为g(t,i)。
      
      
      图3：参数更新公式
      Adagrad方法是在每个时间步中，根据过往已计算的参数梯度，来为每个参数θ(i)修改对应的学习率η。
      
      Adagrad方法的主要好处是，不需要手工来调整学习率。大多数参数使用了默认值0.01，且保持不变。
      
      Adagrad方法的主要缺点是，学习率η总是在降低和衰减。
      
      因为每个附加项都是正的，在分母中累积了多个平方梯度值，故累积的总和在训练期间保持增长。这反过来又导致学习率下降，变为很小数量级的数字，该模型完全停止学习，停止获取新的额外知识。
      
      因为随着学习速度的越来越小，模型的学习能力迅速降低，而且收敛速度非常慢，需要很长的训练和学习，即学习速度降低。
      
      另一个叫做Adadelta的算法改善了这个学习率不断衰减的问题。
      
      (6). AdaDelta方法
      
      这是一个AdaGrad的延伸方法，它倾向于解决其学习率衰减的问题。Adadelta不是累积所有之前的平方梯度，而是将累积之前梯度的窗口限制到某个固定大小w。
      
      与之前无效地存储w先前的平方梯度不同，梯度的和被递归地定义为所有先前平方梯度的衰减平均值。作为与动量项相似的分数γ，在t时刻的滑动平均值Eg2仅仅取决于先前的平均值和当前梯度值。
      
      Eg2=γ.Eg2+(1?γ).g2(t)，其中γ设置为与动量项相近的值，约为0.9。
      
      Δθ(t)=?η?g(t,i).
      
      θ(t+1)=θ(t)+Δθ(t)
      
      
      图4：参数更新的最终公式
      AdaDelta方法的另一个优点是，已经不需要设置一个默认的学习率。
      
      目前已完成的改进
      
      1) 为每个参数计算出不同学习率；
      
      2) 也计算了动量项momentum；
      
      3) 防止学习率衰减或梯度消失等问题的出现。
      
      还可以做什么改进？
      
      在之前的方法中计算了每个参数的对应学习率，但是为什么不计算每个参数的对应动量变化并独立存储呢？这就是Adam算法提出的改良点。
      
      (7)Adam算法
      
      Adam算法即自适应时刻估计方法（Adaptive Moment Estimation），能计算每个参数的自适应学习率。这个方法不仅存储了AdaDelta先前平方梯度的指数衰减平均值，而且保持了先前梯度M(t)的指数衰减平均值，这一点与动量类似：
      
      M(t)为梯度的第一时刻平均值，V(t)为梯度的第二时刻非中心方差值。
      
      
      图5：两个公式分别为梯度的第一个时刻平均值和第二个时刻方差
      则参数更新的最终公式为：
      
      
      图6：参数更新的最终公式
      其中，β1设为0.9，β2设为0.9999，?设为10-8。
      
      在实际应用中，Adam方法效果良好。与其他自适应学习率算法相比，其收敛速度更快，学习效果更为有效，而且可以纠正其他优化技术中存在的问题，如学习率消失、收敛过慢或是高方差的参数更新导致损失函数波动较大等问题。
      
      对优化算法进行可视化
      
      
      图8：对鞍点进行SGD优化
      从上面的动画可以看出，自适应算法能很快收敛，并快速找到参数更新中正确的目标方向；而标准的SGD、NAG和动量项等方法收敛缓慢，且很难找到正确的方向。
      
      总结:
      Adam在实际应用中效果良好，超过了其他的自适应技术。

      如果输入数据集比较稀疏，SGD、NAG和动量项等方法可能效果不好。因此对于稀疏数据集，应该使用某种自适应学习率的方法，
      且另一好处为不需要人为调整学习率，使用默认参数就可能获得最优值。

      如果想使训练深层网络模型快速收敛或所构建的神经网络较为复杂，则应该使用Adam或其他自适应学习速率的方法，
      因为这些方法的实际效果更优。
       
       
       
6.最后。说下我在深度学习注意事项，和基础知识总结:
1.训练时drop-out为小于等于1=》假如是0.5，训练时每个神经元50%会死掉。最终50%的神经元效果通过迭代也达到了最终效果，所以间接的增加了单个神经元的能力(鲁棒性)。这个死掉是假死，下次有可能是另一批去训练。不仅增加鲁棒性还能提高准确率。
测试时drop-out必须为1=》全连接。

drop-out:论文: http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf

2.反向传播为什么用链式求导，为了让其更新w,所以才用链式求导规则，正好中间用out01层和net层可以到w,所以这样写。
存在问题1：假设不可导该怎么办? 
不可导的条件是激活函数不连续，所以我们的目的是取激活函数连续即可。所以我们之前就强制将激活函数连续，就肯定让他可导。

存在问题2: 梯度消失-》relu函数取代sigmod.relu是max(0,x) =>这个结果至少为0.而且大于0的地方都是连续的，所以肯定是可导的。

存在问题3: 越学越差。设置停止条件，除了迭代次数和损失值准确率外。还可以加上，迭代100次时，精度不涨反而降低，必须停止。

存在问题4: 调节△w的学习率主要看准确率的变化情况，如果波浪性大，就调小点。一般0.001

存在问题5:为什么w-△w。为什么是减。
因为是求极值点，都是往最小点移动，就像一个抛物线，都要向最小点移动。这个过程肯定是梯度下降(无论是y轴左边还是右边都是下降的。)

存在问题6: 先前向传播(bp),再反向传播(fp)。这样一个轮回才是对w更新一次。

存在问题7:深度学习本质是特征提取，特征提取是存放在w和b中，然后得到一个目标结果。也就是特征是存在于w和b中。

存在问题8: 为什么PCA去燥: PCA本质是矩阵分解,选取90%的信息代表图像数据，剩下10%就是噪音。所以有个很强的假设性，不一定完全准确。需要根据业务场景使用这个。还有就是降低数据的相关性，使得数据更独立。做数据挖掘预估时可以删除掉相关性很高的数据。
以下是详细解释:
这个可以这样理解，比如x1,x2,x3,相关性高的话，那么这三个数中任意一个数都可以用另外两个表示，那么第三个数的存在就没有意义了
实际上对应的就是线性相关，其中的某一数都可以用其他数表示出来

存在问题9: 白化的作用
D:\aconda;D:\aconda\Scripts;D:\aconda\Library\bin;
存在问题10: 
数据增强: http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf

存在问题11： 局部响应归一化
是让神经元中影响力大的更大，同时抑制影响力小的神经元
目的就是找到最好的特征

存在问题12: 激活层(relu/sigmod)作用:增加非线性。类似变为多项式。通俗说就是该神经元层的每个神经元乘以的那个函数就是激活层这个函数。本质是分段线性函数,在多维空间任何一个曲面可以分解为多段平面。
在全连接层中的激活过程就很好理解了，因为全连接层内所有的神经元的输出都是一个数，只要这个数x>0，则x=x；x<0，则x=0。 
在卷积层中的激活针对的是每一个像素值，比如某卷积层输出中某个通道中i行j列像素值为x，只要这个数x>0，则x=x；x<0，则x=0。

存在问题13: ALnext第一个全连接层另外一个作用:
CNN中的全连接层与浅层神经网络中的作用是一样的，负责逻辑推断，所有的参数都需要学习得到。有一点区别在于第一层的全连接层用于链接卷积层的输出，它还有一个作用是去除空间信息（通道数），是一种将三维矩阵变成向量的过程（一种全卷积操作）,原理是降维映射成向量。其实就是拉平操作。

存在问题14:为什么机器学习，推荐系统，深度学习(模型更新时间:10-15天)能离线就离线处理，实在不行在线处理。
因为离线训练完后，将模型参数存储在数据库中，用户点击时基于模型结果产生一个实时的推荐结果，然后从数据库中获取相应的模型参数，进行模型的公式的计算得出结果，所以仅仅是存储模型参数和计算最后结果，不用再创建模型，很省内存时间。而在线需要当场构建模型计算，相当费内存时间。

存在问题15:
Alex第三层，卷积了2遍，类似全连接，卷积到俩个显卡(俩个通道上)。导致通道=下层深度。为什么这样做?

未解决。
       
       
       
    
    
    
    
