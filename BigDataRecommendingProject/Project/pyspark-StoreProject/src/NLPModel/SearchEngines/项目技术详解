1.TfidfVectorizer
具体请看skt-learn官网。
将原始文档集合转换为TF-IDF特征矩阵。相当于CountVectorizer，后跟TfidfTransformer。

参数:我只列我常用的。其他具体看skt-learn官网
(1)stop_words 停止词，正常写 English 直接可以。另外还有中文格式是传输列表： stop_words = ["是","的"] 
(2)ngram_range：要提取的不同 n-gram 的n值范围的下边界和上边界。将使用n的所有值，使得min_n <= n <= max_n。
n-gram是一种基于HMM的语言模型，你可以理解为，计算当前词的概率，除了考虑自身，还要考虑前面，但后面的词汇不会对当前词产生影响。
也就是 W|Wn-1*Wn-2*Wn-3......

2.TruncatedSVD
具体看 https://blog.csdn.net/qq_24464989/article/details/79834564
说到这个，我想到了以前做kaggle比赛的实验。TSVD更让数据趋近于0，而PCA更容易产生噪点，当时是我的实验结论。当然它俩本质也不同，一个是降维后离散，使其独立性。
一个是找到权值最大的K个点，然后做矩阵分解。M*N = M*R R*R R*N 类似这个公式，
我总结下它与PCA区别以及用法;
PCA降维，需要找到样本协方差矩阵X˜TX˜的最大的k个特征向量，然后用这最大的k个特征向量张成的矩阵来做低维投影降维。 
注意，SVD也可以得到协方差矩阵X˜TX˜最大的k个特征向量张成的矩阵。 
但是SVD有个好处，有一些SVD的实现算法可以不求先求出协方差矩阵，也能求出右奇异矩阵V。也就是说，PCA算法可以不
用做特征分解，而是做SVD来完成。这也是为什么很多工具包中PCA算法的背后真正的实现是用的SVD，而不是我们认为的暴力特征分解。 
另一方面，注意到PCA仅仅使用了我们SVD的右奇异矩阵，没有使用左奇异矩阵。而左奇异矩阵可以用于行数的压缩，右奇异矩阵可以用于列数，也就是PCA降维。
所以，有了SVD就可以得到两个方向的PCA。
所以，PCA的背后就是SVD,TSVD机制也是SVD，而PCA主要用于相关性强的且接近的高维矩阵，为了离散强相关，使其独立。
而SVD更适合处理大量稀疏矩阵。

3.管道机制-流水线处理(Pipline makepipline)
Pipline；最后必须用拟合器完成，makepipline是生成一个numpy.addary.所以主要用在特征处理上。
流水线机制:
除最后一个工作以外，其他都要执行fit_transform(就是有fit和transform)方法，且上一个工作输出作为下一个工作的输入。最后一个工作必须实
现fit方法，输入为上一个工作的输出；但是不限定一定有transform方法，因为流水线的最后一个工作可能是训练！

4.管道机制-并行处理(FeatureUnion)
并行处理使得多个特征处理工作能够并行地进行。根据对特征矩阵的读取方式不同，可分为整体并行处理和部分并行处理。

整体并行处理，即并行处理的每个工作的输入都是特征矩阵的整体；

部分并行处理，即可定义每个工作需要输入的特征矩阵的列。在一些场景下，我们只需要
对特征矩阵的某些列进行转换，而不是所有列。pipeline并没有提供相应的类（仅OneHotEncoder类实现了该功能），需要我们在FeatureUnion的基础上进行优化：

5.管道机制的举例以及本次项目的使用说明。
(1)整体并行案例:
from numpy import log1p
from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import Binarizer
from sklearn.pipeline import FeatureUnion

#新建将整体特征矩阵进行对数函数转换的对象
step2_1 = ('ToLog', FunctionTransformer(log1p))
#新建将整体特征矩阵进行二值化类的对象
step2_2 = ('ToBinary', Binarizer())
#新建整体并行处理对象
#该对象也有fit和transform方法，fit和transform方法均是并行地调用需要并行处理的对象的fit和transform方法
#参数transformer_list为需要并行处理的对象列表，该列表为二元组列表，第一元为对象的名称，第二元为对象
step2 = ('FeatureUnion', FeatureUnion(transformer_list=[step2_1, step2_2, step2_3]))

(2)部分并行处理案例
from sklearn.pipeline import FeatureUnion, _fit_one_transformer, _fit_transform_one, _transform_one 
from sklearn.externals.joblib import Parallel, delayed
from scipy import sparse
import numpy as np

#部分并行处理，继承FeatureUnion
class FeatureUnionExt(FeatureUnion):
    #相比FeatureUnion，多了idx_list参数，其表示每个并行工作需要读取的特征矩阵的列
    def __init__(self, transformer_list, idx_list, n_jobs=1, transformer_weights=None):
        self.idx_list = idx_list
        FeatureUnion.__init__(self, transformer_list=map(lambda trans:(trans[0], trans[1]), transformer_list),
        n_jobs=n_jobs, transformer_weights=transformer_weights)

    #由于只部分读取特征矩阵，方法fit需要重构
    def fit(self, X, y=None):
        transformer_idx_list = map(lambda trans, idx:(trans[0], trans[1], idx), self.transformer_list, self.idx_list)
        transformers = Parallel(n_jobs=self.n_jobs)(
            #从特征矩阵中提取部分输入fit方法
            delayed(_fit_one_transformer)(trans, X[:,idx], y)
            for name, trans, idx in transformer_idx_list)
        self._update_transformer_list(transformers)
        return self

    #由于只部分读取特征矩阵，方法fit_transform需要重构
    def fit_transform(self, X, y=None, **fit_params):
        transformer_idx_list = map(lambda trans, idx:(trans[0], trans[1], idx), self.transformer_list, self.idx_list)
        result = Parallel(n_jobs=self.n_jobs)(
            #从特征矩阵中提取部分输入fit_transform方法
            delayed(_fit_transform_one)(trans, name, X[:,idx], y,
                                        self.transformer_weights, **fit_params)
            for name, trans, idx in transformer_idx_list)

        Xs, transformers = zip(*result)
        self._update_transformer_list(transformers)
        if any(sparse.issparse(f) for f in Xs):
            Xs = sparse.hstack(Xs).tocsr()
        else:
            Xs = np.hstack(Xs)
        return Xs

    #由于只部分读取特征矩阵，方法transform需要重构
    def transform(self, X):
        transformer_idx_list = map(lambda trans, idx:(trans[0], trans[1], idx), self.transformer_list, self.idx_list)
        Xs = Parallel(n_jobs=self.n_jobs)(
            #从特征矩阵中提取部分输入transform方法
            delayed(_transform_one)(trans, name, X[:,idx], self.transformer_weights)
            for name, trans, idx in transformer_idx_list)
        if any(sparse.issparse(f) for f in Xs):
            Xs = sparse.hstack(Xs).tocsr()
        else:
            Xs = np.hstack(Xs)
        return Xs

我们对特征矩阵的第1列（花的颜色）进行定性特征编码，对第2、3、4列进行对数
函数转换，对第5列进行定量特征二值化处理。使用FeatureUnionExt类进行部分并行处理的代码如下：


from numpy import log1p
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import Binarizer

#新建将部分特征矩阵进行定性特征编码的对象
step2_1 = ('OneHotEncoder', OneHotEncoder(sparse=False))
#新建将部分特征矩阵进行对数函数转换的对象
step2_2 = ('ToLog', FunctionTransformer(log1p))
#新建将部分特征矩阵进行二值化类的对象
step2_3 = ('ToBinary', Binarizer())
#新建部分并行处理对象
#参数transformer_list为需要并行处理的对象列表，该列表为二元组列表，第一元为对象的名称，第二元为对象
#参数idx_list为相应的需要读取的特征矩阵的列
step2 = ('FeatureUnionExt', FeatureUnionExt(transformer_list=[step2_1, step2_2, step2_3], idx_list=[[0], [1, 2, 3], [4]]))

(3)流水线处理
from numpy import log1p
from sklearn.preprocessing import Imputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import Binarizer
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

#新建计算缺失值的对象
step1 = ('Imputer', Imputer())
#新建将部分特征矩阵进行定性特征编码的对象
step2_1 = ('OneHotEncoder', OneHotEncoder(sparse=False))
#新建将部分特征矩阵进行对数函数转换的对象
step2_2 = ('ToLog', FunctionTransformer(log1p))
#新建将部分特征矩阵进行二值化类的对象
step2_3 = ('ToBinary', Binarizer())
#新建部分并行处理对象，返回值为每个并行工作的输出的合并
step2 = ('FeatureUnionExt', FeatureUnionExt(transformer_list=[step2_1, step2_2, step2_3], idx_list=[[0], [1, 2, 3], [4]]))
#新建无量纲化对象
step3 = ('MinMaxScaler', MinMaxScaler())
#新建卡方校验选择特征的对象
step4 = ('SelectKBest', SelectKBest(chi2, k=3))
#新建PCA降维的对象
step5 = ('PCA', PCA(n_components=2))
#新建逻辑回归的对象，其为待训练的模型作为流水线的最后一步
step6 = ('LogisticRegression', LogisticRegression(penalty='l2'))
#新建流水线处理对象
#参数steps为需要流水线处理的对象列表，该列表为二元组列表，第一元为对象的名称，第二元为对象
pipeline = Pipeline(steps=[step1, step2, step3, step4, step5, step6])

(4)本次案例讲解
clf = pipeline.Pipeline([
    ('union', FeatureUnion(
        # transformer_list 要应用于数据的变换器对象的列表。每个元组的前半部分是变换器的名称。 下面四个管道开始处理四个特征，都用tf计算相似性，再用tsvd进行矩阵降维。
        transformer_list=[
            ('cst', cust_regression_vals()),  # 确保特征拼接同一，这里先删除特恒。
            ('txt1', pipeline.Pipeline([('s1', cust_txt_col(key='search_term')), ('tfidf1', tfidf), ('tsvd1', tsvd)])),
            (
            'txt2', pipeline.Pipeline([('s2', cust_txt_col(key='product_title')), ('tfidf2', tfidf), ('tsvd2', tsvd)])),
            ('txt3',
             pipeline.Pipeline([('s3', cust_txt_col(key='product_description')), ('tfidf3', tfidf), ('tsvd3', tsvd)])),
            ('txt4', pipeline.Pipeline([('s4', cust_txt_col(key='brand')), ('tfidf4', tfidf), ('tsvd4', tsvd)]))
        ],
        # 每个变压器的特征的乘法权重。键是变换器名称，值是权重。
        transformer_weights={
            'cst': 1.0,
            'txt1': 0.5,
            'txt2': 0.25,
            'txt3': 0.0,
            'txt4': 0.5
        },
        n_jobs = 4
    )),
    ('xgb_model', xgb_model)])






