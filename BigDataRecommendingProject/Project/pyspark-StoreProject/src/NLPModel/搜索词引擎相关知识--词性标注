在做完聚类后，需要进行一些词性标注，标签处理。在这里，我简单介绍一下NLP常用的一些方法。

一.词编码
1.给每个词一个索引方便dict搜索，前提是需要确保词的相似性。
2.设计向量空间子结构 : 比如男人与女人的词向量关系 = 国王与皇后。 最终目标：词向量表示作为机器学习、特别是深度学
习的输入和表示空间 

二.词编码的问题.
词字典可以帮助我们配合切词工具更有效的切词，但是他也存在一些问题，如下：
(1)不能分辨细节差距。
(2)需要大量人为操作。
(3)过于主观
(4)无法发现新词，无法自动化
(5)难以精确计算词之间的相似度： 漂亮 好看。

解决方法：
(1)设计语料库
(2)设计词典，给下标，假设是10个词，再One-HOt编码时就是10个长度，然后相应下标就是1，其他都是0
(3)为了解决稀疏问题：做频次统计，这样先考虑到比较重要的词： [1,2,1,1,10,0,0,1,1]
(4)词权重(先去掉停用词):词袋法-> TF-IDF / Binary weighting(词权重)
(5)基于(4)我们必须要考虑词向量的顺序：Bi-gram (二元语言模型) N-gram（N元语言模型）
做法是为2-gram建立索引。依靠索引来排序，这样考虑词的顺序，使其更准确，缺点是引起词表的膨胀。
下面表是模型参数数量和n的关系. n是几元的意思。

n     模型参数数量
1     2 * 10^5
2     4 * 10^10             
3     8 * 10^15
4     16 * 10^20

这就是语言模型，你可以理解为相邻的词之间有关系，一个词通过它之前所有词的关联来推测出来。

产生问题: 
(1)增长膨胀 数据稀疏问题。
(2)但是同义词，以及地方语言的不同说法表达同一意思，这类问题无法表示其相近度。

再解决问题:
1.分布式来表示传统的说法。
比如一个语料中
有20个颜色，3个星号，30个车型。
所以需要记忆的单元数: 20 * 3 *30

如果用分布式:
存储单元数： 20+3+30
原理:用一个词附近的其他词表示该词。
具体做法:
(1)共现矩阵
共现矩阵主要用于发现主题模型 topic,用于主题模型 如 LSA。
需要一个局域窗口，也就是窗口就是预测词周边词的范围，一般是5-10.使用对称的窗函数。

共现矩阵里的值是单词同时出现的频数。横坐标和纵坐标是相同的，就是句子本身的词。假如，我们的
左右窗口为1，比如 句子中都有俩个 I like 那么在like的位置都写2，表示like与前面的I共同出现2次，
我们就叫他共现了2次。
共现矩阵的每一行和每一列就是对里面向量的一种表示，就类似预测你的收入，把你朋友收入拿过来取均值。

共现矩阵问题：
向量维数随着词典大小线性增长 
• 存储整个词典的空间消耗非常大 
• 一些模型如文本分类模型会面临稀疏性问题 
模型会欠稳定  ： 每次新加词汇，然后再加维度太高的原因。
构造低维稠密向量作为词的分布式表示 (25~1000维)！ 降维，一般用SVD。

最直接的想法：
(1)用SVD对共现矩阵向量做降维
原理就是 m * n = m* r r*r r*n

SVD问题：
（1）计算量随语料库和词典增长膨胀太快，对X(n,n)维
的矩阵，计算量O(n^3)。 而对大型的语料库，
n~400k，语料库大小1~60B token 
（2）难以为词典中新加入的词分配词向量 
（3）与其他深度学习模型框架差异大

(2)NNLM
NNLM (Neural Network Language model) 
直接从语言模型出发，将模型最优化过程转化为求词向量表示的过程
目标函数 ： L（θ） = ∑logP(Wt | Wt-n+1,Wt-1)
Ø 使用了非对称的前向窗函数，窗长度为n-1 。 也就是前面的n-1个词去预测下一个词。
Ø 滑动窗口遍历整个语料库求和，计算量正比于语料库大小 
Ø 概率P满足归一化条件，这样不同位置t处的概率才能相加，即 
∑P(W|Wt-n+1,...Wt-1) = 1
 (N-1)个前向词:one-hot表示
• 采用线性映射将one-hot表
示投影到稠密D维表示 这个稠密矩阵就是我们要学出来的。
• 输出层:Softmax 
• 各层权重最优化:BP＋SGD 

NNLM神经网络解释: 
(1)输入数据: 1*高维的向量，只有1个是1，其他都是0

(2)开始会被一个索引矩阵去做乘法，也就是 Table look-up。
如果你的词向量是500维，那么它就是8万 * 500维。 但是我们投影后，只取1行或1列。
这样结果就是1*500维，

(3)所以本质就是通过索引矩阵选出一行/一列。这就是投影。

(4)然后放到一个隐层，做拼接(假如10个词预测下个词，在我们这里就是10*500，就是拼接5000维度)

(5)最后全连接，最后softmax处理。

NNLM 计算复杂度：N*D + N*D*H+H*V
N是词的数量。D是神经元个数。H是神经元层数。
缺点:需要大量的数据和计算。如果数据量少，不太好。

（3）word2vec(在NNLM基础上的升级)
我们的目的是将数据模型变成有监督的学习。
无隐层 (无hidden了)
使用双向上下窗口
上下文词序无关（BoW）
输入层直接使用低维稠密表示
投影层简化为求和(平均)

word2vec的本质
也就是目标函数 : 遍历所有的词，通过上下文的关系，预测我们要预测的词汇的概率。
问题:softmax相当于一个全连接，如果是40万维，消耗量大。
后面的FastText就是在softmax改为树的结构，降低为logN。本质权重共享一个树的权重，然后像决策树那样，每次
选择左还是右。
我们最终是将原来word2vec的CBOW遍历所有节点求出最后概率改为遍历路径，然后将路径的所有点概率求和来求出答案。
类似最短路径的二叉树求法。
优化是求偏导，沿着梯度方式。

(4)除了FastText之外的优化方案--负例采样。
因为我们时稀疏，只有1个正样本，其他都是负样本。所以CBOW做出一个采样。使其降低负样本的数量。
本质是NEG负样本采样自己，但不一定采样是一致性。所以采取一个方法:
将所有数据弄成一个线段，然后划分为一个数量的等分。然后对所有词的权重求出概率，
比如根据频度来划分，频度大的，划分就多。最后是统计词频的3/4做的。

(6)Skip-Gram 反之，从自身词汇推出周边。

(7)embedding layer:嵌入层，稀疏——》稠密。
但他不是独立的。属于神经网络的一个层。

(8)Glove : https://blog.csdn.net/qq_38096703/article/details/80654345

总结 
❖ 离散表示
• One-hot representation, Bag Of Words Unigram语言模型
• N-gram词向量表示和语言模型
• Co-currence矩阵的行(列)向量作为词向量 
❖ 分布式连续表示
• Co-currence矩阵的SVD降维的低维词向量表示  (SVD时间复杂度是N^3)
• Word2Vec: Continuous Bag of Words Model 
• Word2Vec: Skip-Gram Model 

四.sense2vec
多义性： word2vec遇到的问题。
word2vec模型的问题在于词语的多义性。比如duck这个单词常见的含义有
水禽或者下蹲，但对于 word2vec 模型来说，它倾向于将所有概念做归一化
平滑处理，得到一个最终的表现形式。
我们结合上下文和标签，会有更好的处理结果











