一.解决问题:
1）首先，使用DQN网络来有效建模用户兴趣事件活动推荐的动态变化属性，DQN可以将短期回报和长期回报进行有效的模拟。
2）将用户对事件是否感兴趣作为一种新的反馈信息，可以进行动态的检测，然后再对比用户在感兴趣的前提下是否去参加这个事件活动的概率
3) 可以在线学习与更新。 http://www.algorithmdog.com/advance-to-normal1

二.项目流程简介。

1.思路：
用户和事件的信息根据当前的state不断的组合action，然后训练出的结果反馈给agent.也就是DQN的target值。
然后在里面进行DQN的target生成和数据探索，形成一系列策略，存放在记忆池中，这些策略得到的reward奖励值会返回给我们的起始点，用来判断
当前策略对结果造成的影响的好坏程度。最后我们要预测的target会从记忆池中取出一个最好的方案。

2.流程。
由于我们这个数据是基于用户历史行为的数据，所以需要把时间拆分，我们最后按照不同时间来模拟做。
(1)选出几天时间的数据，然后agent根据当前的state判断用户需要什么样的事件活动，进行推荐,这个结果需要探索
(2)用户针对这个事件得到一个反馈结果，其实就是比较这个AUC值，我们设置一个阈值，如果大于这个阈值多少就进行奖励加分。
(3)agent针对评估exploitation netword Q 和 exploration netword Q~的表现到底谁更好，exploitation network Q的参数会
向exploration netword Q~变化。
(4)根据以上策略申城，DQN的经验池会存放历史经验，然后对exploitation network Q模型参数进行更新。

3.数据分析
(1)事件的特征：events.csv 包含有关我们系统中事件的数据，并且有110列。前九栏是  event_id，user_id，start_time，city，state，zip，country
其中我们确定了100个最常见的词干（通过Porter Stemming获得）出现在我们事件的大型随机子集的名称或描述中。最后101列是count_1， count_2，...，
count_100， count_other，其中 count_N是一个整数，
示第N个最常用词干出现在该事件的名称或描述中的次数。  count_other 是其余词干不是100个最常见词干之一的词的数量。

(2)用户的特征： user_id，  locale，  birthyear，  gender，  joinedAt，  位置和  时区。 user_id是我们系统中用户的ID。
还有user_friends.csv包含有关此用户的社交数据，并包含两列：用户和朋友。  用户是我们系统中的用户ID， 朋友是用户朋友ID的空格分隔列表。

(3)用户对事件的感兴趣程度特征:含关于哪些用户参加了各种活动的信息，并且具有以下列：  event_id，是，可能，受邀以及否。event_id 标识事件。是的，也许， 邀请，
并没有为用户ID代表用户的空间分隔的名单谁表示，他们打算，也许会，邀请，或不打算事件。

(4)训练/测试的特征： 用户，事件， 邀请，时间戳，感兴趣和不感兴趣。主要就是结果，某用户对该事件是否感兴趣，是否会参加活动。
而timesamp是一个ISO-8601 UTC时间字符串，表示当用户在我们的应用程序中看到事件时的大致时间（+/- 2小时）。
有兴趣是一个二进制变量，用于指示用户是否单击此事件的“感兴趣”按钮; 如果用户点击兴趣，则为1，如果用户没有点击该按钮，则为0。
同样， not_interested是一个二进制变量，指示用户是否单击此事件的“不感兴趣”按钮; 如果用户点击该按钮则为1，否则为0。这可能是用户看到的事件，并点击既不感兴趣，也没有兴趣，因此存在包含0,0为值的行 感兴趣，not_interested。


在这四组特征中，
用户特征和训练/测试集的特征用于表示当前的state，
事件特征和用户对事件的感兴趣程度特征表示当前的一个action。
所以用户特征与train表示当前状态(state)，也就是是否参加的概率和敢不敢兴趣的概率(最后俩个特征)。 
而event的属性与eventted(用户对事件的评分) 来表示当前的action, 我们的目的是让这个action得到的结果不断让用户特征与train参加的概率提升。


三.参数介绍
Q值就是我们这次数据使用模型得到的Auc值。
这里深度强化学习用的是DDPG。
将user和user_friends特征和train上得到的是否参加的结果和是否感兴趣的结果用于表示当前的state。
event特征和eventted(用户对事件的评分)表示当前的一个action。
经过模型可以输出当前状态state采取这个action的预测Q值。
Q现实值包含两个部分：立即获得的奖励和未来获得奖励的折现：
Ys,a = Q(s,a) = Rpredition + γRfeature 也就是当前奖励+未来奖励。
当前奖励可能包含两部分，即用户参加的奖励+用户对这个事情感兴趣的奖励。
而这个奖励是通过一种概率值求得，这个概率值就是我们的神经网络MLP来实现。
DDPG是通过当前的state,action估算出估计的Q值，而下一时刻的状态state',动作action' 输入到状态现实网络而得到Q值的折现值加和得到。
所以我们的Q 为如下公式:
Ys,a,t = Ra+1,t+1 + γQ(Sa+1,t+1  ,  argmaxQ(Sa,t+1,a';Wt);W't) - Q(Sa,t)

我们的奖励值为 参加获得的奖励值*能参加的概率 + 对事件感兴趣的奖励值*对事件感兴趣的概率。
Rtotal = R1*W1+R2*W2

评估指标：AUC

