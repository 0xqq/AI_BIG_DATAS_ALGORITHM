一.解决问题:
1）首先，使用DQN网络来有效建模用户兴趣事件活动推荐的动态变化属性，DQN可以将短期回报和长期回报进行有效的模拟。
2）将用户对事件是否感兴趣作为一种新的反馈信息，可以进行动态的检测，然后再对比用户在感兴趣的前提下是否去参加这个事件活动的概率
3) 可以在线学习与更新。 http://www.algorithmdog.com/advance-to-normal1

二.项目流程简介。

1.思路：
用户和事件的信息根据当前的state不断的组合action，然后训练出的结果反馈给agent.也就是DQN的target值。
然后在里面进行DQN的target生成和数据探索，形成一系列策略，存放在记忆池中，这些策略得到的reward奖励值会返回给我们的起始点，用来判断
当前策略对结果造成的影响的好坏程度。最后我们要预测的target会从记忆池中取出一个最好的方案。

2.流程。
由于我们这个数据是基于用户历史行为的数据，所以需要把时间拆分，我们最后按照不同时间来模拟做。
(1)选出几天时间的数据，然后agent根据当前的state判断用户需要什么样的事件活动，进行推荐,这个结果需要探索
(2)用户针对这个事件得到一个反馈结果，其实就是比较这个AUC值，我们设置一个阈值，如果大于这个阈值多少就进行奖励加分。
(3)agent针对评估exploitation netword Q 和 exploration netword Q~的表现到底谁更好，exploitation network Q的参数会
向exploration netword Q~变化。
(4)根据以上策略申城，DQN的经验池会存放历史经验，然后对exploitation network Q模型参数进行更新。

3.数据分析
(1)事件的特征：events.csv 包含有关我们系统中事件的数据，并且有110列。前九栏是  event_id，user_id，start_time，city，state，zip，country
其中我们确定了100个最常见的词干（通过Porter Stemming获得）出现在我们事件的大型随机子集的名称或描述中。最后101列是count_1， count_2，...，
count_100， count_other，其中 count_N是一个整数，
示第N个最常用词干出现在该事件的名称或描述中的次数。  count_other 是其余词干不是100个最常见词干之一的词的数量。

(2)用户的特征： user_id，  locale，  birthyear，  gender，  joinedAt，  位置和  时区。 user_id是我们系统中用户的ID。
还有user_friends.csv包含有关此用户的社交数据，并包含两列：用户和朋友。  用户是我们系统中的用户ID， 朋友是用户朋友ID的空格分隔列表。

(3)用户对事件的感兴趣程度特征:含关于哪些用户参加了各种活动的信息，并且具有以下列：  event_id，是，可能，受邀以及否。event_id 标识事件。是的，也许， 邀请，
并没有为用户ID代表用户的空间分隔的名单谁表示，他们打算，也许会，邀请，或不打算事件。

(4)训练/测试的特征： 用户，事件， 邀请，时间戳，感兴趣和不感兴趣。主要就是结果，某用户对该事件是否感兴趣，是否会参加活动。
而timesamp是一个ISO-8601 UTC时间字符串，表示当用户在我们的应用程序中看到事件时的大致时间（+/- 2小时）。
有兴趣是一个二进制变量，用于指示用户是否单击此事件的“感兴趣”按钮; 如果用户点击兴趣，则为1，如果用户没有点击该按钮，则为0。
同样， not_interested是一个二进制变量，指示用户是否单击此事件的“不感兴趣”按钮; 如果用户点击该按钮则为1，否则为0。这可能是用户看到的事件，并点击既不感兴趣，也没有兴趣，因此存在包含0,0为值的行 感兴趣，not_interested。


在这四组特征中，用户特征和训练/测试集的特征用于表示当前的state，事件特征和用户对事件的感兴趣程度特征表示当前的一个action。
所以用户特征与train表示当前状态，也就是是否参加的概率和敢不敢兴趣的概率(最后俩个特征)。 
而event的属性与eventted(用户对事件的评分) 来表示当前的action, 我们的目的是让这个action得到的结果不断让用户特征与train参加的概率提升。


三.参数介绍
这里深度强化学习用的是Dueling-Double-DQN。之前我们介绍过DQN的三大改进，包括Double-DQN，Dueling-DQN和优先经验回放，这里用到了两个。将用户特征和上下文特征用于表示当前的state，新闻特征和交互特征用语表示当前的一个action，经过模型可以输出当前状态state采取这个action的预测Q值。

Q现实值包含两个部分：立即获得的奖励和未来获得奖励的折现：


立即的奖励可能包含两部分，即用户的点击奖励和用户活跃度奖励。由于采取了Double-DQN 的结构，Q现实值的计算变为：


再加上Dueling的考虑，模型的网络结构如下：


文章中关于DQN的理论部分没有详细介绍，可以参考我之前写过的强化学习系列的文章进行理解。

3.4 用户活跃度
用户活跃度（User Activeness） 是本文提出的新的可以用作推荐结果反馈的指标。用户活跃度可以理解为使用app的频率，好的推荐结果可以增加用户使用该app的频率，因此可以作为一个反馈指标。

用户活跃度的图示如下：


如果用户在一定时间内没有点击行为，活跃度会下降，但一旦有了点击行为，活跃度会上升。

在考虑了点击和活跃度之后，之前提到过的立即奖励变为：


3.5探索
本文的探索采取的是Dueling Bandit Gradient Descent 算法，算法的结构如下：


在DQN网络的基础上又多出来一个exploration network Q ̃ ，这个网络的参数是由当前的Q网络参数基础上加入一定的噪声产生的，具体来说：


当一个用户请求到来时，由两个网络同时产生top-K的新闻列表，然后将二者产生的新闻进行一定程度的混合，然后得到用户的反馈。如果exploration network Q ̃的效果好的话，那么当前Q网络的参数向着exploration network Q ̃的参数方向进行更新，具体公式如下：


否则的话，当前Q网络的参数不变。

总的来说，使用深度强化学习来进行推荐，同时考虑了用户活跃度和对多样性推荐的探索，可以说是一个很完备的推荐框架了！

五.评估指标
AUC

