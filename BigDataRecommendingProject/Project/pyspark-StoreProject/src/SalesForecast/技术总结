前言(可不看):
    我在整理完数据后，可以分成俩种方式来做模型处理。
    一种是不加特征构造的数据集，这种数据集只是拼接和离散时间序列，最后取出20维的强特征性进行构建。这种方式用可以用弹性网络去做回归处理，然后再用梯度下降。
    最后用XGB/LGB进行处理。这段代码我写在了草稿代码上。
    还有一种是加特征构造的数据集，这种适合用SVM中的SVR做回归处理，当然用集成学习 RF,XGB,LGB,CAT要更好。
    最后我都用了XGB处理，由于只用XGB进行了粗略的网格搜索，其他算法没有太细致调参，导致stacking/mean模型融合效果不如单XGB好。所以我这里就讲下我用的技术，和
    我设计算法的代码。至于项目和流程业务分析，请看项目流程与总结部分。
    
一技术讲解(这里不涉及业务分析)：

1.特征工程:
这里，我先不说明数据探索了，因为在下面内容中会有介绍，再加这组数据难度不大，不像CTR项目和银行项目需要大量数据探索。

首先，业务是回归问题，所以大致思路如下：
(1)加载数据:
如果是大量数据集，可用read_csv中其他函数进行业务分割，或者用文件格式分割，或者采用循环方式分割。但这种都是下采样方式。
如果不是业务允许，会影响最终结果，所以数据挖掘的大数据量最好还是高配置电脑处理更好。或者是大数据集群处理。我在加载数据时处理了一个异常特征，这是我在
做标准化时遇到的，后来看了这个特征里面内容有错误，既有数字又有字母，所以我当异常特征drop掉了。另外将目标值进行高斯分布，设置log1p

(2)离散时间序列:
有时间的业务，通常都要离散化，而离散的标准你要根据业务需求和人们的生活习惯。比如我们这个业务是销量和货物预测，销量受到时间的影响主要有俩个，一个是季节性，
一个是节日，最后是促销活动。所以我将所有促销活动月份分离出来，切记分离出的特征要drop掉。否则后面进行One-Hot处理时将全部离散化，你手动分离就没意义。
另外节日这里设置了隐藏数据，后面直接One-Hot让算法自己分离即可。另外再分离年月日即可。销量我们不需要按照上午，下午来分，因为没意义，都是以日为单位。

(3)数据处理:
对数值型数据进行标准化，原因有2：
a. 将数据统一化处理，单位不一样的容易造成噪点。所以统一化。数学公式: y = w1.x1+w2.x2+损失。 x1太大，x2太小，会让x2这个特征没啥用。
b. 数据平滑化。统计学中，我们通常尽量将数据呈现高斯分布处理，这样更方便处理。
标准化公式:X-mean/方差。 我直接用公式进行标准化，没使用skt-learn中的包。
c. 类别型数据(离散型)标准化，也就是连续数值化再标准归一化，机器学习的原则是一切元素最终都数值化，这样方便计算机读取。
具体做法:
先LabelEncoder标签编码:将类别也就是离散数据都用数字表示。 也就是 a,b,c 变成 001 002 003 这种格式。 

再One-Hot编码:简单说就是每个类别数据中的类别都变为连续型，假设时间有12个类别，One-Hot后就变成了12个特征维度。
我们的计算本质是计算距离(欧式)。如果不标准化，距离拉开太大，数据集噪点太多，标准化后成比例缩放，距离缩小。
但如果是类别超多的数据，希望还是自己手动去聚类，再离散吧。 另外也可以在离散中设置范围，比如CTR预估项目中我就设计了点击率范围大小，因为根据业务需求有的
类别没几个点击率可以删掉了。
有些树型算法，并不是基于向量空间度量，数值只是个类别符号，即没有偏序关系，所以不用进行独热编码。  
Tree Model不太需要one-hot编码： 对于决策树来说，one-hot的本质是增加树的深度。
通常One-Hot+PCA是个不错的组合，因为PCA本质是降低维度，将离散数据聚集到稠密数据集上。而One-Hot是让离散数据变成多个连续特征，这样容易造成大量的0出现。

不过我个人认为，如果想要精确的处理，有些地方要结合业务去One-Hot, PCA不一定就是很好的选择，毕竟是一种黑盒机制。人工结合业务需求处理才是更佳选择。

(4)特征选择，其实也是一种数据处理，一般就是处理方差，偏度，峰度，标准差这种差异大的数据。
通常来讲，如果方差越小，这个特征的特征性就越弱，我们往往可以设定阈值处理掉。skt-learn中有个 VarianceThreshold可以处理方差问题，我们俗称管他叫噪音
数据。你也可以手动处理。我这里是手动处理的，并将特征列相同数据，和俩特征相同数据都处理掉，因为这种情况没思考的意义，所有数据都一样，我们做向量计算时
无法影响结果，现实中一样的数据也没有思考意义。还会影响到我们的模型结果。

(5)噪点处理:
我先说明下几个重要指标。

标准差:S = Sqr(∑(xn-x拨)^2 /(n-1)) 一种量度数据分布的分散程度之标准，用以衡量数据值偏离算术平均值的程度。标准偏差越小,数据越稳定，高斯分布越强。
标准差的大小与平均值的倍率关系来衡量。如果标准差太小，这个特征性也越弱，在数据探索阶段。我们会重点观察标准差。

偏差(度):样本输出与真实之间的差异。偏差大容易欠你和。    -->噪点程度。

方差:样本的离散程度。方差大容易过拟合，因为太分散曲线容易扩张拟合，这样离散点太多真实的样本放里面容易错误。  -->离散程度

统计学中:我们的损失值 = 偏差+方差。 如果一组数据偏差大，它的方差就会小。反之同理。我们处理特征数据原则就是方差不能过大，

峰度:是一个用于衡量离群数据离群度的指标(峰度越大，离群值越多)。这个用于正态分布。   -->离群程度。

综上所述。我们主要针对偏差和标准差来计算。
代码中有详细说明:先去掉偏差大的，再扩大标准差。让均值-标准差*3为下界，均值-标准差*3为上界。然后做循环处理掉偏差不再这范围的。

(5)特征拼接: 通常我们可以用 XGB,RF,RandomizedLasso(特征稳定性选择)，RFE（特征递归消除）四个方法配合plt来可视化特征性强弱。然后选取特征性强的，与
业务结合起来跟一些可能先关的业务进行拼接，去看看这样组合特征性是否强，对模型结果是否影响大，是正相关还是负相关。
我这里用消费者数量和竞争对手距离与大量特征做了拼接，这是个很漫长的过程，需要大量的调试。

(6)特征提取，这里我用了RandomizedLasso(特征稳定性选择).是循环拟合数据特征比较强弱提取特征并可视化特征的算法。如果你想做好点，可以用交叉验证+RF
或者XGB+交叉验证。

(7)特征分类，这是一种聚类操作，其实我这里没必要用，因为类型特征One-Hot维度不高，不需要聚类，更不需要PCA/Random降维。
我这里用的分类是RF+交叉验证，每次分2,3,4,5类来比较，看看哪种更好，再进行最后的特征构造

(8)特征构造,通常我们最后的业务指标不好时，我们会构建他的最大值/最小值/标准差/方差/平均值/偏度/峰度等指标(这些不一定都使用)，
并一些地方做log1p平滑处理或者diff/diff2处理。
diff是表示俩个维度之间线性关系，越小线性关系越接近。diff2，是前俩个特征与第三个特征比较，我这里没必要，适合高维未知业务。
我将这些指标与数据的中位数进行了循环大量拼接，是一种很变态的做法，后期可以根据自己的数据业务进行代码修改。我特意用了管道方式，就是为了维护。


2.XGB特征比较和模型融合:
XGB自己有个特征比较方法，通常与KFold交叉验证，可以设计rmse为基准线来衡量各个特征与某一特征的强弱关系。
通常选择一个强特征，然后与其他特征一一比较，并在不同数据集上做交叉比较，防止过拟合不准确。最后得到一组特征与该特征的rmse指标，表示强弱关系，
这样方便我们特征拼接，特征构造。

XGB算法详解：
自己话详解:
首先，他是boosting算法。而boosting核心都是求残差:误差: y-f1(x) 也就是真实值-预测值。
残差y的导数/f1(x)导数做偏导。假误差，也就是残差。gbdt是一阶导。而Xgboost是二阶导数。
正常损失函数=正则项+算法模型损失函数。XGB的损失函数是进行二阶泰勒展开。也是梯度下降。这样，它的本质就是通过迭代次数，做梯度下降，达到最优化，
还是基于树的残差计算。所以准确率相当高。而且，他还可以自定义损失函数，我这里就自定义了rmpse,根据业务需求来做。
简单说就是损失函数中加了正则项来控制模型的复杂度。包含叶子节点个数，每个叶子节点输出的score的L2模的平方和。这样防止过拟合也是他的优势。

算法流程:
xgboost每进行完一次迭代，会将叶子节点权重乘以学习率，削弱每颗树的影响，所以一般把学习率设置小一点，迭代设置大一些。
还借鉴随机森林方法支持列抽样，降低过拟合减少计算。
对于特征值有缺失的样本，xgboost可以自动学习出它的分裂方向。实现层用内置交叉验证法。

注意:xgboost并行不是tree并行，它也是完成一次迭代再进行下一次，第t次迭代包含了前t-1次迭代的预测值。
xgboost并行是在特征粒度上的。因为决策树最消耗性能的步骤就是对特征值排序找到分割(这是个大循环)。所以xgboost训练前先排序，
保存block结构(类似kafka原理)。然后迭代时共享这个结构减少计算量。block结构就是并行的本质。计算出各个特征的增益，
选择增益最大的去分裂(总结就是特征粒度的并行，block结构，预排序。)

优势:
(1)树形结构加了正则项优化，防止过拟合
(2)用了二阶导数信息(速率)
(3)使用列抽样防止过拟合：分层抽样是指在抽样时，将总体分成互不相交 [2]  的层，
然后按照一定的比例，从各层独立地抽取一定数量的个体，将各层取出的个体合在一起作为样本的方法。层内变异越小越好，层间变异越大越好。
(4)节点分类算法能自动利用特征的稀疏性
(5)样本先排序以block存储，利于并行计算。
(6)本质是对树的叶子分数做惩罚，确保树的简单性
(7)支持分布式计算，可以用在yarn上。
(8)剪枝
当分裂时遇到一个负损失时，GBM会停止分裂。因此GBM实际上是一个贪心算法。 
XGBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除这个分裂。 
这种做法的优点，当一个负损失（如-2）后面有个正损失（如+10）的时候，就显现出来了。GBM会在-2处停下来，因为它遇到了一个负值。
但是XGBoost会继续分裂，然后发现这两个分裂综合起来会得到+8，因此会保留这两个分裂。
(9)内置交叉验证:
XGBoost允许在每一轮boosting迭代中使用交叉验证。因此，可以方便地获得最优boosting迭代次数。 而GBM使用网格搜索，只能检测有限个值。
(10)在已有的模型基础上继续
XGBoost可以在上一轮的结果上继续训练。这个特性在某些特定的应用上是一个巨大的优势。 sklearn中的GBM的实现也有这个功能，两种算法在这一点上是一致的。
(11）可以自己设定损失，只要函数里面有1阶和2阶导数。

XGBoost的参数
XGBoost的作者把所有的参数分成了三类：
1、通用参数：宏观函数控制。
2、Booster参数：控制每一步的booster(tree/regression)。
3、学习目标参数：控制训练目标的表现。
在这里我会类比GBM来讲解，所以作为一种基础知识。

通用参数
这些参数用来控制XGBoost的宏观功能。

1、booster[默认gbtree]
选择每次迭代的模型，有两种选择：
gbtree：基于树的模型
gbliner：线性模型

2、silent[默认0]
当这个参数值为1时，静默模式开启，不会输出任何信息。 一般这个参数就保持默认的0，因为这样能帮我们更好地理解模型。

3、nthread[默认值为最大可能的线程数]
这个参数用来进行多线程控制，应当输入系统的核数。 如果你希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。
还有两个参数，XGBoost会自动设置，目前你不用管它。接下来咱们一起看booster参数。

booster参数
尽管有两种booster可供选择，我这里只介绍tree booster，因为它的表现远远胜过linear booster，所以linear booster很少用到。

1、eta[默认0.3]
和GBM中的 learning rate 参数类似。 通过减少每一步的权重，可以提高模型的鲁棒性。 典型值为0.01-0.2。

2、min_child_weight[默认1]
决定最小叶子节点样本权重和。 和GBM的 min_child_leaf 参数类似，但不完全一样。XGBoost的这个参数是最小样本权重的和，
而GBM参数是最小样本总数。 这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。 但是如果这个值过高，
会导致欠拟合。这个参数需要使用CV来调整。

3、max_depth[默认6]
和GBM中的参数相同，这个值为树的最大深度。 这个值也是用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。 
需要使用CV函数来进行调优。 典型值：3-10

4、max_leaf_nodes
树上最大的节点或叶子的数量。 可以替代max_depth的作用。因为如果生成的是二叉树，一个深度为n的树最多生成n2个叶子。
如果定义了这个参数，GBM会忽略max_depth参数。

5、gamma[默认0]
在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，
算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。

6、max_delta_step[默认0]
这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。 
通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。 这个参数一般用不到，但是你可以挖掘出来它更多的用处。

7、subsample[默认1]
和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。 减小这个参数的值，算法会更加保守，避免过拟合。但是
，如果这个值设置得过小，它可能会导致欠拟合。 典型值：0.5-1

8、colsample_bytree[默认1]
和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。 典型值：0.5-1

9、colsample_bylevel[默认1]
用来控制树的每一级的每一次分裂，对列数的采样的占比。 我个人一般不太用这个参数，因为subsample参数和colsample_bytree参数可以起到相同的作用。
但是如果感兴趣，可以挖掘这个参数更多的用处。

10、lambda[默认1]
权重的L2正则化项。(和Ridge regression类似)。 这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，
但是这个参数在减少过拟合上还是可以挖掘出更多用处的。

11、alpha[默认1]
权重的L1正则化项。(和Lasso regression类似)。 可以应用在很高维度的情况下，使得算法的速度更快。

12、scale_pos_weight[默认1]
在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。

学习目标参数
这个参数用来控制理想的优化目标和每一步结果的度量方法。

1、objective[默认reg:linear]
这个参数定义需要被最小化的损失函数。最常用的值有：
binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)。 multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。
在这种情况下，你还需要多设一个参数：num_class(类别数目)。 multi:softprob 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。

2、eval_metric[默认值取决于objective参数的取值]
对于有效数据的度量方法。 对于回归问题，默认值是rmse，对于分类问题，默认值是error。 典型值有：
rmse 均方根误差(∑Ni=1?2N??????√) mae 平均绝对误差(∑Ni=1|?|N) logloss 负对数似然函数值 error 二分类错误率(阈值为0.5) 
merror 多分类错误率 mlogloss 多分类logloss损失函数 auc 曲线下面积

3、seed(默认0)
随机数的种子 设置它可以复现随机数据的结果，也可以用于调整参数

如果你之前用的是Scikit-learn,你可能不太熟悉这些参数。但是有个好消息，python的XGBoost模块有一个sklearn包，XGBClassifier。
这个包中的参数是按sklearn风格命名的。会改变的函数名是：
1、eta ->learning_rate
2、lambda->reg_lambda
3、alpha->reg_alpha
你肯定在疑惑为啥咱们没有介绍和GBM中的’n_estimators’类似的参数。XGBClassifier中确实有一个类似的参数，但是，是在标准XGBoost实现中调用拟合函数时，
把它作为’num_boosting_rounds’参数传入。



3.柯尔莫哥洛夫-斯米尔诺夫测试测试训练数据和测试数据:
统计学中用于衡量训练集与测试集特征差异性的，返回一个k2,一个value,大概意思就是设定阈值，只要返回在这个区域说明训练集和测试集的差异性不大。
不过前提是全部都是数值型，维度必须一样。

4.RandomizedLasso 特征提取:
稳定性选择是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。
它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，
比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。
理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。



****************************************************************************************************************************************


二.代码讲解:
1.流程:
先设计了大量util包的函数，但我都写在一起，大家可以直接复制到电脑上直接跑。而代码都是使用了管道来跑。

2.管道的使用:
管道机制一定要记住:
Pipline-》串行化。让X直接进行所有的类，处理最后构建模型，所有类都必须有重写fit,tansform方法。
最开始都进行初始化管道里的所有类，记住是类，好比一样先开辟个空间占好位置。然后执行fit方法取拟合数据或者模型，记住，如果是Pipline
最后一定是分类器或者聚类器。如果是make-pipline，就不用，可以放入一些类，然后去拟合我们需要的数据。fit方法执行后，再调用tansform转化。而tansform
里的形参是fit拟合后的结果，记住这点。如果我们最后是个分类器，当它predict时，会再跑一遍所有该管道的tansform，为的就是让测试集数据与训练集一样，这样
省的再重新fit拟合一次。但由于我设计算法功底不够，我是将trian和test分别makie-pipline处理，再将train放入pipline最后predict test。

这里还有个方法是featureUnion, 这是个并行机制。我们可以用 Parallel 并行模块处理，我这里是用了特征分类和特征构造这俩个消耗时间的做的并行处理。
python的 Parallel 处理大数据集时要比skt-learn自带的快!
同样通过(key，value)对来设置，通过set_params设置参数。不同的是，每一个step分开计算，
FeatureUnion最后将它们计算得到的结果合并到一块，返回的是一个数组，记住一定要是数组，而且里面自身不具备最后一个estimator的方法。
有些数据需要标准化，或者取对数，或onehot编码最后形成多个特征项，再选择重要特征，这时候FeatureUnion非常管用。
让然，我这里直接放在PipLine里使用了。


3.出现的问题:
(1)做管道操作时，tansform.fit,类型问题，我这里都在代码中改变，具体原因有些地方还不是很明白。
(2)网格搜索的异常错误: ValueError: continuous format is not supported
   你网格搜索的参数与你创建的模型初始化参数不匹配，尽量让其匹配，初始值为网格搜索里的数，最好是第一个数。
(3)TypeError: unhashable type: 'slice' 异常，这个类型不能被哈希。一般是可变类型。
(4)读取数据，平滑目标值,注意，这里测试集和训练集存在很多不同的feature，所以这里我不建议合并数据，我之前这吃了大亏。
(5)处理nan值，看是否有需要二值化特征,这时候再(处理非数值型数据我们最后再处理，因为太慢了。)
(6)1.ValueError: cannot reindex from a duplicate axis
原因(1)：原始索引中可能有重复的值。找到他们这样做：
解决: df[df.index.duplicated()]
原因(2):新建一个空的DataFrame，只有列索引，没有对应的数据，当你需要引用对应的Series数据到DataFrame中时
如果每个Series的index都不一样，那么添加时就会出现NaN值，甚至是错误提示
解决:使用reset_index()方法来重置它们的索引，以便后续的操作。
print(df.reset_index())
(7)（1） TypeError: 'DataFrame' object is not callable
#其实就是变量名和python自身的global变量重名了
#详细解决：https://blog.csdn.net/junbin_h/article/details/68066143
#(2)bad input shape()
#transform里也是要求列表格式，用list转换。
#详细解决:https://jingyan.baidu.com/article/ce09321b846bd72bff858f14.html


