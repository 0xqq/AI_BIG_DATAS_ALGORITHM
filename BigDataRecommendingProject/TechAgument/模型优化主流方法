1 Ensemble Learning-模型融合
通过对多个单模型融合以提升整体性能。

1.1 Voting
投票制即为，投票多者为最终的结果。例如一个分类问题，多个模型投票（当然可以设置权重）。最终投票数最多的类为最终被预测的类。

1.2 Averaging
Averaging即所有预测器的结果平均。

回归问题，直接取平均值作为最终的预测值。（也可以使用加权平均）
分类问题，直接将模型的预测概率做平均。（or 加权）
加权平均，其公式如下：

∑i=1nWeighti∗Pi
其中n表示模型的个数， Weighti表示该模型权重，Pi表示模型i的预测概率值。

例如两个分类器，XGBoost（权重0.4）和LightGBM（权重0.6），其预测概率分别为：0.75、0.5，那么最终的预测概率，(0.4 * 0.75+0.6 * 0.5)/(0.4+0.6)=0.6

模型权重也可以通过机器学习模型学习得到

1.3 Ranking
Rank的思想其实和Averaging一致，但Rank是把排名做平均，对于AUC指标比较有效。

个人认为其实就是Learning to rank的思想，可以来优化搜索排名。具体公式如下：

∑i=1nWeightiRanki
其中n表示模型的个数， Weighti表示该模型权重，所有权重相同表示平均融合。Ranki表示样本在第i个模型中的升序排名。它可以较快的利用排名融合多个模型之间的差异，而不需要加权融合概率。

1.4 Binning
将单个模型的输出放到一个桶中。参考pdf paper ， Guocong Song ，

1.5 Bagging
使用训练数据的不同随机子集来训练每个 Base Model，最后每个 Base Model 权重相同，分类问题进行投票，回归问题平均。

随机森林就用到了Bagging，并且具有天然的并行性。

1.6 Boosting
Boosting是一种迭代的方法，每一次训练会更关心上一次被分错的样本，比如改变被错分的样本的权重的Adaboost方法。还有许多都是基于这种思想，比如Gradient Boosting等。

经典问题：随机森林、Adaboost、GBDT、XGBoost的区别是什么？（面试常常被问）

1.7 Stacking
Stacking

图片来自Wille的博客。

从上图可以看出，类似交叉验证。

将数据集分为K个部分，共有n个模型。

for i in xrange(n):

​ for i in xrange(k):

​ 用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。

​ 对于测试集，直接用这k个模型的预测值均值作为新的特征。

这样k次下来，整个数据集都获得了这个模型构建的New Feature。n个模型训练下来，这个模型就有n个New Features。

把New Features和label作为新的分类器的输入进行训练。然后输入测试集的New Features输入模型获得最终的预测结果。

1.8 Blending
Blending直接用不相交的数据集用于不同层的训练。

以两层的Blending为例，训练集划分为两部分（d1，d2），测试集为test。

第一层：用d1训练多个模型，讲其对d2和test的预测结果作为第二层的New Features。
第二层：用d2的New Features和标签训练新的分类器，然后把test的New Features输入作为最终的预测值。
2 融合的条件
Base Model 之间的相关性要尽可能的小。这就是为什么非 Tree-based Model 往往表现不是最好但还是要将它们包括在 Ensemble 里面的原因。Ensemble 的 Diversity 越大，最终 Model 的 Bias 就越低。
Base Model 之间的性能表现不能差距太大。这其实是一个 Trade-off，在实际中很有可能表现相近的 Model 只有寥寥几个而且它们之间相关性还不低。但是实践告诉我们即使在这种情况下 Ensemble 还是能大幅提高成绩。
3 Python实现
下面只实现了一些常用的融合方法，其他的类推。

3.1 Stacking
'''5折stacking'''
n_folds = 5
skf = list(StratifiedKFold(y, n_folds))
for j, clf in enumerate(clfs):
    '''依次训练各个单模型'''
    dataset_blend_test_j = np.zeros((X_predict.shape[0], len(skf)))
    for i, (train, test) in enumerate(skf):
        '''使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。'''
        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]
        clf.fit(X_train, y_train)
        y_submission = clf.predict_proba(X_test)[:, 1]
        dataset_blend_train[test, j] = y_submission
        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, 1]
    '''对于测试集，直接用这k个模型的预测值均值作为新的特征。'''
    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)

'''融合使用的模型'''
clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)
clf.fit(dataset_blend_train, y)
y_submission = clf.predict_proba(dataset_blend_test)[:, 1]

完整代码见：GitHub_ensemble_stacking

3.2 Blending
下面是一个两层的Blending的实现

'''切分训练数据集为d1,d2两部分'''
X_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=0.5, random_state=2017)
dataset_blend_train = np.zeros((X_d2.shape[0], len(clfs)))
dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))

for j, clf in enumerate(clfs):
    '''依次训练各个单模型'''
    # print(j, clf)
    '''使用第1个部分作为预测，第2部分来训练模型，获得其预测的输出作为第2部分的新特征。'''
    # X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]
    clf.fit(X_train, y_train)
    y_submission = clf.predict_proba(X_test)[:, 1]
    dataset_blend_train[:, j] = y_submission
    '''对于测试集，直接用这k个模型的预测值作为新的特征。'''
    dataset_blend_test[:, j] = clf.predict_proba(X_predict)[:, 1]
    print("val auc Score: %f" % roc_auc_score(y_predict, dataset_blend_test[:, j]))

'''融合使用的模型'''
# clf = LogisticRegression()
clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)
clf.fit(dataset_blend_train, y_test)
y_submission = clf.predict_proba(dataset_blend_test)[:, 1]

四、模型融合的结合策略： 基本学习器学习完后，需要将各个模型进行融合，常见的策略有：

1，平均法： 平均法有一般的评价和加权平均，这个好理解。对于平均法来说一般用于回归预测模型中，在Boosting系列融合模型中，一般采用的是加权平均融合。

2，投票法：有绝对多数投票（得票超过一半），相对多数投票（得票最多），加权投票。这个也好理解，一般用于分类模型。在bagging模型中使用。

3，学习法：一种更为强大的结合策略是使用”学习法”，即通过另一个学习器来进行结合，把个体学习器称为初级学习器，用于结合的学习器称为次级学习器或元学习器。常见的有Stacking和Blending两种 
（1）Stacking方法： Stacking 先从初始数据集训练出初级学习器，然后”生成”一个新数据集用于训练次级学习器。在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记。 
stacking一般使用交叉验证的方式，初始训练集D 被随机划分为k 个大小相似的集合D1 ， D2 ， … ， Dk，每次用k-1个部分训练T个模型，对另个一个部分产生T个预测值作为特征，遍历每一折后，也就得到了新的特征集合，标记还是源数据的标记，用新的特征集合训练一个集合模型。

这里写图片描述
（2）Blending方法： Blending与Stacking大致相同，只是Blending的主要区别在于训练集不是通过K-Fold的CV策略来获得预测值从而生成第二阶段模型的特征，而是建立一个Holdout集，例如说10%的训练数据，第二阶段的stacker模型就基于第一阶段模型对这10%训练数据的预测值进行拟合。说白了，就是把Stacking流程中的K-Fold CV 改成 HoldOut CV。

五、多样性增强： 模型的多样性可以改善融合后的效果，增强的手段有：

数据样本的扰动：bagging中提到的给定初始数据集， 可从中产生出不同的数据子集， 再利用不同的数据子集训练出不同的个体学习器。
输入属性的扰动：随机深林提到的使用控制属性的子空间的不同，产生差异较大的模型。
输出表示的扰动：输出表示进行操纵以增强多样性，可对训练样本的类标记稍作变动。
算法参数的扰动：基学习算法一般都有参数需进行设置，例如神经网络的隐层神经元数、初始连接权值等，通过随机设置不同的参数，往往可产生差别较大的个体学习器。
六、bagging、boosting的对比： Bagging主要在优化variance（即模型的鲁棒性），boosting主要在优化bias（即模型的精确性） 
bagging： Bagging 是 Bootstrap Aggregating 的简称，意思就是再取样 (Bootstrap) 然后在每个样本上训练出来的模型取平均，所以是降低模型的 variance。 
由于E[\frac{\sum X_i}{n}]=E[X_i]，所以bagging后的bias和单个子模型的接近，一般来说不能显著降低bias。另一方面，若各子模型独立，则有Var(\frac{\sum X_i}{n})=\frac{Var(X_i)}{n}，此时可以显著降低variance。 
Boosting： boosting从优化角度来看，是用forward-stagewise这种贪心法去最小化损失函数（指数函数），boosting是在sequential地最小化损失函数，其bias自然逐步下降。但由于是采取这种sequential、adaptive的策略，各子模型之间是强相关的，于是子模型之和并不能显著降低variance。所以说boosting主要还是靠降低bias来提升预测精度。 
具体关于优化角度的认识查看：为什么说bagging是减少variance，而boosting是减少bias?
